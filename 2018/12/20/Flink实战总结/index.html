<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content><title>Flink实战总结 | CHAO LI's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Flink实战总结</h1><a id="logo" href="/.">CHAO LI's Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Flink实战总结</h1><div class="post-meta">Dec 20, 2018<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink 近几年来一直备受业界瞩目，相对于同时期一夜成名的Spark来说，有种厚积薄发的味道。 当然，从根本上来看，也是因为这几年对于实时分布式计算引擎的需求日渐强烈，要求也越来越高（数据的latency，一致性）。而这也意味着以微批次来fake实时处理的Spark Streaming不再能满足实时处理系统的硬性要求(忽略spark continuous processing实现)。最近本司也正在考虑将实时处理任务从Spark Streaming迁移到Flink；于是就有了下面这篇实战总结文章。</p>
<h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><ul>
<li>flink程序能实现在分布式的结合上进行各种转换操作，集合通常来自订阅的来源（文件，kafka,local,in-memory），结果被返回到sinks里（大多数写入分布式文件系统，或者标准输出）</li>
<li>DataSet and DataStream<ul>
<li>DataSet和DataStream在flink中都代表一种数据结构，是不可变且包含重复记录的集合。区别在于DataSet是有限的集合，而DataStream是无界的</li>
</ul>
</li>
<li>flink 配置interlij ideal 在本地运行调试<ul>
<li>只需要将flink依赖的包引入项目中即可启动项目<br><img src="http://jacobs.wanhb.cn/images/flink1.png" alt="HBase 架构图"></li>
</ul>
</li>
<li><strong>讲解Flink怎么序列化objects，怎么分配内存</strong><a href="https://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html" target="_blank" rel="noopener">Apache Flink: Juggling with Bits and Bytes</a></li>
</ul>
<h3 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a>DataStream</h3><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/datastream_api.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Flink DataStream API Programming Guide</a></li>
<li>datasource（数据源）: <ul>
<li>File-based: readTextFile, readFile…</li>
<li>Socket-based: socketTextStream</li>
<li>Collection-based: fromCollection, fromElements</li>
<li>custom: addSource, FlinkKafkaConsumer08 or other connectors</li>
</ul>
</li>
</ul>
<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Flink DataSet API Programming Guide</a></li>
</ul>
<h3 id="savepoint"><a href="#savepoint" class="headerlink" title="savepoint"></a>savepoint</h3><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/savepoints.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Savepoints</a></li>
<li>Savepoints are created, owned, and deleted by the user.</li>
<li>目前savepoint和checkpoint实现和format方式都相同（除了checkpoint选择了rocksdb作为state backend，这样format会有些微不同）</li>
<li>Operations：<ul>
<li>Triggering Savepoints： FsStateBackend or RocksDBStateBackend:</li>
<li>Trigger a Savepoint</li>
<li>Cancel Job with Savepoint<ul>
<li><code>bin/flink cancel -s [:targetDirectory] :jobId</code></li>
</ul>
</li>
<li>Resuming from Savepoints <ul>
<li><code>$ bin/flink run -s :savepointPath [:runArgs]</code></li>
</ul>
</li>
<li>Disposing Savepoints<ul>
<li><code>$ bin/flink savepoint -d :savepointPath</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h3><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/checkpoints.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Checkpoints</a></li>
<li>生命周期是由Flink管理，checkpoint的管理，创建以及释放统一通过Flink，而不需要用户干预</li>
<li><strong>Checkpoints are usually dropped（随应用退出被清除）</strong> after the job was terminated by the user (except if explicitly configured as retained Checkpoints)</li>
<li>checkpoint 优化 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/large_state_tuning.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Tuning Checkpoints and Large State</a><ul>
<li>state 双写：一份在distributed storage(HDFS)；一份在local</li>
<li>task-local recovery：默认是关闭的状态,可以通过<code>state.backend.local-recovery</code> 打开</li>
</ul>
</li>
</ul>
<h3 id="Barriers"><a href="#Barriers" class="headerlink" title="Barriers"></a>Barriers</h3><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-master/internals/stream_checkpointing.html" target="_blank" rel="noopener">Apache Flink 1.8-SNAPSHOT Documentation: Data Streaming Fault Tolerance</a></li>
</ul>
<h3 id="Window，waterMark，Trigger"><a href="#Window，waterMark，Trigger" class="headerlink" title="Window，waterMark，Trigger"></a>Window，waterMark，Trigger</h3><ul>
<li><a href="https://www.jianshu.com/p/a883262241ef" target="_blank" rel="noopener">Window，waterMark，Trigger介绍- 简书</a></li>
<li>window<ul>
<li>滚动窗口：分配器将每个元素分配到一个指定窗口大小的窗口中，并且不会重叠；TumblingEventTimeWindows.of(Time.seconds(5))</li>
<li>滑动窗口：滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率；因此可能出现窗口重叠，如果滑动参数小于滚动参数的话；SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))</li>
<li>会话窗口：通过session活动来对元素进行分组，跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况。当他在一个固定的时间周期内不再收到元素，即非活动间隔产生，那么窗口就会关闭；<ul>
<li>一个session窗口通过一个session间隔来配置，这个session间隔定义了非活跃周期的长度。当这个非活跃周期产生，那么当前的session将关闭并且后续的元素将被分配到新的session窗口中去。如：EventTimeSessionWindows.withGap(Time.minutes(10)</li>
</ul>
</li>
</ul>
</li>
<li>触发器(Triggers)<ul>
<li>触发器定义了一个窗口何时被窗口函数处理</li>
<li>EventTimeTrigger</li>
<li>ProcessingTimeTrigger</li>
<li>CountTrigger</li>
<li>PurgingTrigger</li>
</ul>
</li>
<li>驱逐器(Evictors)</li>
</ul>
<h2 id="任务提交与停止姿势"><a href="#任务提交与停止姿势" class="headerlink" title="任务提交与停止姿势"></a>任务提交与停止姿势</h2><ul>
<li><p>任务提交</p>
<ul>
<li><strong>启动命令详解</strong> :<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: YARN Setup</a></li>
<li><p>参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Usage:</span><br><span class="line">   Required</span><br><span class="line">     -n,--container &lt;arg&gt;   Number of YARN container to allocate (=Number of Task Managers)</span><br><span class="line">   Optional</span><br><span class="line">     -D &lt;arg&gt;                        Dynamic properties</span><br><span class="line">     -d,--detached                   Start detached</span><br><span class="line">     -jm,--jobManagerMemory &lt;arg&gt;    Memory for JobManager Container with optional unit (default: MB)</span><br><span class="line">     -nm,--name                      Set a custom name for the application on YARN</span><br><span class="line">     -q,--query                      Display available YARN resources (memory, cores)</span><br><span class="line">     -qu,--queue &lt;arg&gt;               Specify YARN queue.</span><br><span class="line">     -s,--slots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class="line">     -tm,--taskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with optional unit (default: MB)</span><br><span class="line">     -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths for HA mode</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交到yarn-cluster上需要以 ::y:: 或者::yarn::作为前缀；如: <code>ynm=nm</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink run -c com.jacobs.jobs.realtime.wordcount.WindowWordCount target/real-time-jobs-1.0.0-SNAPSHOT.jar</span><br><span class="line"></span><br><span class="line">flink run -m yarn-cluster -ynm TestSinkUserLogStream -yn 4 -yjm 1024m -ytm 4096m -ys 4 -yqu feed.prod -c com.weibo.api.feed.dm.stream.TestFlinkStream /data1/dm-flink/feed-dm-flink-1.0.4-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>停止任务</strong></p>
<ul>
<li><strong>关闭或重启flink程序不能直接kill掉</strong>，这样会导致flink来不及制作checkpoint，而应该调用flink提供的cancel语意</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//重启正确姿势, with savepoint</span><br><span class="line">1. 调用cancel，cancel之前先触发savepoint</span><br><span class="line">bin/flink cancel -s [:targetDirectory] :jobId -yid: yarnAppId</span><br><span class="line">例子: flink cancel -s hdfs://vcp-yz-nameservice1/user/hcp/hcpsys/feed/flink-checkpoints/test-user-logs 97b4e67859af4bfb1b597355f1c846f3 -yid application_1542801635735_2121</span><br><span class="line">2. 从savepoint中恢复flink程序</span><br><span class="line">bin/flink run -s :savepointPath [:runArgs]</span><br><span class="line">例子: flink run -s hdfs://vcp-yz-nameservice1/user/hcp/hcpsys/feed/flink-checkpoints/test-user-logs/savepoint-97b4e6-22dd5890dd0c -m yarn-cluster -ynm TestSinkUserLogStream -yn 4 -yjm 1024m -ytm 4096m -ys 4 -yqu feed.prod -c com.weibo.api.feed.dm.stream.TestFlinkStream /data1/dm-flink/feed-dm-flink-1.0.4-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
<h2 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h2><h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><ul>
<li>standalone 启动cluster<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/flink-1.6.0/bin;./start-cluster.sh</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="On-Yarn-Cluster"><a href="#On-Yarn-Cluster" class="headerlink" title="On Yarn Cluster"></a>On Yarn Cluster</h3><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: YARN</a></li>
<li><strong>参考文章</strong><a href="https://zhouhai02.com/post/flink-internals/flink1.6-flip6-flink-on-yarn-arch/" target="_blank" rel="noopener">Flink1.6 - flink on yarn分布式部署架构 - 深山含笑</a></li>
<li><p>架构图<br><img src="http://jacobs.wanhb.cn/images/flink2.png" alt="HBase 架构图"></p>
<ul>
<li>JobManager 和 ApplicationMaster  运行在同一个JVM里</li>
</ul>
</li>
<li><p><strong>on yarn 两种模式</strong></p>
<ul>
<li>session模式：允许运行多个作业在同一个Flink集群中，代价是作业之间没有资源隔离（同一个TM中可能跑多个不同作业的task）</li>
<li>per-job模式（生产环境）：per-job模式是指在yarn上为每一个Flink作业都分配一个单独的Flink集群，这样就解决了不同作业之间的资源隔离问题</li>
</ul>
</li>
<li><strong>摘录参考文章</strong>相比旧的Flink-on-YARN架构（Flink 1.5之前），新的yarn架构带来了以下的优势：<ul>
<li>client可以直接在yarn上面启动一个作业，不在像以前需要先启动一个固定大小的Flink集群然后把作业提交到这个Flink集群上</li>
<li>按需申请容器（指被同一个作业的不同算子所使用的容器可以有不同的CPU/Memory配置），没有被使用的容器将会被释放<br><img src="http://jacobs.wanhb.cn/images/flink3.png" alt="HBase 架构图"></li>
</ul>
</li>
<li>slot资源申请/分配流程分析</li>
<li>请求新TaskExecutor的slot分配<br><img src="http://jacobs.wanhb.cn/images/flink4.png" alt="HBase 架构图"></li>
<li>ResourceManager挂掉 ：不会挂掉task,不断尝试重新注册ResourceManager<strong>详细见参考文章</strong></li>
<li>TaskExecutor挂掉</li>
<li>JobMaster挂掉</li>
</ul>
<h2 id="资源分配相关？"><a href="#资源分配相关？" class="headerlink" title="资源分配相关？"></a>资源分配相关？</h2><ul>
<li>在operator中对并行度的设置将决定任务分配到几个task slot里面去</li>
</ul>
<h2 id="Flink程序运行流程分解"><a href="#Flink程序运行流程分解" class="headerlink" title="Flink程序运行流程分解"></a>Flink程序运行流程分解</h2><h3 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h3><ul>
<li><ol>
<li>Obtain an execution environment</li>
</ol>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">getExecutionEnvironment()</span><br><span class="line">createLocalEnvironment()</span><br><span class="line">createRemoteEnvironment(host: String, port: Int, jarFiles: String*)</span><br></pre></td></tr></table></figure>
<ul>
<li><ol>
<li>Load/create the initial data</li>
</ol>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val text: DataStream[String] = env.readTextFile(&quot;file:///path/to/file&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><ol>
<li>Specify transformations on this data</li>
</ol>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//create a new DataStream by converting every String in the original collection to an integer</span><br><span class="line">val mapped = input.map &#123; x =&gt; x.toInt &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><ol>
<li>Specify where to put the results of your computations</li>
</ol>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">writeAsText(path: String)</span><br><span class="line">print()</span><br></pre></td></tr></table></figure>
<ul>
<li><ol>
<li>Trigger the program execution</li>
</ol>
</li>
</ul>
<h2 id="Flink-watermark机制"><a href="#Flink-watermark机制" class="headerlink" title="Flink watermark机制"></a>Flink watermark机制</h2><ul>
<li><p><strong>【重要】详细讲解watermark</strong>:  <a href="https://blog.csdn.net/lmalds/article/details/52704170" target="_blank" rel="noopener">Flink流计算编程—watermark（水位线）</a> </p>
<ul>
<li><p>window 触发的两个条件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1、watermark时间 &gt;= window_end_time</span><br><span class="line">2、在[window_start_time,window_end_time)中有数据存在</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/20585530" target="_blank" rel="noopener">摘录：深入理解Flink核心技术 </a></p>
<ul>
<li>纵坐标为event_time，横坐标为processingTime，理想情况自然是两者一致，但实际情况肯定不可能<br><img src="http://jacobs.wanhb.cn/images/flink5.png" alt="HBase 架构图"></li>
</ul>
</li>
<li><a href="http://shiyanjun.cn/archives/1785.html" target="_blank" rel="noopener">摘录：使用EventTime与WaterMark进行流数据处理</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> // 这块结合上图理解watermark的值</span><br><span class="line">@Override</span><br><span class="line">    public final Watermark getCurrentWatermark() &#123;</span><br><span class="line">        long potentialWM = currentMaxTimestamp - maxOutOfOrderness; // 当前最大事件时间戳，减去允许最大延迟到达时间</span><br><span class="line">        if (potentialWM &gt;= lastEmittedWatermark) &#123; // 检查上一次emit的WaterMark时间戳，如果比lastEmittedWatermark大则更新其值</span><br><span class="line">            lastEmittedWatermark = potentialWM;</span><br><span class="line">        &#125;</span><br><span class="line">        return new Watermark(lastEmittedWatermark);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Windowing, WaterMark,Trigger 三者依赖关系<ul>
<li>Windowing：就是负责该如何生成Window，比如Fixed Window、Slide Window，当你配置好生成Window的策略时，Window就会根据时间动态生成，最终得到一个一个的Window，包含一个时间范围：[起始时间, 结束时间)，它们是一个一个受限于该时间范围的事件记录的容器，每个Window会收集一堆记录，满足指定条件会触发Window内事件记录集合的计算处理</li>
<li>WaterMark：它其实不太好理解，可以将它定义为一个函数E=f(P)，当前处理系统的处理时间P，根据一定的策略f会映射到一个事件时间E，可见E在坐标系中的表现形式是一条曲线，根据f的不同曲线形状也不同。假设，处理时间12:00:00，我希望映射到事件时间11:59:30，这时对于延迟30秒以内（事件时范围11:59:30~12:00:00）的事件记录到达处理系统，都指派到时间范围包含处理时间12:00:00这个Window中。事件时间超过12:00:00的就会由Trigger去做补偿了。</li>
<li>Trigger：为了满足实际不同的业务需求，对上述事件记录指派给Window未能达到实际效果，而做出的一种补偿，比如事件记录在WaterMark时间戳之后到达事件处理系统，因为已经在对应的Window时间范围之后，我有很多选择：选择丢弃，选择是满足延迟3秒后还是指派给该Window，选择只接受对应的Window时间范围之后的5个事件记录，等等，这都是满足业务需要而制定的触发Window重新计算的策略，所以非常灵活。</li>
</ul>
</li>
</ul>
<h2 id="Sink-Connectors"><a href="#Sink-Connectors" class="headerlink" title="Sink Connectors"></a>Sink Connectors</h2><ul>
<li>Kafka </li>
<li>Elasticsearch</li>
<li>RabbitMQ</li>
<li>Rolling File Sink (HDFS)<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/filesystem_sink.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: HDFS Connector</a></li>
</ul>
</li>
<li>Streaming File Sink <ul>
<li>partfile 有三种状态：in-progress, pending,finished；part file先被写成in-progress，一旦被关闭写入，会变成pending，当检查点成功之后，pending状态的文件将变成finished; </li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/connectors/streamfile_sink.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Streaming File Sink</a></li>
<li>Using Row-encoded Output Formats <ul>
<li>可以指定RollingPolicy 来滚动生成分区中的文件</li>
</ul>
</li>
<li>Using Bulk-encoded Output Formats<ul>
<li><strong>支持parquet，orc等文件格式</strong>，批量编码文件</li>
<li>通过BulkWriter.Factory定义不同的文件格式   <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/api/java/org/apache/flink/formats/parquet/avro/ParquetAvroWriters.html" target="_blank" rel="noopener">ParquetAvroWriters (flink 1.7-SNAPSHOT API)</a><br>/Users/lichao15/Documents/github/awesome-big-data/README.md        - <strong>源码：</strong> <a href="https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java" target="_blank" rel="noopener">flink/StreamingFileSink.java at master · apache/flink · GitHub</a></li>
<li>使用这种方式只能配合 <code>OnCheckpointRollingPolicy</code>  使用来滚动生成分区文件，通过设置 <code>env.enableCheckpointing(interval)</code>来设置文件滚动间隔</li>
<li><strong>Streaming to parquet in hdfs 出现问题，内存溢出导致job无限崩溃重启，大量part file</strong></li>
<li>如果失败，将从上一个检查点开始重新store，期间回滚in-progress中的文件，以确保不会重复保存上一个检查点之后的数据</li>
<li><strong>part文件过多问题</strong> <a href="https://stackoverflow.com/questions/52638193/streaming-to-parquet-files-not-happy-with-flink-1-6-1" target="_blank" rel="noopener">Streaming to parquet files not happy with flink 1.6.1 - Stack Overflow</a></li>
<li><strong>rolling parquet file 重点邮件</strong> <a href="http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Streaming-to-Parquet-Files-in-HDFS-td23492.html" target="_blank" rel="noopener">Apache Flink User Mailing List archive. - Streaming to Parquet Files in HDFS</a></li>
<li>注意压缩的时候内存溢出的情况，flink陷入无限的重启循环中<br><img src="http://jacobs.wanhb.cn/images/flink6.png" alt="HBase 架构图"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="StreamingFileSink与Kafka-结合"><a href="#StreamingFileSink与Kafka-结合" class="headerlink" title="StreamingFileSink与Kafka 结合"></a>StreamingFileSink与Kafka 结合</h2><h3 id="如何做到exactly-once？"><a href="#如何做到exactly-once？" class="headerlink" title="如何做到exactly once？"></a>如何做到exactly once？</h3><ul>
<li><a href="https://flink.apache.org/features/2018/03/01/end-to-end-exactly-once-apache-flink.html" target="_blank" rel="noopener"> An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)</a><ul>
<li>二阶段提交</li>
</ul>
</li>
<li>partfile 有三种状态：in-progress, pending,finished；part file先被写成in-progress，一旦被关闭写入，会变成pending，当检查点成功之后，pending状态的文件将变成finished;</li>
<li>如果失败，将从上一个检查点开始重新store，期间回滚in-progress中的文件，以确保不会重复保存上一个检查点之后的数据<h3 id="flink如何控制kafka-offset提交与checkpoint-amp-amp-savepoint相结合"><a href="#flink如何控制kafka-offset提交与checkpoint-amp-amp-savepoint相结合" class="headerlink" title="flink如何控制kafka offset提交与checkpoint&amp;&amp;savepoint相结合"></a>flink如何控制kafka offset提交与checkpoint&amp;&amp;savepoint相结合</h3></li>
<li><a href="http://m.sohu.com/a/168546400_617676" target="_blank" rel="noopener">FlinkKafkaConsumer使用详解</a></li>
<li>关闭checkpoint(<strong>Checkpointingdisabled</strong>): <ul>
<li>此时， Flink Kafka Consumer依赖于它使用的具体的Kafka client的自动定期提交offset的行为，相应的设置是 Kafka properties中的 enable.auto.commit (或者 auto.commit.enable 对于Kafka 0.8) 以及 auto.commit.interval.ms</li>
</ul>
</li>
<li>开启checkpoint(<strong>Checkpointingenabled</strong>):<ul>
<li>在这种情况下，Flink Kafka Consumer会将offset存到checkpoint中</li>
<li><strong>制作完checkpoint 一并提交offsets</strong> 当checkpoint 处于completed的状态时（<strong>整个job的所有的operator都收到了这个checkpoint的barrier</strong>）。将offset记录起来并提交，从而保证exactly-once</li>
</ul>
</li>
</ul>
<ul>
<li>::exactly once的两个风险点：可结合savepoint来做::<ul>
<li><ol>
<li>异常退出的情况，没法来得及做checkpoint，而checkpoint间隔太长会导致丢失大量数据；可以通过airflow周期性手动触发savepoint恢复；封装hflink脚本<ul>
<li>解决思路是结合savepoint来做，通过<strong>airflow定时的触发savepoint</strong>操作，防止因checkpoint未及时做数据丢失</li>
<li>规定一分钟savepoint一次，这样即使分钟级别的数据丢失也是可以容忍</li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>第一点利用savepoint来做也有风险：在做savepoint的时候，如果异常退出，parfile未及时关闭导致数据丢失<ul>
<li><strong>暂时可以认为问题较小？</strong></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="如何控制背压"><a href="#如何控制背压" class="headerlink" title="如何控制背压"></a><strong>如何控制背压</strong></h3><ul>
<li>如何做到挂很久之后重新启动时限制拉取的消息量？（类似spark.streaming.kafka.maxRatePerPartition）<ul>
<li>背压通过task slot 的stackTrace判断</li>
<li>可以在kafka source那层控制一次性消费量，类似于spark</li>
</ul>
</li>
</ul>
<h2 id="Flink-高性能部署"><a href="#Flink-高性能部署" class="headerlink" title="Flink 高性能部署"></a>Flink 高性能部署</h2><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-master/ops/jobmanager_high_availability.html" target="_blank" rel="noopener">Apache Flink 1.8-SNAPSHOT Documentation: JobManager High Availability (HA)</a></li>
</ul>
<h2 id="metric监控rest-api"><a href="#metric监控rest-api" class="headerlink" title="metric监控rest api"></a>metric监控rest api</h2><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-master/monitoring/rest_api.html" target="_blank" rel="noopener">Apache Flink 1.8-SNAPSHOT Documentation: Monitoring REST API</a></li>
</ul>
<h2 id="Restart-Strategies"><a href="#Restart-Strategies" class="headerlink" title="Restart Strategies"></a>Restart Strategies</h2><ul>
<li><strong>doc</strong> <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/restart_strategies.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Restart Strategies</a></li>
<li>Fixed Delay Restart Strategy</li>
<li>Failure Rate Restart Strategy</li>
<li>No Restart Strategy</li>
<li>Fallback Restart Strategy</li>
</ul>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://ol7zjjc80.bkt.clouddn.com/271524552778_.pic.jpg&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yoursite.com/2018/12/20/Flink实战总结/" data-id="ck979jwi400036f16jgp11u2g" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACOElEQVR42u3a0WrkMAwF0P7/T89C6cPC7CT3yulCnOOnkCaxzxSELOvrKx6v7/Hp/t9/fb9+H8ezHD+zNDAwMG7LeB2OZIL3+7OlH8978k0MDIwHMI4XdxxYPz2fLz2f5eN9DAwMjCCkJkE5D+4YGBgY1zKSbe1xwtcGbgwMjGcy8iCYF8jWg+mv7MUxMDBuyGgPBv7n9S+eb2BgYNyE8SrH+lvXrufnmxgYGFszkjJ923iRL7R99yQZxcDA2JRx7aKTwtmsDHeyBgwMjK0ZSXkrKavlwbH9ZlGww8DA2JQxa4xIimVJKf+qDTMGBsbejDYIzhq58oW2YRoDAwNjPYFLfpS87Wwp4GJgYNyckYfF9VaMNhzXZTgMDIxNGbOkbb2dYsYbNltgYGDclpEcOs4SwfwgoS3qRQcDGBgYmzLabeQKL08ro20wBgbG4xltiJy1TbRtHxgYGE9jtKlYG3bzhQ6TTgwMjK0ZbZls1pAxS/7a+xgYGHsz8lDYpnSzQl5+5+R/goGBsQUjL4TN2iNWksKiyQMDA+MxjDYFbNPHNgGtjzAxMDA2ZbRlsjYgJgtt3z35AgYGxnaMVzna/fFxsSwpqEUhGwMDY2vGVYGybRSbXedlOwwMjP0YbdPDrLjfUutkFAMD4wGMpHw2K/HPmiqSrew/Ai4GBsbjGbO21LZ81v5kGBgYGMmUeTBdOS49eR4DA+MBjNkmdr183x4VYGBgPJOxcjDQJoUrzRx50omBgbER4w+UJBQZh/HJjgAAAABJRU5ErkJggg==" class="article-share-link">分享</a><div class="tags"></div><div class="post-nav"><a href="/2019/01/12/【spark-tips】spark2-4-0触发的executor内存溢出排查/" class="pre">【spark-tips】spark2.4.0触发的executor内存溢出排查</a><a href="/2018/10/15/Spark学习笔记/" class="next">Spark实战总结</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8yOTg3MC82NDM1"><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/ReentrantLock/" style="font-size: 15px;">ReentrantLock</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/架构/" style="font-size: 15px;">架构</a> <a href="/tags/Hbase/" style="font-size: 15px;">Hbase</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Yarn/" style="font-size: 15px;">Yarn</a> <a href="/tags/InnoDB/" style="font-size: 15px;">InnoDB</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/成长/" style="font-size: 15px;">成长</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/Hadoop-Yarn/" style="font-size: 15px;">Hadoop - Yarn</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/kylin/" style="font-size: 15px;">kylin</a> <a href="/tags/infrastructure/" style="font-size: 15px;">infrastructure</a> <a href="/tags/data/" style="font-size: 15px;">data</a> <a href="/tags/分布式/" style="font-size: 15px;">分布式</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/paper/" style="font-size: 15px;">paper</a> <a href="/tags/多线程/" style="font-size: 15px;">多线程</a> <a href="/tags/BigData/" style="font-size: 15px;">BigData</a> <a href="/tags/spring/" style="font-size: 15px;">spring</a> <a href="/tags/ioc/" style="font-size: 15px;">ioc</a> <a href="/tags/aop/" style="font-size: 15px;">aop</a> <a href="/tags/源码/" style="font-size: 15px;">源码</a> <a href="/tags/事务处理/" style="font-size: 15px;">事务处理</a> <a href="/tags/学习/" style="font-size: 15px;">学习</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/kubernetes-源码/" style="font-size: 15px;">kubernetes - 源码</a> <a href="/tags/Kylin/" style="font-size: 15px;">Kylin</a> <a href="/tags/kylin-Java-源码/" style="font-size: 15px;">kylin - Java - 源码</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Crawler/" style="font-size: 15px;">Crawler</a> <a href="/tags/superset/" style="font-size: 15px;">superset</a> <a href="/tags/二次开发/" style="font-size: 15px;">二次开发</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/Spring/" style="font-size: 15px;">Spring</a> <a href="/tags/java8/" style="font-size: 15px;">java8</a> <a href="/tags/sqlGenerator/" style="font-size: 15px;">sqlGenerator</a> <a href="/tags/函数式编程/" style="font-size: 15px;">函数式编程</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/04/20/kubernetes-federation深度解析/">kubernetes federation深度解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/07/MR任务在Hadoop子系统中状态流转/">MR任务在Hadoop子系统中状态流转</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/05/Yarn-Federation源码串读/">Yarn Federation源码串读</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/05/Hadoop-Rpc源码分析/">Hadoop Rpc源码分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/10/【Spark源码分析】Job提交执行过程详解/">【Spark源码分析】Job提交执行过程详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/02/【Spark源码分析】Broadcast/">【Spark源码分析】Broadcast</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/【Spark源码分析】Dynamic-Resource-Allocation设计的思考/">【Spark源码分析】Dynamic Resource Allocation设计的思考</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/01/Raft论文学习/">Raft论文学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/12/【spark-tips】spark2-4-0触发的executor内存溢出排查/">【spark-tips】spark2.4.0触发的executor内存溢出排查</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/Flink实战总结/">Flink实战总结</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/chao-li-11/activities" title="知乎" target="_blank">知乎</a><ul></ul><a href="http://weibo.com/3101672623/profile?topnav=1&amp;wvr=6" title="微博" target="_blank">微博</a><ul></ul><a href="https://github.com/lichaojacobs" title="GitHub" target="_blank">GitHub</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">CHAO LI's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?4ca08f1c48fe3bf3d0e2bfb54473d985## Your Baidu Analytics tracking id, e.g. 8006843039519956000";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>