<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content><title>airflow实战总结 | CHAO LI's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">airflow实战总结</h1><a id="logo" href="/.">CHAO LI's Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">airflow实战总结</h1><div class="post-meta">Aug 30, 2018<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="post-content"><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>airflow是一款开源的，分布式任务调度框架，它将一个具有上下级依赖关系的工作流，组装成一个有向无环图。</p>
<ul>
<li>特点:<ul>
<li>分布式任务调度：允许一个工作流的task在多台worker上同时执行</li>
<li>可构建任务依赖：以有向无环图的方式构建任务依赖关系</li>
<li>task原子性：工作流上每个task都是原子可重试的，一个工作流某个环节的task失败可自动或手动进行重试，不必从头开始任务</li>
</ul>
</li>
<li><p>工作流示意图</p>
<p>  <img src="https://pic4.zhimg.com/v2-fbd8d77be2eda3c9766c300359e8eba3_1200x500.jpg" alt="airflow-dags"></p>
<ul>
<li>一个dag表示一个定时的工作流，包含一个或者多个具有依赖关系的task</li>
</ul>
</li>
<li><p>task依赖图</p>
<p>  <img src="https://pic1.zhimg.com/80/v2-57deb1228a73c290c666539bc56ee8ac_hd.jpg" alt="airflow-tasks"></p>
</li>
<li><p>架构图及集群角色</p>
<p>  <img src="https://pic1.zhimg.com/80/v2-35a160b63e7389fe12f451e299ab0c00_hd.jpg" alt="airflow-infra"></p>
<ul>
<li>webserver : 提供web端服务，以及会定时生成子进程去扫描对应的目录下的dags，并更新数据库</li>
<li>scheduler : 任务调度服务，根据dags生成任务，并提交到消息中间件队列中 (redis或rabbitMq)</li>
<li>celery worker : 分布在不同的机器上，作为任务真正的的执行节点。通过监听消息中间件: redis或rabbitMq 领取任务</li>
<li>flower : 监控worker进程的存活性，启动或关闭worker进程，查看运行的task</li>
</ul>
</li>
</ul>
<h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><ul>
<li><p>构建docker镜像</p>
<ul>
<li><p>采用的airflow是未发行的1.10.0版本，原因是从1.10.0开始，支持时区的设置，而不是统一的UTC</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">//self.registry.domain 为docker私有镜像仓库</span><br><span class="line">//self.mvn.registry.com maven 私有镜像仓库</span><br><span class="line">//data0 为数据目录，data1为日志目录，运维统一配置日志清楚策略</span><br><span class="line">#docker build --network host -t self.registry.domain/airflow_base_1.10.7:1.0.0 .</span><br><span class="line">FROM self.registry.domain/airflow/centos_base_7.4.1708:1.0.0</span><br><span class="line">LABEL AIRFLOW=1.10.7</span><br><span class="line"></span><br><span class="line">ARG CELERY_REDIS=4.1.1</span><br><span class="line">ARG DOCKER_VERSION=1.13.1</span><br><span class="line">ARG AIRFLOW_VERSION=1.10.7</span><br><span class="line"></span><br><span class="line">ADD sbin /data0/airflow/sbin</span><br><span class="line"></span><br><span class="line">ENV SLUGIFY_USES_TEXT_UNIDECODE=yes \</span><br><span class="line">    #如果构建镜像的机器需要代理才能连接外网的话，配置https_proxy</span><br><span class="line">    https_proxy=https://ip:port </span><br><span class="line"></span><br><span class="line">RUN curl http://self.mvn.registry.com/python/python-3.5.6.jar -o /tmp/Python-3.5.6.tgz &amp;&amp; \</span><br><span class="line">    curl http://self.mvn.registry.com/airflow/$&#123;AIRFLOW_VERSION&#125;/airflow-$&#123;AIRFLOW_VERSION&#125;.jar -o /tmp/incubator-airflow-$&#123;AIRFLOW_VERSION&#125;.tar.gz &amp;&amp; \</span><br><span class="line">    curl http:/self.mvn.registry.com/docker/$&#123;DOCKER_VERSION&#125;/docker-$&#123;DOCKER_VERSION&#125;.jar -o /tmp/docker-$&#123;DOCKER_VERSION&#125;.tar.gz &amp;&amp; \</span><br><span class="line">    tar zxf /tmp/docker-$&#123;DOCKER_VERSION&#125;.tar.gz -C /data0/software &amp;&amp; \</span><br><span class="line">    tar zxf /tmp/Python-3.5.6.tgz -C /data0/software &amp;&amp; \</span><br><span class="line">    tar zxf /tmp/incubator-airflow-$&#123;AIRFLOW_VERSION&#125;.tar.gz -C /data0/software &amp;&amp; \</span><br><span class="line">    yum install -y libtool-ltdl policycoreutils-python &amp;&amp; \</span><br><span class="line">    rpm -ivh --force --nodeps /data0/software/docker-$&#123;DOCKER_VERSION&#125;/docker-engine-selinux-$&#123;DOCKER_VERSION&#125;-1.el7.centos.noarch.rpm &amp;&amp; \</span><br><span class="line">    rpm -ivh --force --nodeps /data0/software/docker-$&#123;DOCKER_VERSION&#125;/docker-engine-$&#123;DOCKER_VERSION&#125;-1.el7.centos.x86_64.rpm &amp;&amp; \</span><br><span class="line">    yum -y install gcc &amp;&amp; yum -y install gcc-c++ &amp;&amp; yum -y install make &amp;&amp; \</span><br><span class="line">    yum -y install zlib-devel mysql-devel python-devel cyrus-sasl-devel cyrus-sasl-lib libxml2-devel libxslt-devel &amp;&amp; \</span><br><span class="line">    cd /data0/software/Python-3.5.6 &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make install &amp;&amp; \</span><br><span class="line">    ln -sf /usr/local/bin/pip3 /usr/local/bin/pip &amp;&amp; \</span><br><span class="line">    ln -sf /usr/local/bin/python3 /usr/local/bin/python &amp;&amp; \</span><br><span class="line">    cd /data0/software/incubator-airflow-$&#123;AIRFLOW_VERSION&#125; &amp;&amp; python setup.py install &amp;&amp; \</span><br><span class="line">    pip install -i https://pypi.douban.com/simple/ apache-airflow[crypto,celery,hive,jdbc,mysql,hdfs,password,redis,devel_hadoop] &amp;&amp; \</span><br><span class="line">    pip install -i https://pypi.douban.com/simple/ celery[redis]==$CELERY_REDIS &amp;&amp; \</span><br><span class="line">    pip install -i https://pypi.douban.com/simple/ docutils &amp;&amp; \</span><br><span class="line">    ln -sf /usr/local/lib/python3.5/site-packages/apache_airflow-1.10.0-py3.5.egg/airflow /data0/software/airflow &amp;&amp; \</span><br><span class="line">    mkdir -p /data0/airflow/bin &amp;&amp; \</span><br><span class="line">    ln -sf /data0/airflow/sbin/airflow-200.sh /data0/airflow/bin/200.sh &amp;&amp; \</span><br><span class="line">    ln -sf /data0/airflow/sbin/airflow-503.sh /data0/airflow/bin/503.sh &amp;&amp; \</span><br><span class="line">    chown -R root:root /data0/software/ &amp;&amp; \</span><br><span class="line">    chown -R root:root /data0/airflow/ &amp;&amp; \</span><br><span class="line">    chmod -R 775 /data0/airflow/sbin/* &amp;&amp; \</span><br><span class="line">    chmod -R 775 /data0/airflow/bin/* &amp;&amp; \</span><br><span class="line">    echo &apos;source /data0/airflow/sbin/init-airflow.sh&apos; &gt;&gt; ~/.bashrc &amp;&amp; \</span><br><span class="line">    rm -rf /tmp/* /data0/software/Python-3.5.6 /data0/software/incubator-airflow-$&#123;AIRFLOW_VERSION&#125; /data0/software/docker-$&#123;DOCKER_VERSION&#125;</span><br><span class="line">    </span><br><span class="line">ENV PATH=$PATH:/data0/software/jdk/bin:/data0/software/airflow/bin:/data0/airflow/sbin/:/data0/airflow/sbin/airflow/:/data0/airflow/bin/</span><br><span class="line"></span><br><span class="line">WORKDIR /data0/airflow/bin/</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过docker 启动容器的话需要暴露几个端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">webserver: 8081</span><br><span class="line">worker: 8793</span><br><span class="line">flower: 5555</span><br><span class="line">//启动示例</span><br><span class="line">docker run --name airflow -it -d --privileged --net=host -p 8081:8081 -p 5555:5555 -p 8793:8793 -v /var/run/docker.sock:/var/run/docker.sock -v /data1:/data1 -v /data0/airflow:/data0/airflow self.registry.domain/airflow_1.10.7:1.0.0</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>airflow 升级到未release的1.10.0的版本</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//如果之前用的是低版本的话，需要执行</span><br><span class="line">airflow upgradedb 来更新迁移数据库的schema</span><br><span class="line">//执行之前首先需要set mysql property</span><br><span class="line">set global explicit_defaults_for_timestamp=1 //会提示is readonly variable</span><br><span class="line">需要在my.cnf中添加这个设置:explicit_defaults_for_timestamp=1 并重启mysql</span><br><span class="line">//update celery几个设置</span><br><span class="line">celeryd_concurrency -&gt; worker_concurrency</span><br><span class="line">celery_result_backend -&gt; result_backend</span><br></pre></td></tr></table></figure>
<ul>
<li><p>修改时区，以及界面上执行时间的显示(airlfow 默认界面上还是按照UTC显示)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//需要update configuration</span><br><span class="line">default_timezone = Etc/GMT-8</span><br><span class="line"></span><br><span class="line">//修改dags.html中的显示时间，使得界面上看起来方便</span><br><span class="line">// jinjia2 传入转换函数，在views.py 的homeview的render中 </span><br><span class="line">//（方法验证有点问题，再优化）</span><br><span class="line">def utc2local(utc):</span><br><span class="line">    epoch = time.mktime(utc.timetuple())</span><br><span class="line">    offset = datetime.fromtimestamp(epoch) - datetime.utcfromtimestamp(epoch)</span><br><span class="line">    return utc + offset</span><br><span class="line">utc2local(last_run.execution_date).strftime(&quot;%Y-%m-%d %H:%M&quot;)</span><br><span class="line">utc2local(last_run.start_date).strftime(&quot;%Y-%m-%d %H:%M&quot;)`</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>airflow plugins 定制化开发</p>
<ul>
<li><a href="https://airflow.apache.org/plugins.html" target="_blank" rel="noopener">官方文档</a></li>
<li>plugin 这个没法传给worker，还是得重新分发到各个worker节点，建议打入airflow基础镜像中</li>
<li>增加operator时需要重启webserver和scheduler</li>
</ul>
</li>
<li><p>由于dag的删除现在官方没有暴露直接的api,而完整的删除又牵扯到多个表，总结出删除dag的sql如下</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set @dag_id = &apos;BAD_DAG&apos;;</span><br><span class="line">delete from airflow.xcom where dag_id = @dag_id;</span><br><span class="line">delete from airflow.task_instance where dag_id = @dag_id;</span><br><span class="line">delete from airflow.sla_miss where dag_id = @dag_id;</span><br><span class="line">delete from airflow.log where dag_id = @dag_id;</span><br><span class="line">delete from airflow.job where dag_id = @dag_id;</span><br><span class="line">delete from airflow.dag_run where dag_id = @dag_id;</span><br><span class="line">delete from airflow.dag where dag_id = @dag_id;</span><br></pre></td></tr></table></figure>
</li>
<li><p>自己实现的200和503脚本，用于集群统一的上下线操作</p>
<ul>
<li><p>200脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">function usage() &#123;</span><br><span class="line">    echo -e &quot;\n A tool used for starting airflow services</span><br><span class="line">Usage: 200.sh &#123;webserver|worker|scheduler|flower&#125;</span><br><span class="line">&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PORT=8081</span><br><span class="line">ROLE=webserver</span><br><span class="line">ENV_ARGS=&quot;&quot;</span><br><span class="line">check_alive() &#123;</span><br><span class="line">    PID=`netstat -nlpt | grep $PORT | awk &apos;&#123;print $7&#125;&apos; | awk -F &quot;/&quot; &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">    [ -n &quot;$PID&quot; ] &amp;&amp; return 0 || return 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">check_scheduler_alive() &#123;</span><br><span class="line">    PIDS=`ps -ef | grep &quot;/usr/local/bin/airflow scheduler&quot; | grep &quot;python&quot; | awk &apos;&#123;print $2&#125;&apos;`</span><br><span class="line">    [ -n &quot;$PIDS&quot; ] &amp;&amp; return 0 || return 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function get_host_ip()&#123;</span><br><span class="line">    local host=$(ifconfig | grep &quot;inet &quot; | grep &quot;\-\-&gt;&quot; | awk &apos;&#123;print $2&#125;&apos; | tail -1)</span><br><span class="line">    if [[ -z &quot;$host&quot; ]]; then</span><br><span class="line">        host=$(ifconfig | grep &quot;inet &quot; | grep &quot;broadcast&quot; | awk &apos;&#123;print $2&#125;&apos; | tail -1)</span><br><span class="line">    fi</span><br><span class="line">    echo &quot;$&#123;host&#125;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">start_service() &#123;</span><br><span class="line">    if [ $ROLE = &apos;scheduler&apos; ];then</span><br><span class="line">        check_scheduler_alive</span><br><span class="line">    else</span><br><span class="line">        check_alive</span><br><span class="line">    fi</span><br><span class="line">    if [ $? -ne 0 ];then</span><br><span class="line">        nohup airflow $ROLE $ENV_ARGS &gt; $BASE_LOG_DIR/$ROLE/$ROLE.log 2&gt;&amp;1 &amp;</span><br><span class="line">        sleep 5</span><br><span class="line">        if [ $ROLE = &apos;scheduler&apos; ];then</span><br><span class="line">            check_scheduler_alive</span><br><span class="line">        else</span><br><span class="line">            check_alive</span><br><span class="line">        fi</span><br><span class="line">        if [ $? -ne 0 ];then</span><br><span class="line">            echo &quot;service start error&quot;</span><br><span class="line">            exit 1</span><br><span class="line">        else</span><br><span class="line">            echo &quot;service start success&quot;</span><br><span class="line">            exit 0</span><br><span class="line">        fi</span><br><span class="line">    else</span><br><span class="line">        echo &quot;service alreay started&quot;</span><br><span class="line">        exit 0</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">    if [ -z &quot;$&#123;POOL&#125;&quot; ]; then</span><br><span class="line">        echo &quot;the environment variable POOL cannot be empty&quot;</span><br><span class="line">        exit 1</span><br><span class="line">    fi</span><br><span class="line">    source /data0/hcp/sbin/init-hcp.sh</span><br><span class="line">    case &quot;$1&quot; in</span><br><span class="line">        webserver)</span><br><span class="line">            echo &quot;starting airflow webserver&quot;</span><br><span class="line">            ROLE=webserver</span><br><span class="line">            PORT=8081</span><br><span class="line">            start_service</span><br><span class="line">            ;;</span><br><span class="line">        worker)</span><br><span class="line">            echo &quot;starting airflow worker&quot;</span><br><span class="line">            ROLE=worker</span><br><span class="line">            PORT=8793</span><br><span class="line">            local host_ip=$(get_host_ip)</span><br><span class="line">            ENV_ARGS=&quot;-cn $&#123;host_ip&#125;@$&#123;host_ip&#125;&quot;</span><br><span class="line">            start_service</span><br><span class="line">            ;;</span><br><span class="line">        flower)</span><br><span class="line">            echo &quot;starting airflow flower&quot;</span><br><span class="line">            ROLE=flower</span><br><span class="line">            PORT=5555</span><br><span class="line">            start_service</span><br><span class="line">            ;;</span><br><span class="line">        scheduler)</span><br><span class="line">            echo &quot;starting airflow scheduler&quot;</span><br><span class="line">            ROLE=scheduler</span><br><span class="line">            start_service</span><br><span class="line">            ;;     </span><br><span class="line">        *)</span><br><span class="line">            usage</span><br><span class="line">            exit 1</span><br><span class="line">    esac</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>503脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">function usage() &#123;</span><br><span class="line">    echo -e &quot;\n A tool used for stop airflow services</span><br><span class="line">Usage: 200.sh &#123;webserver|worker|scheduler|flower&#125;</span><br><span class="line">&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function get_host_ip()&#123;</span><br><span class="line">    local host=$(ifconfig | grep &quot;inet &quot; | grep &quot;\-\-&gt;&quot; | awk &apos;&#123;print $2&#125;&apos; | tail -1)</span><br><span class="line">    if [[ -z &quot;$host&quot; ]]; then</span><br><span class="line">        host=$(ifconfig | grep &quot;inet &quot; | grep &quot;broadcast&quot; | awk &apos;&#123;print $2&#125;&apos; | tail -1)</span><br><span class="line">    fi</span><br><span class="line">    echo &quot;$&#123;host&#125;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">    if [ -z &quot;$&#123;POOL&#125;&quot; ]; then</span><br><span class="line">        echo &quot;the environment variable POOL cannot be empty&quot;</span><br><span class="line">        exit 1</span><br><span class="line">    fi</span><br><span class="line">    source /data0/hcp/sbin/init-hcp.sh</span><br><span class="line">    case &quot;$1&quot; in</span><br><span class="line">        webserver)</span><br><span class="line">            echo &quot;stopping airflow webserver&quot;</span><br><span class="line">            cat $AIRFLOW_HOME/airflow-webserver.pid | xargs kill -9</span><br><span class="line">            ;;</span><br><span class="line">        worker)</span><br><span class="line">            echo &quot;stopping airflow worker&quot;</span><br><span class="line">            PORT=8793</span><br><span class="line">            PID=`netstat -nlpt | grep $PORT | awk &apos;&#123;print $7&#125;&apos; | awk -F &quot;/&quot; &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">            kill -9 $PID</span><br><span class="line">            local host_ip=$(get_host_ip)</span><br><span class="line">            ps -ef | grep celeryd | grep $&#123;host_ip&#125;@$&#123;host_ip&#125; | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9</span><br><span class="line">            ;;</span><br><span class="line">        flower)</span><br><span class="line">            echo &quot;stopping airflow flower&quot;</span><br><span class="line">            PORT=5555</span><br><span class="line">            PID=`netstat -nlpt | grep $PORT | awk &apos;&#123;print $7&#125;&apos; | awk -F &quot;/&quot; &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">            kill -9 $PID</span><br><span class="line">            start_service</span><br><span class="line">            ;;</span><br><span class="line">        scheduler)</span><br><span class="line">            echo &quot;stopping airflow scheduler&quot;</span><br><span class="line">            PID=`ps -ef | grep &quot;/usr/local/bin/airflow scheduler&quot; | grep &quot;python&quot; | awk &apos;&#123;print $2&#125;&apos;`</span><br><span class="line">            kill -9 $PID</span><br><span class="line">            ;;     </span><br><span class="line">        *)</span><br><span class="line">            usage</span><br><span class="line">            exit 1</span><br><span class="line">    esac</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="遇到的坑以及定制化解决方案"><a href="#遇到的坑以及定制化解决方案" class="headerlink" title="遇到的坑以及定制化解决方案"></a>遇到的坑以及定制化解决方案</h2><ul>
<li><p>问题1: airflow worker 角色不能使用根用户启动</p>
<ul>
<li><p>原因：不能用根用户启动的根本原因，在于airflow的worker直接用的celery，而celery 源码中有参数默认不能使用ROOT启动，否则将报错, <a href="http://docs.celeryproject.org/en/latest/_modules/celery/platforms.html" target="_blank" rel="noopener">源码链接</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">C_FORCE_ROOT = os.environ.get(&apos;C_FORCE_ROOT&apos;, False)</span><br><span class="line"></span><br><span class="line">ROOT_DISALLOWED = &quot;&quot;&quot;\</span><br><span class="line">Running a worker with superuser privileges when the</span><br><span class="line">worker accepts messages serialized with pickle is a very bad idea!</span><br><span class="line"></span><br><span class="line">If you really want to continue then you have to set the C_FORCE_ROOT</span><br><span class="line">environment variable (but please think about this before you do).</span><br><span class="line"></span><br><span class="line">User information: uid=&#123;uid&#125; euid=&#123;euid&#125; gid=&#123;gid&#125; egid=&#123;egid&#125;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">ROOT_DISCOURAGED = &quot;&quot;&quot;\</span><br><span class="line">You&apos;re running the worker with superuser privileges: this is</span><br><span class="line">absolutely not recommended!</span><br><span class="line"></span><br><span class="line">Please specify a different user using the --uid option.</span><br><span class="line"></span><br><span class="line">User information: uid=&#123;uid&#125; euid=&#123;euid&#125; gid=&#123;gid&#125; egid=&#123;egid&#125;</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>解决方案一：修改airlfow源码，在celery_executor.py中强制设置C_FORCE_ROOT</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from celery import Celery, platforms </span><br><span class="line">在app = Celery(…)后新增 </span><br><span class="line">platforms.C_FORCE_ROOT = True</span><br><span class="line">重启即可</span><br></pre></td></tr></table></figure>
</li>
<li><p>解决方案二：在容器初始化环境变量的时候，设置C_FORCE_ROOT参数，以零侵入的方式解决问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#强制celery worker运行采用root模式</span><br><span class="line">export C_FORCE_ROOT=True</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>问题2: docker in docker</p>
<ul>
<li>在dags中以docker方式调度任务时，为了container的轻量话，不做重型的docker pull等操作，我们利用了docker cs架构的设计理念，只需要将宿主机的/var/run/docker.sock文件挂载到容器目录下即可 <a href="http://wangbaiyuan.cn/docker-in-docker.html#prettyPhoto" target="_blank" rel="noopener">docker in docker 资料</a></li>
</ul>
</li>
<li><p>问题3: 由于我们运行airlfow的机器是高配机器切分的虚机，host并非是传统的ip段，多节点执行后无法在master节点上通过worker节点提供的日志服务获取执行日志</p>
<ul>
<li><p>查看celery源码(celery/celery/worker/worker.py)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from celery.utils.nodenames import default_nodename, worker_direct</span><br><span class="line">self.hostname = default_nodename(hostname)</span><br><span class="line">// 查看default_nodename方法</span><br><span class="line">def default_nodename(hostname):</span><br><span class="line">    &quot;&quot;&quot;Return the default nodename for this process.&quot;&quot;&quot;</span><br><span class="line">    name, host = nodesplit(hostname or &apos;&apos;)</span><br><span class="line">    return nodename(name or NODENAME_DEFAULT, host or gethostname())</span><br><span class="line"></span><br><span class="line">//默认在worker.py 的构造方法中没有传入hostname 所以在celery nodenames.py中default_nodename方法里面调用了gethostname</span><br><span class="line">//可以看到gethostname的实现，调用了socket.gethostname，这个直接得到了虚拟机的host</span><br><span class="line">gethostname = memoize(1, Cache=dict)(socket.gethostname)</span><br></pre></td></tr></table></figure>
</li>
<li><p>解决方案：发现airflow worker的启动命令中其实提供了设置celery host name的参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">airflow worker -cn=ip@ip</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>问题4: 多个worker节点进行调度反序列化dag执行的时候，报找不到module的错误</p>
<ul>
<li><p>当时考虑到文件更新的一致性，采用所有worker统一执行master下发的序列化dag的方案，而不依赖worker节点上实际的dag文件，开启这一特性操作如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">worker节点上： airflow worker -cn=ip@ip -p //-p为开关参数，意思是以master序列化的dag作为执行文件，而不是本地dag目录中的文件</span><br><span class="line">master节点上： airflow scheduler -p</span><br></pre></td></tr></table></figure>
</li>
<li><p>错误原因在于远程的worker节点上不存在实际的dag文件，反序列化的时候对于当时在dag中定义的函数或对象找不到module_name</p>
</li>
<li>解决方案一：在所有的worker节点上同时发布dags目录，缺点是dags一致性成问题</li>
<li><p>解决方案二：修改源码中序列化与反序列化的逻辑，主体思路还是替换掉不存在的module为main。修改如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">//models.py 文件，对 class DagPickle(Base) 定义修改</span><br><span class="line">import dill</span><br><span class="line">class DagPickle(Base):</span><br><span class="line">id = Column(Integer, primary_key=True)</span><br><span class="line"># 修改前: pickle = Column(PickleType(pickler=dill))</span><br><span class="line">pickle = Column(LargeBinary)</span><br><span class="line">created_dttm = Column(UtcDateTime, default=timezone.utcnow)</span><br><span class="line">pickle_hash = Column(Text)</span><br><span class="line"></span><br><span class="line">__tablename__ = &quot;dag_pickle&quot;</span><br><span class="line">def __init__(self, dag):</span><br><span class="line">    self.dag_id = dag.dag_id</span><br><span class="line">    if hasattr(dag, &apos;template_env&apos;):</span><br><span class="line">        dag.template_env = None</span><br><span class="line">    self.pickle_hash = hash(dag)</span><br><span class="line">    raw = dill.dumps(dag)</span><br><span class="line">    # 修改前: self.pickle = dag</span><br><span class="line">    reg_str = &apos;unusual_prefix_\w*&#123;0&#125;&apos;.format(dag.dag_id)</span><br><span class="line">    result = re.sub(str.encode(reg_str), b&apos;__main__&apos;, raw)</span><br><span class="line">    self.pickle =result</span><br><span class="line"></span><br><span class="line">//cli.py 文件反序列化逻辑 run(args, dag=None) 函数</span><br><span class="line">// 直接通过dill来反序列化二进制文件，而不是通过PickleType 的result_processor做中转</span><br><span class="line">修改前: dag = dag_pickle.pickle</span><br><span class="line">修改后：dag = dill.loads(dag_pickle.pickle)</span><br></pre></td></tr></table></figure>
</li>
<li><p>解决方案三：源码零侵入，使用python的types.FunctionType重新创建一个不带module的function，这样序列化与反序列化的时候不会有问题（待验证)</p>
<ul>
<li>注意，使用types.FunctionType的方式装饰函数时，由于所有的引用都会从golbals里面找，所以对于module的导入，建议在被装饰的函数里面变成local的方式引入</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//这里把globals()传入是为了把builtlins等一些模块传入，省事</span><br><span class="line">new_func = types.FunctionType((lambda df: df.iloc[:, 0].size == xx).__code__, globals())</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>问题5：由于airflow在master查看task执行日志是通过各个节点的http服务获取的，但是存入task_instance表中的host_name不是ip，可见获取hostname的方式有问题.</p>
<ul>
<li><p>解决方案：修改airflow/utils/net.py 中get_hostname函数，添加优先获取环境变量中设置的hostname的逻辑</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">//models.py TaskInstance</span><br><span class="line">self.hostname = get_hostname()</span><br><span class="line">//net.py 在get_hostname里面加入一个获取环境变量的逻辑</span><br><span class="line">import os</span><br><span class="line">def get_hostname():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Fetch the hostname using the callable from the config or using</span><br><span class="line">    `socket.getfqdn` as a fallback.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 尝试获取环境变量</span><br><span class="line">    if &apos;AIRFLOW_HOST_NAME&apos; in os.environ:</span><br><span class="line">        return os.environ[&apos;AIRFLOW_HOST_NAME&apos;]</span><br><span class="line">    # First we attempt to fetch the callable path from the config.</span><br><span class="line">    try:</span><br><span class="line">        callable_path = conf.get(&apos;core&apos;, &apos;hostname_callable&apos;)</span><br><span class="line">    except AirflowConfigException:</span><br><span class="line">        callable_path = None</span><br><span class="line"></span><br><span class="line">    # Then we handle the case when the config is missing or empty. This is the</span><br><span class="line">    # default behavior.</span><br><span class="line">    if not callable_path:</span><br><span class="line">        return socket.getfqdn()</span><br><span class="line"></span><br><span class="line">    # Since we have a callable path, we try to import and run it next.</span><br><span class="line">    module_path, attr_name = callable_path.split(&apos;:&apos;)</span><br><span class="line">    module = importlib.import_module(module_path)</span><br><span class="line">    callable = getattr(module, attr_name)</span><br><span class="line">    return callable()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://ol7zjjc80.bkt.clouddn.com/271524552778_.pic.jpg&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yoursite.com/2018/08/30/airflow实战总结/" data-id="ck88bdhyz000jttyheymbv9yd" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACLElEQVR42u3aQQ7CMAwF0d7/0kViCynzbUCqM1khBGleF5Zj+zjwOp+Lf3Mu1uq/r/86frFkyJBxW8Z5uVZH5J/TI5LX8WYfGTJkbMDg25FD831Wx+VnkyFDhgxOuv6evA7+gmTIkCGDpGhx6tZOQ2XIkCGDbM1hfP/0KV+4i8uQIeOGjLQx8M/PP+xvyJAh4yaMM1ydx/cTzeWpZMiQMZrBBylqTUreEuiU52TIkDGbUbsu8qP8aURDhgwZWzL4hbY2ltEpnAUBV4YMGSMYaUGt34wkF930RciQIWM2I21DEkZtjIyH4GWhTYYMGUMZvDNYK8ylhTOCiZudMmTIuDmD/JQH2VrZLk1D33wjQ4aMbRj8UtofquAlPPQKZMiQMZpRG5XgIxQ81euEfhkyZOzAqJXDeCCuNUrTa7MMGTKmMtKjXIfXtGDXrxF+yHBlyJAxgpGmfZ2BiX4q+eHpMmTIGM3gwxN8nKK2c+0qK0OGjB0YvChfC8EkKaw1Boq9BRkyZNyKwQtqPOzW2pl88CJIB2XIkDGIUbuO1o7+3T1lyJCxAyMd3uoEa5J01p4oQ4aM2YwzXGnSxktstSRShgwZOzD4IslZh0r24WFdhgwZ8xhpkE1/mVbJioFehgwZGzDSpiN5KTx890OzDBkyZPARsU47M22mFgOuDBkytmR0wmXaEoj7sTJkyBjEqBXr07I+iZDFnWXIkDGaUWsMdEJkWpJDAVeGDBkzGQ8gVvAuOX6CIwAAAABJRU5ErkJggg==" class="article-share-link">分享</a><div class="tags"></div><div class="post-nav"><a href="/2018/10/15/Spark学习笔记/" class="pre">Spark实战总结</a><a href="/2018/08/12/impala集群搭建/" class="next">impala集群搭建</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8yOTg3MC82NDM1"><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/paper/" style="font-size: 15px;">paper</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/架构/" style="font-size: 15px;">架构</a> <a href="/tags/Hbase/" style="font-size: 15px;">Hbase</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Yarn/" style="font-size: 15px;">Yarn</a> <a href="/tags/InnoDB/" style="font-size: 15px;">InnoDB</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/成长/" style="font-size: 15px;">成长</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/kylin/" style="font-size: 15px;">kylin</a> <a href="/tags/infrastructure/" style="font-size: 15px;">infrastructure</a> <a href="/tags/data/" style="font-size: 15px;">data</a> <a href="/tags/Hadoop-Yarn/" style="font-size: 15px;">Hadoop - Yarn</a> <a href="/tags/多线程/" style="font-size: 15px;">多线程</a> <a href="/tags/ReentrantLock/" style="font-size: 15px;">ReentrantLock</a> <a href="/tags/分布式/" style="font-size: 15px;">分布式</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/BigData/" style="font-size: 15px;">BigData</a> <a href="/tags/学习/" style="font-size: 15px;">学习</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/spring/" style="font-size: 15px;">spring</a> <a href="/tags/ioc/" style="font-size: 15px;">ioc</a> <a href="/tags/aop/" style="font-size: 15px;">aop</a> <a href="/tags/源码/" style="font-size: 15px;">源码</a> <a href="/tags/Kylin/" style="font-size: 15px;">Kylin</a> <a href="/tags/事务处理/" style="font-size: 15px;">事务处理</a> <a href="/tags/kylin-Java-源码/" style="font-size: 15px;">kylin - Java - 源码</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Crawler/" style="font-size: 15px;">Crawler</a> <a href="/tags/superset/" style="font-size: 15px;">superset</a> <a href="/tags/二次开发/" style="font-size: 15px;">二次开发</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/Spring/" style="font-size: 15px;">Spring</a> <a href="/tags/java8/" style="font-size: 15px;">java8</a> <a href="/tags/sqlGenerator/" style="font-size: 15px;">sqlGenerator</a> <a href="/tags/函数式编程/" style="font-size: 15px;">函数式编程</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/01/07/MR任务在Hadoop子系统中状态流转/">MR任务在Hadoop子系统中状态流转</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/05/Yarn-Federation源码串读/">Yarn Federation源码串读</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/05/Hadoop-Rpc源码分析/">Hadoop Rpc源码分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/10/【Spark源码分析】Job提交执行过程详解/">【Spark源码分析】Job提交执行过程详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/02/【Spark源码分析】Broadcast/">【Spark源码分析】Broadcast</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/【Spark源码分析】Dynamic-Resource-Allocation设计的思考/">【Spark源码分析】Dynamic Resource Allocation设计的思考</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/01/Raft论文学习/">Raft论文学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/12/【spark-tips】spark2-4-0触发的executor内存溢出排查/">【spark-tips】spark2.4.0触发的executor内存溢出排查</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/Flink实战总结/">Flink实战总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/15/Spark学习笔记/">Spark实战总结</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/chao-li-11/activities" title="知乎" target="_blank">知乎</a><ul></ul><a href="http://weibo.com/3101672623/profile?topnav=1&amp;wvr=6" title="微博" target="_blank">微博</a><ul></ul><a href="https://github.com/lichaojacobs" title="GitHub" target="_blank">GitHub</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">CHAO LI's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?4ca08f1c48fe3bf3d0e2bfb54473d985## Your Baidu Analytics tracking id, e.g. 8006843039519956000";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>