<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Yarn Federation源码串读</title>
      <link href="/2019/11/05/Yarn-Federation%E6%BA%90%E7%A0%81%E4%B8%B2%E8%AF%BB/"/>
      <url>/2019/11/05/Yarn-Federation%E6%BA%90%E7%A0%81%E4%B8%B2%E8%AF%BB/</url>
      <content type="html"><![CDATA[<h2 id="Federation架构总览"><a href="#Federation架构总览" class="headerlink" title="Federation架构总览"></a>Federation架构总览</h2><ul><li>Federation: 主要有四个模块，Router ，StateStore，AMRMProxy, Global Policy Generator；从架构上来看，有点类似于后端的微服务架构中<strong>服务注册发现</strong>模块</li></ul><p><img src="https://pic4.zhimg.com/v2-7ee20bc86d8be49d25b5ca3897d3278f_b.png" alt="img"></p><h2 id="Router模块"><a href="#Router模块" class="headerlink" title="Router模块"></a>Router模块</h2><ul><li>类似于微服务的网关模块；通过state store获取具体的集群配置策略，将client端submit请求转发到对应的subCluster中</li><li>代码结构</li><li>hadoop-yarn-server-router：router组件核心实现，分为对接admin用户的协议和client用户协议，以及web server三个子模块实现  </li></ul><p><img src="https://pic3.zhimg.com/v2-b6ee24339266083c97b6642c4f3a081e_b.png" alt="img"></p><ul><li>hadoop-yarn-server-common-federation-router：包含了Router的各种Policy，具体控制router给子集群分配app的策略</li></ul><p><img src="https://pic2.zhimg.com/v2-e7402699fce4808743375957f68a8b11_b.png" alt="img"></p><h3 id="Router-clientrm"><a href="#Router-clientrm" class="headerlink" title="Router- clientrm"></a><strong>Router- clientrm</strong></h3><ul><li>负责接收客户端命令请求，并根据对应router具体配置的policy将客户端请求转发到HomeSubcluster上</li><li>在每一个router服务上随着启动，用来监听客户端作业提交，实现了Client与RM沟通的RPC协议接口(ApplicationClientProtocol)；作为client的proxy，执行一系列的chain interceptor），通常FederationClientInterceptor需作为最后一个拦截器</li><li>当然RouterClientRMService某种程度上针对的是Server测，取代原来RM侧<strong>RMClientService</strong>；在客户端具体的调用还是在<strong>YarnClientImpl</strong>；之间通过RPC通信</li><li>初始化： 获取配置文件中配置的拦截器，默认是DefaultClientRequestInterceptor  </li></ul><p><img src="https://pic3.zhimg.com/v2-74b97de7e4a90be9f4f7f5e703dcbd56_b.png" alt="img"></p><ul><li>DefaultClientRequestInterceptor只是做了简单的请求透明转发；没涉及到多子集群的处理</li><li>FederationClientInterceptor：面向client，隐藏了多个sub cluster RM；但是目前只实现了四个接口：<strong>getNewApplication, submitApplication, forceKillApplication and getApplicationReport</strong></li><li><strong>FederationClientInterceptor</strong></li><li>clientRMProxies: 子集群id与对应的通信client的key value集合</li><li>federationFacade: 对应的state store具体实现</li><li>policyFacade: 路由策略的工厂  </li></ul><p><img src="https://pic1.zhimg.com/v2-d6bbfd466b88388bdb9777d17d159210_b.png" alt="img"></p><ul><li>一个任务的提交需经过<strong>FederationClientInterceptor.getNewApplication</strong>和<strong>submitApplication</strong>接口，前者获得新的<strong>applicationId</strong>, 后者通过获得的<strong>applicationId</strong>将任务提交到具体的sub Cluster RM；这一个阶段没有经过与state store的写操作</li></ul><p><img src="https://pic2.zhimg.com/v2-6824ab1d9c5489ea0264ef5b79f9f075_b.png" alt="img"></p><ul><li>getNewApplication实现只是<strong>随机</strong>的选择一个active sub cluster来获取一个新的<strong>applicationId</strong>；而subClustersActive是通过具体实现的<strong>state store</strong>来获取，此处有过滤active的字段</li><li>submitApplication，方法注释有讨论各种failover的处理情况；</li><li>RM没挂的情况：如果state store 更新成功了，则多次提交任务都是幂等的</li><li>RM挂了：则router time out之后重试，选择其他的sub cluster</li><li>Client挂了：跟原来的/ClientRMService/一样</li><li>通过policyFacade加载策略，根据context与blacklist为当前提交选择sub cluster；具体逻辑在<strong>FederationRouterPolicy.getHomeSubcluster</strong>  </li></ul><p><img src="https://pic4.zhimg.com/v2-c6dbd82e45d5c69bd30b07e4f7e077a3_b.png" alt="img"></p><ul><li>同步提交任务至目标sub cluster</li></ul><p><img src="https://pic3.zhimg.com/v2-da2fa220baae8ba7238a1b77bebe009a_b.png" alt="img"></p><p><strong>疑问&amp;&amp;待确定的点</strong></p><ul><li>client —&gt; router —&gt; rm： 这条链路如果router挂了如何failover；<strong>在submitApplication方法上方有较为详细的边界情况处理解释</strong></li><li><strong>是否支持多个router？以及在配置中如何指定多个router？防止一个router挂掉的情况</strong></li><li><strong>需要确定是否有机制来维系真正存活的cluster，是否会动态摘除down掉的RM</strong></li></ul><h2 id="Policy-State-Store模块"><a href="#Policy-State-Store模块" class="headerlink" title="Policy State Store模块"></a>Policy State Store模块</h2><h3 id="FederationStateStoreFacade"><a href="#FederationStateStoreFacade" class="headerlink" title="FederationStateStoreFacade"></a>FederationStateStoreFacade</h3><ul><li>作为statestore的封装，抽象出一些重试和缓存的逻辑</li></ul><h3 id="FederationStateStore"><a href="#FederationStateStore" class="headerlink" title="FederationStateStore"></a>FederationStateStore</h3><ul><li>一般采用<strong>ZookeeperFederationStateStore</strong>的方式</li><li><strong>ZookeeperFederationStateStore</strong>  实现中，对应的数据存储结构如下  </li></ul><p><img src="https://pic4.zhimg.com/v2-b4e79639629bdec688ec4efb6f9b275f_b.png" alt="img"></p><ul><li>通过心跳维系了RM是否是active；通过<strong>filterInactiveSubClusters</strong>来决定是否需要过滤存活的RM</li></ul><p><img src="https://pic1.zhimg.com/v2-1de1b20d5b5a5c01c26e6724695cf440_b.png" alt="img"></p><ul><li><strong>实例化过程</strong></li><li>加载配置<strong><em>yarn.federation.state-store.class</em></strong>：默认实现是<strong><em>MemoryFederationStateStore</em></strong></li></ul><p><img src="https://pic4.zhimg.com/v2-e5888bd36c0c482c10c2bb22782ceb27_b.png" alt="img"></p><h3 id="SubClusterResolver"><a href="#SubClusterResolver" class="headerlink" title="SubClusterResolver"></a>SubClusterResolver</h3><ul><li>用来判断某个指定的node是属于哪个子集群的工具类;主要有getSubClusterForNode，getSubClustersForRack方法</li><li>实例化过程</li><li>加载配置yarn.federation.subcluster-resolver.class: 默认实现是DefaultSubClusterResolverImpl</li></ul><p><img src="https://pic2.zhimg.com/v2-cf9ebad47e5c250ef5a545ee4e1c1b05_b.png" alt="img"></p><ul><li>在<strong>load</strong>方法中，获取了machineList，定义list的地方是在一个文件中通过<strong>yarn.federation.machine-list</strong>获取文件位置；且文件中的内容格式如下</li></ul><p><img src="https://pic3.zhimg.com/v2-7d1685ce8602e9884412e42a9faaf72e_b.png" alt="img"></p><ul><li>解析文件之后，将machine依次添加到<strong>nodeToSubCluster</strong>，<strong>rackToSubClusters</strong>集合中</li></ul><h2 id="AMRMProxy模块"><a href="#AMRMProxy模块" class="headerlink" title="AMRMProxy模块"></a>AMRMProxy模块</h2><ul><li>看完client—&gt;rm侧的提交任务模块之后（<strong>router</strong>），接下来可以分析AM与RM侧的交互模块(<strong>AMRMProxy</strong>)  </li></ul><p><img src="https://pic4.zhimg.com/v2-d90061b8beeb05e40586a3dada4222a7_b.png" alt="img"></p><ul><li>AMRMProxyService ：如上图所示，起于所有的NM之上的服务，作为AM与RM之间通信的代理；会将AM请求转发到正确的HomeSubCluster</li><li>FederationInterceptor: 作为AMRMProxyService中的拦截器，主要做AM与RM之间请求转发</li></ul><h3 id="AMRMProxyService-—-FederationInterceptor"><a href="#AMRMProxyService-—-FederationInterceptor" class="headerlink" title="AMRMProxyService — FederationInterceptor"></a>AMRMProxyService — FederationInterceptor</h3><ul><li>类比Router，FederationInterceptor作为AMRMProxy的请求拦截处理</li><li>在AM的视角，<strong>FederationInterceptor</strong>的作用就RM上的<strong>ApplicationMasterService</strong>；AM通过<strong>AMRMClientAsyncImpl</strong>或<strong>AMRMClientImpl</strong> 走RPC协议与<strong>AMRMProxyService</strong> 交互</li></ul><p><img src="https://pic1.zhimg.com/v2-0ec5e08fa702ab8a00bb11528da4b138_b.png" alt="img"></p><p><strong>registerApplicationMaster详解</strong></p><ul><li>按照正常的AM流程分析，由<strong>AMLauncher</strong>启动container之后须首先会调用<strong>registerApplicationMaster</strong>方法初始化权限信息以及将自己注册到对应的RM上去；对应到<strong>FederationInterceptor</strong>是如下方法</li></ul><p><img src="https://pic1.zhimg.com/v2-a322c31e9908ed4f8b08c05130c67688_b.png" alt="img"></p><ul><li>制造一种假象：RM永不会挂掉；有可能会因为超时或者RM挂掉等原因而导致发出多个重复注册的请求，此时都会返回最近一次成功的注册结果；所以这也就是为什么registermaster这个方法必须为线程安全的原因</li></ul><p><img src="https://pic4.zhimg.com/v2-e127ca2f6e0120575bc76c5b38126a43_b.png" alt="img"></p><ul><li>目前只是往HomeSubCluster上注册AM，而不会往其他子集群上注册。是为了不影响扩展性；即不会随着集群的增多AM呈线性扩展；应该是后续按需注册sub-cluster rm</li></ul><p><img src="https://pic1.zhimg.com/v2-571134517c85d3c6fd5a237412618a74_b.png" alt="img"></p><ul><li><strong>this.homeRMRelayer</strong>是具体的跟RM通信的代理，其创建方式在<strong>FederationInterceptor.init</strong>方法中</li></ul><p><img src="https://pic1.zhimg.com/v2-4afe5125de9a0e80a3440f6807e12e84_b.png" alt="img"></p><ul><li>最后在返回response之前，会根据作业所属的queue信息从statestore中获取对应的策略，并初始化<strong>policyInterpreter</strong></li></ul><p><img src="https://pic1.zhimg.com/v2-bab165a60bd63a3c43549950825d28a0_b.png" alt="img"></p><h3 id="Allocate详解"><a href="#Allocate详解" class="headerlink" title="Allocate详解"></a>Allocate详解</h3><ul><li>周期性的通过心跳与HomeCluster和SubCluster RMs交互；期间可能伴随有SubCluster 上AM的启动和注册</li><li><strong>splitAllocateRequest</strong>：将原来的request重新构造成面向所有已经注册的sub-cluster rm request</li></ul><p><img src="https://pic2.zhimg.com/v2-bf455e34f88d53a7dcc5ffea1d51d8a9_b.png" alt="img"></p><ul><li>具体到实现：通过requestMap来放置clusterId与allocateRequest的对应关系；通过uamPool获取已经注册UAM的sub clusterId并构建request</li></ul><p><img src="https://pic3.zhimg.com/v2-3bb90c887c818638bb335ff6d46b9d46_b.png" alt="img"></p><ul><li>后面的步骤是根据所有已经注册的home cluster和sub cluster id构建release, ask, blacklist等请求</li><li>对于资源的请求拆分：这里会去调federation policy interpreter将原来request中的<strong>askList(Resource Request List)</strong>根据策略拆分到各个子集群；所以这里会涉及到Federation Policy调用，具体的分析接下来会单独拎出一小节解释</li></ul><p><img src="https://pic1.zhimg.com/v2-675411d4dfdc72c8d89ee301a8709b7c_b.png" alt="img"></p><ul><li>拿到<strong>asks</strong>后，会将<subclusterid, resourcerequestlist="">的对应关系，加入到<strong>requestMap</strong>中</subclusterid,></li><li><strong>注意：</strong>这里借助<strong>findOrCreateAllocateRequestForSubCluster</strong>方法实现如果requestMap中不存在asks中对应的subClusterId，会新new一个request塞入map；后续这个request会在对应的subCluster上启动<strong>UAM</strong></li><li><strong>因为对于新的job，刚开始确实是只在homeCluster上启动了AM</strong></li><li><strong>sendRequestsToResourceManagers</strong></li><li>splitAllocateRequest之后就是将构造好的请求发送到对应的cluster上；顺带在所有的subcluster启动UAM并注册上(如果之前没有启动的话)；返回值是所有新注册上的UAM</li></ul><p><img src="https://pic3.zhimg.com/v2-34f142d2135a9f8b12ae234966bde9b6_b.png" alt="img"></p><ul><li><strong>registerWithNewSubClusters</strong> 用来在其他子集群中创建新的UAM实例</li><li>在uamPool中不存在的被认为是新集群（<em>有点与<strong>splitAllocateRequest</strong>） 取AllUAMIds逻辑矛盾</em>）</li><li>对newSubClusters集合迭代，依次在subClaster上启动UAM，并注册UAM</li></ul><p><img src="https://pic4.zhimg.com/v2-1051b6772a9c893a910f3497cb9cc327_b.png" alt="img"></p><ul><li>最后针对不同的cluster，调用不同的clientRPC请求资源</li></ul><p><img src="https://pic2.zhimg.com/v2-2e0d3265a416a8bd9131559ce958b2f1_b.png" alt="img"></p><ul><li><strong>mergeAllocateResponses</strong></li><li>用于合并所有资源请求返回的allocateResponse。实现里面是对<strong>asyncResponseSink</strong>容器的迭代，而asyncResponseSink的写入是在HeartBeatCallback逻辑里的</li><li>对于allocateResponse的合并操作在<strong>mergeAllocateResponse</strong>中</li><li><strong>mergeRegistrationResponses</strong></li><li>是在注册完其他的sub cluster之后将UAM加入到最终合并的AllocateResponse中；主要是对allocatedContainers以及NMTokens集合做增加</li></ul><h3 id="finishApplicationMaster详解"><a href="#finishApplicationMaster详解" class="headerlink" title="finishApplicationMaster详解"></a>finishApplicationMaster详解</h3><ul><li>结束任务的时候有点类似allocate，需要向所有的sub cluster发送finish请求；目前是丢到一个compSvc线程池中批量执行*finshApplicationMaster</li><li>在线程池中执行sub cluster finish的同时，也会调用home cluster rm进行finish操作</li></ul><h2 id="Federation-Policy模块"><a href="#Federation-Policy模块" class="headerlink" title="Federation Policy模块"></a>Federation Policy模块</h2><ul><li>federation policy模块通过FederationPolicyManager的接口实现来统一加载</li></ul><p><img src="https://pic1.zhimg.com/v2-6a2db128fc5762aba3591cf4912bfc40_b.png" alt="img"></p><ul><li><strong>FederationPolicyInitializationContext</strong>：初始化FederationAMRMProxyPolicy和FederationRouterPolicy的上下文类</li><li><strong>federationStateStoreFacade</strong>: policy state strore的具体实现实例</li><li><strong>federationPolicyConfiguration</strong>: 具体的策略配置</li><li><strong>federationSubclusterResolver</strong>：用来判断某个指定的node是属于哪个子集群的工具类</li><li><strong>homeSubcluster</strong>：当前application实际AM运行的集群ID</li></ul><h2 id="Policy-具体的实现列举"><a href="#Policy-具体的实现列举" class="headerlink" title="Policy 具体的实现列举"></a>Policy 具体的实现列举</h2><h3 id="amrmproxy模块的policy实现"><a href="#amrmproxy模块的policy实现" class="headerlink" title="amrmproxy模块的policy实现"></a>amrmproxy模块的policy实现</h3><ul><li><strong>LocalityMulticastAMRMProxyPolicy</strong></li><li>\1. 如果是有偏好的host的话，会根据<em>SubClusterResolver</em> resolve cluster的结果转发到对应的cluster，但如果没有resolve的话，会默认将请求转向home cluster</li><li>\2. 如果有机架的限制，策略同上</li><li>\3. 如果没有host/rack偏好的话，会根据<em>weights</em>转发到对应的集群；weights的计算根据<em>WeightedPolicyInfo</em>以及<em>headroom</em>中的信息</li><li>\4. 所有请求量为0的请求都会转发到所有我们曾经调度过的子集群中（以防用户在尝试取消上一次的请求）</li><li>注：该实现始终排除当前未活跃的RM</li><li><strong>具体实现细节待深究</strong></li></ul><p><strong>router模块的policy实现</strong></p><ul><li>总体来说router端的策略偏简单，自己定制也容易</li><li>默认实现是<strong>UniformRandomRouterPolicy</strong>，随机转发client请求到某个alive的cluster</li></ul><h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><ul><li>在NM侧，不能开启<strong>FederationRMFailoverProxyProvider</strong>，这个统一在获取RMAddress逻辑上有不足，导致NM启动时拿到的RMAddress是localhost无法通过ResourceTracker连上RM，最终注册失败</li></ul><p><img src="https://pic1.zhimg.com/v2-edc97c0d9ba93b9094e561e6cd7b5464_b.png" alt="img"></p>]]></content>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> Hadoop </tag>
            
            <tag> Yarn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop Rpc源码分析</title>
      <link href="/2019/11/05/Hadoop-Rpc%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
      <url>/2019/11/05/Hadoop-Rpc%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>Hadoop生态系统中Rpc底层基本都是走的一套实现，所以有必要对Rpc底层实现做一次系统性的梳理总结。</p><p><strong>Client&amp;Server实现入口</strong></p><p>RpcEngine作为Rpc实现的接口，用来获取client端proxy和server端的server</p><ul><li>主要的实现是WritableRpcEngine，ProtobufRpcEngine（现默认），两者的区别主要是序列化与反序列化的协议不同；内部都有继承Server构成完整Rpc Server的实现类</li><li>IPC.Server是两种序列化协议的基类，org.apache.hadoop.ipc.Server 主要实现了Reactor的请求处理模式</li></ul><p><img src="https://pic4.zhimg.com/v2-c8db58fa71163284be19a5ef5d18226b_b.jpg" alt="img"></p><h2 id="Client-amp-Server-构造方式"><a href="#Client-amp-Server-构造方式" class="headerlink" title="Client &amp; Server 构造方式"></a>Client &amp; Server 构造方式</h2><ul><li>按照序列化协议区分两种实现：ProtobufRpcEngine, WriteableRpcEngine</li><li>通过接口getProxy 构造RpcClient</li></ul><p><img src="https://pic2.zhimg.com/v2-88bd21ee56a1cd64a95d970c67c743a5_b.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/v2-92891b943d699abd3dbb68332be7c6e4_b.jpg" alt="img"></p><p><img src="https://pic2.zhimg.com/v2-1a71694dbcf4d96672256ed37fa33cf9_b.jpg" alt="img"></p><ul><li>getServer构造RpcServer</li></ul><p><img src="https://pic2.zhimg.com/v2-7ea92e7484f556a3f658e79a751e19a1_b.jpg" alt="img"></p><h3 id="RPC-Client剖析"><a href="#RPC-Client剖析" class="headerlink" title="RPC Client剖析"></a>RPC Client剖析</h3><p>总体来说Client端实现比较简单，用hashTable的结构来维护connectionId -&gt; connections以及callId -&gt; calls 对应关系，使得请求响应不需要有严格的顺序性</p><ul><li>Ipc.Client构成</li><li>callIdCounter：callId 发号器</li><li>connections: HashTable结构，用来维护Id → Connection的映射</li><li>sendParamsExecutor：请求发送线程池    </li></ul><p><img src="https://pic2.zhimg.com/v2-9b923f98235d7881f79f9525f2a025f5_b.jpg" alt="img"></p><ul><li>Connection：自身是一个线程</li><li>calls: HashTable结构，请求结束将从call从HashTable中移除</li><li>sendRpcRequest：用户线程中通过call入口调用，用户线程阻塞</li><li>receiveRpcResponse:  run中不断轮询server看结果是否就绪</li><li>client 处理过程</li></ul><p><img src="https://pic1.zhimg.com/v2-6a94a1d6f6792b1ae190c54992ba3158_b.jpg" alt="img"></p><p>图片摘自《Hadoop技术内幕：深入解析MapReduce架构设计与实现原理 》</p><ul><li>通过反射获取到方法描述，走client Invoker调用远程实现</li></ul><p><img src="https://pic3.zhimg.com/v2-a73c35b6ffe44b32f40f638f110d0ac6_b.jpg" alt="img"></p><ul><li>getConnection中与远程server 建立socket 连接，并将连接加入connections集合中</li><li>在用户线程中调用connection.sendRpcRequest，阻塞的获取结果</li></ul><p><img src="https://pic3.zhimg.com/v2-9ace991f5df04fde6559437de1e46402_b.jpg" alt="img"></p><ul><li>Connection自身run方法中不停的轮询Server接收返回结果</li></ul><p><img src="https://pic4.zhimg.com/v2-d4ebd6403c80e86bb7ea7acff2b3e643_b.jpg" alt="img"></p><ul><li>waitForWork用来判断当前connection是否应该继续存在，返回true则继续轮询server，如果是false则关闭当前connection</li></ul><p><img src="https://pic4.zhimg.com/v2-c46e3a852373a74fa752a8b1edfe176f_b.jpg" alt="img"></p><ul><li><strong>receiveRpcResponse</strong>接收服务端返回结果，将calls移除table，可以乱序，通过ConnectionId索引，<strong>不需要同步代码块，因为只有一个receiver</strong></li></ul><p><img src="https://pic1.zhimg.com/v2-347d4bb3e5d6bafe471a1b8e686343e8_b.jpg" alt="img"></p><h3 id="RPC-Server剖析"><a href="#RPC-Server剖析" class="headerlink" title="RPC Server剖析"></a>RPC Server剖析</h3><ul><li>Server端采用经典的Reactor模式，利用IO多路复用实现事件驱动</li><li>痛点在于多路复用之前的处理模式，socket read/write是阻塞的，一个线程只能处理一个socket；使用selector之后一个进程可以监视多个进程文件描述符</li></ul><p>参考阅读：<a href="https://www.cnblogs.com/crazymakercircle/p/9833847.html" target="_blank" rel="noopener">Reactor模式</a>、<a href="https://www.cnblogs.com/crazymakercircle/p/10225159.html#4310290" target="_blank" rel="noopener">Java  NIO   底层原理 </a> 、<a href="https://www.jianshu.com/p/dfd940e7fca2" target="_blank" rel="noopener">select、poll、epoll</a></p><p><img src="https://pic1.zhimg.com/v2-58df22a9108b9c724c8b757f6357d8c4_b.jpg" alt="img"></p><p>图片摘自《Hadoop技术内幕：深入解析MapReduce架构设计与实现原理 》</p><ul><li>Reactor 工作图</li><li>Reactor：负责响应IO事件，将事件派发到工作线程</li><li>Acceptor：用来接收Client端的请求，建立Client与handler的联系；向Reactor注册handler</li><li>Reader/Sender：为了加快速度，同时做到请求和处理过程的隔离，reader和sender 分别是两个线程池，用来存放该过程处理完后的连接，处理完之后塞入中间队列，等待下一个过程的线程拿去处理就行</li><li>Handler：connection对应的工作线程，会做一些decode, compute, encode工作</li></ul><p><img src="https://pic4.zhimg.com/v2-64e02ba7cce407bf7d191ce4b08bfdeb_b.jpg" alt="img"></p><p><strong>Hadoop RpcServer组成结构</strong></p><ul><li><strong>序列化层</strong>：RpcRequestWrapper, RpcResponseWrapper</li><li><strong>接口调用层</strong>：RpcInvoker，通过反射方式阻塞调用Server端具体的Service方法；调用前后记录一些metrics信息</li><li>在handler线程处理逻辑中，通过注册的rpcKind获取对应的RpcInvoker实现，通过反射来调用工作层的Service</li><li><strong>请求接收/返回层Ipc.Server</strong>：基于Java NIO实现的Reactor 事件驱动模式</li><li>Listener</li><li>selector：监听请求 → 建立连接 → 派发到Reader线程</li><li>Readers</li><li>readSelector：解析&amp;封装Call → 塞入CallQueue </li><li>Handlers：工作线程</li><li>并行pull CallQueue，调用RpcInvoker处理</li><li>Responder：read request和write response采用不同的selector实现读写分离</li><li>writeSelector</li><li>connectionManager: 定时清理idle时间过长的Connection</li><li>CallQueue：reader handler之间的缓冲队列，<strong>生产消费者模型</strong></li></ul><h3 id="RPC-Server-处理流程"><a href="#RPC-Server-处理流程" class="headerlink" title="RPC Server 处理流程"></a>RPC Server 处理流程</h3><p><img src="https://pic2.zhimg.com/v2-38a1ef7504f6e74ba8bb4aa3a0a1bdb5_b.jpg" alt="img"></p><ul><li>Listener → Reader 请求建立过程：Listener<em>Reader</em>Connection</li><li>Listener线程只有一个，通过Selector方式监听客户端的Rpc请求(OP_ACCEPT事件)，调用doAccept方法建立连接；此时connectionManager线程开始工作</li><li>建立连接后，roundbin方式获取一个reader线程，将连接塞入reader线程的pending队列和connectionManager中</li></ul><p><img src="https://pic4.zhimg.com/v2-3d7ddb917b9a9bba707c2208e9b71c4f_b.jpg" alt="img"></p><p><img src="https://pic4.zhimg.com/v2-fb8edf28b12530ea9f49ce030d780f7b_b.jpg" alt="img"></p><ul><li>Reader线程doRunLoop中，将pending的connections注册到readSelector中，用来监听一个connection读就绪事件</li></ul><p><img src="https://pic3.zhimg.com/v2-c3056224c1b460860b0b9c26c2ccc182_b.jpg" alt="img"></p><ul><li>数据读入 → 工作线程 : Reader<em>Connection</em>CallQueue</li><li>而后Reader通过selector方式，只要监听的channel有读事件，则调用doRead方法；其中通过selectionKey获取关联的connection对象，调用connection的readAndProcess方法</li><li>connection.readAndProcess: 主要是将channel里面的数据读入data byteBuffer中，数据读完之后调用processOneRpc 进一步处理</li></ul><p><img src="https://pic3.zhimg.com/v2-01d207ac87060718231e7d6a32599412_b.jpg" alt="img"></p><p><img src="https://pic4.zhimg.com/v2-74ff999fb2c4844c9470e9d3bfefa943_b.jpg" alt="img"></p><ul><li>connection. processOneRpc 对buffer decode构造成DataInputStream以及RpcHeader（请求元信息，协议类型等）通过processRpcRequest将请求塞入CallQueue中，等待handlers处理</li><li>connection.processRpcRequest：通过header中指定的rpc engine将dataInputStream根据不同engine反序列化协议反序列化成rpcRequestWrapper；构造Call对象塞入CallQueue, 并incrRpcCount</li></ul><p><img src="https://pic2.zhimg.com/v2-46cd9573bbe1ffd820a5a22ca16628d9_b.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/v2-195514acc9cc988cd2b7c14f45a535d4_b.jpg" alt="img"></p><ul><li>Handler → RpcInvoker → Responder</li><li>Handler线程在Server start的时候就已经构建启动了</li><li>并行pull callQueue获取队列中未处理的call，调用call方法</li></ul><p><img src="https://pic1.zhimg.com/v2-cc649767778d9769384fde11b8b7a5c0_b.jpg" alt="img"></p><ul><li>通过rpcKind获取对应的RpcInvoker实现；主看ProtoBufRpcInvoker.call</li></ul><p><img src="https://pic1.zhimg.com/v2-07fc870601bd5199e9c298828106c598_b.jpg" alt="img"></p><ul><li>通过反射获取server端对应的接口实现，阻塞调用，在调用前后记录一些metrics信息；最后将结果包装成RpcResponseWrapper</li></ul><p><img src="https://pic2.zhimg.com/v2-2c1300aac072c27f7a7f6d02326643d5_b.jpg" alt="img"></p><p><img src="https://pic3.zhimg.com/v2-10f7ea1450e7fcba8eeba0b28cfade16_b.jpg" alt="img"></p><ul><li>当结果处理完成之后，通过setupResponse将结果序列化成byte buffer根据不同engine实现的wrapper 序列化方式有所不同</li></ul><p><img src="https://pic4.zhimg.com/v2-ced2fa047079e775740313b85008ea2b_b.jpg" alt="img"></p><p><img src="https://pic3.zhimg.com/v2-0ec60ac2232816c3d38b72ce5ac1c02e_b.jpg" alt="img"></p><ul><li>调用Responder.doRespond将请求结果返回客户端</li><li><strong>请求返回处理过程:</strong>  通过Responder线程+ writeSelector</li><li>Responder.doRespond</li><li>在handler中尽可能的将response一次性写入channel buffer，如果没有剩余则不用注册Responder的Responder.doRespond</li><li>如果一次性写不完且是在handler线程中，则唤醒writeSelector，将当前channel 注册 SelectionKey.OP_WRITE 异步去处理</li></ul><p><img src="https://pic4.zhimg.com/v2-3b339c9e8e15060518996edccee9aebf_b.jpg" alt="img"></p><ul><li>Responder 线程自身的doRunLoop里面也是通过writeSelector监听OP_WRITE事件处理</li></ul><p><img src="https://pic1.zhimg.com/v2-7c4c403ee9bff19653e58088d81d7428_b.jpg" alt="img"></p><ul><li><strong>CallQueueManager</strong> 相关</li><li>默认实现是LinkedBlockingQueue</li><li>大小通过queueSizePerHandler或ipc.server.handler.queue.size * handler_count 决定</li></ul><p><img src="https://pic3.zhimg.com/v2-56e6f38dd99dc26c8dbe3ccb84a581aa_b.jpg" alt="img"></p><ul><li><strong>ConnectionManager相关</strong>：用来定时清理idle时间过长的connection</li><li>idleScanThreshold: 每次轮询扫描的connections 阈值default 4000</li><li>idleScanInterval: 定时检测线程轮询间隔 default 10000</li><li>maxIdleTime:  一个connection最长idle时间，default 2* 10000</li><li>maxIdleToClose : 一次轮询最多关闭的连接数 default 10</li><li>一个connection是不是可以被清理由以下条件决定</li><li>connection.isIdle(): rpcCount为0, 也就是Call没有塞入callQueue；在connection.processRpcRequest末尾，如果成功塞入callQueue中的话会incrRpcCount</li><li>lastContact &lt; minLastContact: </li><li>minLastContact:  Time.now() - maxIdleTime</li><li>startIdleScan：开启清理线程，随Listener线程启动</li></ul>]]></content>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> Hadoop </tag>
            
            <tag> Yarn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>【Spark源码分析】Job提交执行过程详解</title>
      <link href="/2019/06/10/%E3%80%90Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E3%80%91Job%E6%8F%90%E4%BA%A4%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/"/>
      <url>/2019/06/10/%E3%80%90Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E3%80%91Job%E6%8F%90%E4%BA%A4%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近恰好有点时间梳理一下整个Spark job提交执行流程的相关源码。首先，给一个总的代码流程图（在Executor那块还需补充完整），方便理解整个处理逻辑</p><p><img src="https://pic3.zhimg.com/80/v2-6e794771cf2317012b3986ed35979476_hd.jpg" alt=""></p><h2 id="Spark-Job-提交处理过程源码解析"><a href="#Spark-Job-提交处理过程源码解析" class="headerlink" title="Spark Job 提交处理过程源码解析"></a>Spark Job 提交处理过程源码解析</h2><h3 id="submitJob解析"><a href="#submitJob解析" class="headerlink" title="submitJob解析"></a>submitJob解析</h3><ul><li>macos IntelliJ 中 command+7 查看DagScheduler所有方法，从submitJob方法开始分析，提交了JobSubmitted事件进事件队列，等待处理</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job1.png" alt=""></p><ul><li>在<strong>DagScheduler</strong>中有<strong>DAGSchedulerEventProcessLoop</strong>类，主要用来集中分发处理事件队列中的事件</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job2.png" alt=""></p><ul><li>移步DagScheduler.handleJobSubmitted方法，更新UI数据，同时调用submitStage方法；这里finalStage是createResultStage这个方法从最后一个stage生成所有stage的过程</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job3.png" alt=""></p><h3 id="submitStage解析"><a href="#submitStage解析" class="headerlink" title="submitStage解析"></a>submitStage解析</h3><p><img src="http://jacobs.wanhb.cn/images/spark-job4.png" alt=""></p><p><img src="http://jacobs.wanhb.cn/images/spark-job5.png" alt=""></p><p>可以看到<strong>getOrCreateParentStages</strong>方法中只有shuffle操作时才会创建新的stage</p><ul><li>再来看SubmitStage方法实现细节，看之前如果对spark运行有了解的话，也大概知道，submitStage里面是提交task的细节</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job6.png" alt=""></p><p>这里先判断几个集合：<strong>waitingStages,runningStages,failedStages</strong>中是否已经存在该stage，防止重复提交stage；通过<strong>getMissingParentStages</strong>，深度遍历地从后往前判断当前stage是否存在需要重新计算的stage，加入<strong>missing stages</strong>集合中。那么什么条件下才算是一个missing stage呢？我们来分析<strong>getMissingParentStages</strong>实现</p><p><img src="http://jacobs.wanhb.cn/images/spark-job7.png" alt=""></p><p>可以发现判断当前rdd是否被cache了是通过DagScheduler.getCacheLocs获取缓存的location，观察到cacheLocs的数据结构是一个HashMap，key为rdd id，value为TaskLocation集合；虽说HashMap是个非线程安全集合，不过这里写操作线程安全通过加锁实现，所以说用HashMap实现倒也无妨</p><p><img src="http://jacobs.wanhb.cn/images/spark-job8.png" alt=""></p><p>这个集合是在getCacheLocs中写入的</p><p><img src="http://jacobs.wanhb.cn/images/spark-job9.png" alt=""></p><p>如果当前rdd本身没有设置storage level的话，也就无需查找缓存了，直接返回，否则通过blockManagerMaster.getLocations查找具体block对应的位置；blockManagerMaster上存储了所有Executor汇报上来的所有block位置元数据信息<strong>（后面有一小节来分析block的写入和上报过程）</strong></p><p>接着，对于rdd如果没有显式缓存的情况，需要遍历rdd所有的依赖，对于是宽依赖的stage，调用<strong>getOrCreateShuffleMapStage</strong>获取或者创建mapStage，通过isAvailable判断所有output是否都已经准备好，isAvailable是通过查询<strong>mapOutputTracker</strong>已经注册的task output信息得到的，对于isAvailable为false的情况，说明output没有，或丢失。需要重新计算</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Number of partitions that have shuffle outputs.</span><br><span class="line">   * When this reaches [[numPartitions]], this map stage is ready.</span><br><span class="line">   */</span><br><span class="line">  def numAvailableOutputs: Int = mapOutputTrackerMaster.getNumAvailableOutputs(shuffleDep.shuffleId)</span><br><span class="line">  /**</span><br><span class="line">   * Returns true if the map stage is ready, i.e. all partitions have shuffle outputs.</span><br><span class="line">   */</span><br><span class="line">def isAvailable: Boolean = numAvailableOutputs == numPartitions</span><br></pre></td></tr></table></figure><h3 id="submitMissingTasks解析"><a href="#submitMissingTasks解析" class="headerlink" title="submitMissingTasks解析"></a><strong>submitMissingTasks解析</strong></h3><ul><li>接下来分析submitStage中submitMissingTasks实现，这个方法是根据需要计算的stage来提交stage中的taskset。taskIdToLocation获取task要处理的数据的所在节点</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job10.png" alt=""></p><p>然后根据task所属的stage类型来创建实际的task实例(ShuffleMapTask与ResultTask)</p><p><img src="http://jacobs.wanhb.cn/images/spark-job11.png" alt=""></p><p>最后如果待计算的tasks集合不为空，则通过taskScheduler引用将task set提交到TaskScheduler中去调度</p><p><img src="http://jacobs.wanhb.cn/images/spark-job12.png" alt=""></p><p>具体看<strong>TaskSchedulerImpl.submitTasks</strong>实现，首先会创建一个<strong>TaskSetManager</strong>。<strong>TaskSetManager</strong>实际调度TaskSet的的实现，跟踪并且根据数据优先级分发task，以及重试失败的task。因为一个stage同一时刻只能有至多一个<strong>TaskSetManager</strong>处于活跃状态，所以创建完<strong>TaskSetManager</strong>实例之后，需要将Stage中其他<strong>TaskSetManager</strong>实例标记为<strong>Zombie</strong>状态</p><p><img src="http://jacobs.wanhb.cn/images/spark-job13.png" alt=""></p><p>随后，根据运行模式来判断要不要启动资源分配情况是否是饥饿状态的监控线程，最后调用<strong>CoarseGrainedSchedulerBackend.reviveOffers()</strong> 方法开始task调度</p><p><img src="http://jacobs.wanhb.cn/images/spark-job14.png" alt=""></p><p>实际上是发了一个actor消息，直接看receive中针对ReviveOffers消息的处理方法(<strong>CoarseGrainedSchedulerBackend.makeOffers</strong>)实现</p><p><img src="http://jacobs.wanhb.cn/images/spark-job15.png" alt=""></p><h3 id="Scheduler-resourceOffers解析"><a href="#Scheduler-resourceOffers解析" class="headerlink" title="Scheduler.resourceOffers解析"></a>Scheduler.resourceOffers解析</h3><ul><li>先筛选出存活的executor，然后调用TaskSchedulerImpl.resourceOffers方法开始为每个TaskSet中的task分配具体执行节点</li><li>在分配前将那些之前被加入黑名单又重新生效的节点包括进来；然后打散workerOffer集合，防止task分配不均</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job16.png" alt=""></p><ul><li>获取shuffleOffer中节点剩余cpu以及slot(cores/CPU_PER_TASK(default 1))集合availableCpus，availableSlots</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val availableCpus = shuffledOffers.map(o =&gt; o.cores).toArray</span><br><span class="line">val availableSlots = shuffledOffers.map(o =&gt; o.cores / CPUS_PER_TASK).sum</span><br></pre></td></tr></table></figure><ul><li><p>获取sortedTaskSets，在循环期间随时关注是否有新的executor加入</p><p><img src="http://jacobs.wanhb.cn/images/spark-job17.png" alt=""></p></li><li><p>对sortedTaskSets集合的每个taskSet，如果taskSet是barrier模式，且可用slot小于taskSet中的task数量，则直接跳过分配；因为barrier模式中，所有的task都需要并行启动</p></li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job18.png" alt=""></p><ul><li>对于非barrier模式的taskSet，根据taskSet中所有tasks的数据优先级调度task。如下图，myLocalityLevels是taskSet中所有tasks数据本地性优先级集合。由TaskSetManager. computeValidLocalityLevels方法计算得到</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job19.png" alt=""></p><ul><li>优先级从高到低依次为<strong>PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY</strong> 也是按照这个顺序优先调度task</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job20.png" alt=""></p><ul><li><p>具体的调度见<strong>TaskSchedulerImpl.resourceOfferSingleTaskSet</strong>方法，里面实际依赖<strong>TaskSetManager.resourceOffer</strong>方法</p></li><li><ul><li>首先对应的executor不能是被拉入黑名单，且当前TaskSetManager不能被标记为zombie</li><li>从taskSet中出队一个指定locality的 task（实现见TaskSetManager.dequeueTask）加入runningTasks结合中。对于非barrier模式的stage来说，只要有task被调度成功了就可以跑起来</li></ul></li><li><p>这里再回过头看CoarseGrainedSchedulerBackend.makeOffers实现。当调用scheduler.resourceOffers之后如果有TaskDescription集合返回的的话，就可以调用launchTasks了</p></li><li><ul><li>在launchTasks方法中，发送了LaunchTask消息，将序列化的Task信息通过rpc发送给Executor端（CoarseGrainedExecutorBackend实现）</li><li><img src="http://jacobs.wanhb.cn/images/spark-job22.png" alt=""></li></ul></li><li><p>看CoarseGrainedExecutorBackend.receive中对LaunchTask消息的处理逻辑</p><p><img src="http://jacobs.wanhb.cn/images/spark-job23.png" alt=""></p></li><li><p>executor.launchTask中，实例化了TaskRunner，并将taskRunner提交到线程池中调度执行。<strong>具体的执行逻辑在下一个小节描述</strong></p></li></ul><h2 id="ShuffleMapTask-block写入过程分析"><a href="#ShuffleMapTask-block写入过程分析" class="headerlink" title="ShuffleMapTask block写入过程分析"></a><strong>ShuffleMapTask block写入过程分析</strong></h2><ul><li>在上文中，我们分析到了TaskRunner。直接跳到TaskRunner里面的run方法实现</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job25.png" alt=""></p><p>可以看到通过执行执行task.run方法拿到执行task后的结果，跟进去看结果是什么数据结构。</p><p><img src="http://jacobs.wanhb.cn/images/spark-job26.png" alt=""></p><p>发现调用了runTask方法，查看接口的定义，发现有多个实现</p><p><img src="http://jacobs.wanhb.cn/images/spark-job27.png" alt=""></p><p>看到了熟悉的<strong>ShuffleMapTask</strong>,<strong>ResultTask</strong>字眼，结果明朗了，其实就是根据宽窄依赖来调用具体的Task实现。<strong>ResultTask</strong>生成的result是func在rdd各个partition上的执行结果而<strong>ShuffleMapTask</strong>生成的result是shuffle 文件输出信息(<strong>MapStatus</strong>)</p><ul><li>我们选<strong>ShuffleMapTask.runTask</strong>实现分析，返回的数据结构是<strong>MapStatus，MapStatus</strong>封装了task 所在的blockManager的信息（executorId+host+port）以及map task到每个reducer task的输出FileSegment的大小</li></ul><p><img src="http://jacobs.wanhb.cn/images/spark-job28.png" alt=""></p><p>来分析outputFile的实现细节，首先这里需要获取具体的ShuffleWriter实现</p><p><img src="http://jacobs.wanhb.cn/images/spark-job29.png" alt=""></p><p>每个shuffleId对应的ShuffleHandle（也即是ShuffleWriter实现）由ShuffleManager统一管理，通过registerShuffle注册具体的ShuffleWriter</p><p><img src="http://jacobs.wanhb.cn/images/spark-job30.png" alt=""></p><p><img src="http://jacobs.wanhb.cn/images/spark-job31.png" alt=""></p><p>如图所示，目前spark中的shuffleWriter实现大概有三种，这里不详细比较，后续有专门文章分析Spark ShuffleWriter实现；选最常用的<strong>SortShuffleWriter.write</strong> 实现深入分析MapStatus产生过程</p><p><img src="http://jacobs.wanhb.cn/images/spark-job32.png" alt=""></p><p>可以看到SortShuffleWriter对于每个mapTask只会产生一个output，通过indexfile start和end offset 来计算后续reduceTask获取数据的位置，这样做大大减小了output 文件数量。最终返回mapStatus结果</p><ul><li><p><a href="https://link.zhihu.com/?target=http%3A//%E4%BA%8E%E6%98%AF%E7%8E%B0%E5%9C%A8%E7%9F%A5%E9%81%93%E8%B0%83%E7%94%A8TaskRunner.run" target="_blank" rel="noopener">于是现在知道调用TaskRunner.run</a> 根据task的类型不同返回的结果也是不同的，统一将其包装成DirectResult发送到driver上；这里根据实际得到的resultSize有不同的处理情况</p></li><li><ul><li>如果result比较大，超过了maxDirectResultSize则会先把result存到本地的blockManager托管，storageLevel是内存+磁盘，然后把存储信息封装成IndirectTaskResult发送给driver</li><li>否则直接将序列化的result发送给driver。通过statusUpdate封装StatusUpdate事件将result发送给driver端处理</li><li><img src="http://jacobs.wanhb.cn/images/spark-job33.png" alt=""></li></ul></li><li><p>这里可以再分析一下result过大，blockManager是如何处理的细节。先看<strong>blockManager.doPutBytes</strong>，这里可以看到优先将result写入本地内存(LinkedHashMap实现)，如果内存不够<strong>（totalSize&gt;memory*spark.storage.memoryFraction）</strong>，则会将result通过diskStore直接写入磁盘</p><p><img src="http://jacobs.wanhb.cn/images/spark-job34.png" alt=""></p></li><li><p>看CoarseGrainedSchedulerBackend中具体处理StatusUpdate的实现，这里其实嵌套的比较多。按正常的路径首先会经过<strong>taskResultGetter.enqueueSuccessfulTask</strong>方法，在这里会将result反序列化（有<strong>DirectTaskResult</strong>与<strong>IndirectTaskResult</strong>之分），接着调用<strong>DagScheduler.handleSuccessfulTask</strong>。这里按照task类型不同有不同的处理方式：</p><ul><li><p>task是ResultTask的话，可以使用ResultHandler对result进行driver端计算（比如count()会对所有ResultTask的result做sum）</p></li><li><p>如果是ShuffleMapTask的话会注册mapOutputTracker，方便后续reduce task查询，然后submit 下一个stage</p><p><img src="http://jacobs.wanhb.cn/images/spark-job35.png" alt=""></p></li></ul></li></ul><h2 id="Rdd-Cache的过程"><a href="#Rdd-Cache的过程" class="headerlink" title="Rdd Cache的过程"></a>Rdd Cache的过程</h2><ul><li><p>分析至此，我们似乎还没看到ShuffleMapTask cache的过程，只知道如果是ResultTask产生的数据会优先塞入内存（不够溢写磁盘）。那么我们在平时操作中调用的rdd.cache在哪个环节起作用了呢？其实我们分析ShuffleMapTask处理过程时，忽略了一块代码（ShuffleMapTask.runTask 中 rdd.iterator的调用）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ShuffleMapTask.runTask</span><br><span class="line">=&gt; writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ &lt;: Product2[Any, Any]]])</span><br></pre></td></tr></table></figure></li><li><p>进入iterator实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Rdd.iterator</span><br><span class="line">=&gt; getOrCompute (if storageLevel != StorageLevel.NONE)</span><br><span class="line">=&gt; computeOrReadCheckPoint (if storageLevel == StorageLevel.NONE)</span><br></pre></td></tr></table></figure></li><li><p>发现这里获取rdd的时候取决于当前rdd的存储方式，默认应该是<strong>StorageLevel.NONE</strong>，显示调cache的话将会走<strong>getOrCompute</strong>读取缓存逻辑，先看rdd不缓存的情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RDD.computeOrReadCheckpoint</span><br><span class="line">=&gt; 如果被checkpoint了，读checkpoint数据</span><br><span class="line">=&gt; 如果没有则直接重新计算</span><br></pre></td></tr></table></figure><p>如果有checkpoint会获取，否则直接compute重新计算rdd。看<strong>getOrCompute</strong>实现</p><p><img src="http://jacobs.wanhb.cn/images/spark-job38.png" alt=""></p><ul><li>核心还是调用从blockManager中去拿缓存的rdd或者重新计算更新blockManager，在getLocalValues方法里面会根据当前的StorageLevel到memory或者diskStore里面去拿blockResult，发现diskStore.getBytes实现里面，<strong>diskManager.getFile</strong>方法正是SortShuffleWriter中获取output路径的底层实现</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DiskStore.getBytes</span><br><span class="line">=&gt; val file = diskManager.getFile(blockId.name)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SortShuffleWriter.write</span><br><span class="line">=&gt; val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line">IndexShuffleBlockResolver.getDataFile</span><br><span class="line"> =&gt;blockManager.diskBlockManager.getFile(ShuffleDataBlockId(shuffleId, mapId, NOOP_REDUCE_ID)) </span><br><span class="line">     =&gt;DiskManager.getFile</span><br></pre></td></tr></table></figure></li><li><p>经过一番分析，也能看出，cache主要适用于数据量不大的且反复使用的rdd；如果数据量过大，会发生频繁的数据溢写，还可能导致OOM的错误，收益大于成本，需要慎用</p></li><li>至此，shuffleMapTask从提交到输出到磁盘，以及DagScheduler如何处理Task Completion事件分析完了。后续文章中将分析ResultTask如何从<strong>mapOutputTracker</strong>拉取数据，以及如何计算的逻辑</li></ul>]]></content>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 学习 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>【Spark源码分析】Broadcast</title>
      <link href="/2019/06/02/%E3%80%90Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E3%80%91Broadcast/"/>
      <url>/2019/06/02/%E3%80%90Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E3%80%91Broadcast/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Spark的broadcast机制本意在于两表做join时，如果其中某一个表足够的小，且又不是join的基表时（left或right join），可以将小表通过网络全量分发到各个executor节点上；通过在各个分区本地做join的方式来减少一次shuffle带来的开销</p><h2 id="Broadcast-原理"><a href="#Broadcast-原理" class="headerlink" title="Broadcast 原理"></a>Broadcast 原理</h2><h3 id="满足broadcast-join的条件源码分析"><a href="#满足broadcast-join的条件源码分析" class="headerlink" title="满足broadcast join的条件源码分析"></a>满足broadcast join的条件源码分析</h3><ul><li>来看SparkStrategies.scala文件</li><li><p>Broadcast 策略入口 broadcastSideBySizes<br><img src="http://jacobs.wanhb.cn/images/spark-broadcast1.png" alt=""></p><ul><li><p>可以发现broadcast 左表或者是右表是根据两个策略来控制的：canBuildLeft/canBuildRight， canBroadcast；</p></li><li><p>canBroadcast控制的是数据大小是否符合参数设定<br><img src="http://jacobs.wanhb.cn/images/spark-broadcast2.png" alt=""></p></li><li><p>canBuildLeft/canBuildRight是判断被广播的表是否作为left或right join基表的情况；如果作为基表的话是不能被broadcast的；当然Inner join不用管是不是基表<br><img src="http://jacobs.wanhb.cn/images/spark-broadcast3.png" alt=""></p><p><img src="http://jacobs.wanhb.cn/images/spark-broadcast4.png" alt=""></p></li></ul></li><li><p>基表不能被广播的原因</p><ul><li><p>left/right join 之所以基表不能broadcast是因为这样做会破坏left join语义，产生重复的数据(比如广播了n份基表，因为最后都要保留基表的数据，不管有没有匹配上，所以会导致归并的时候有重复的情况)</p></li><li><p>翻阅其他博客对broadcast的解释，也能发现基表不能被广播的事实 <a href="https://www.iteblog.com/archives/2086.html" target="_blank" rel="noopener">Spark SQL中Join常用的几种实现</a> </p><p><img src="http://jacobs.wanhb.cn/images/spark-broadcast5.png" alt=""></p></li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 学习 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>【Spark源码分析】Dynamic Resource Allocation设计的思考</title>
      <link href="/2019/05/26/%E3%80%90Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E3%80%91Dynamic-Resource-Allocation%E8%AE%BE%E8%AE%A1%E7%9A%84%E6%80%9D%E8%80%83/"/>
      <url>/2019/05/26/%E3%80%90Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E3%80%91Dynamic-Resource-Allocation%E8%AE%BE%E8%AE%A1%E7%9A%84%E6%80%9D%E8%80%83/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在用spark的dynamicAllocation时发现：如果一个executor超过了设置的executorIdleTimeout时间，触发了回收策略，停止executor之后在sparkUI上会显示该executor的状态为Dead的情况</p><p>这引起了我的疑问，因为凭我自身的经验判断会误以为这个executor是一种<strong>被动退出</strong>的情况；也即是executor进程因为某种原因被nodemanager kill了，导致driver将这个executor状态置为dead并进行一系列的清理工作。而如果是dynamicAllocation的话，我认为是一种<strong>主动退出</strong>的情况，是安全的。spark自身系统设计不应该将这两个概念的状态笼统的用一个Dead来混淆视听</p><p>本着对真理追求到底的态度，我决定对sparkUI统计数据的来源这块代码逻辑进行梳理，以给自己提出的问题寻求答案</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><h4 id="Spark-UI-Server启动"><a href="#Spark-UI-Server启动" class="headerlink" title="Spark UI Server启动"></a>Spark UI Server启动</h4><ul><li><p>我们知道启动一个spark application之后相应的也会启动一个sparkUI server，用于实时监控展示 jobs，stages， executors等一些统计信息，那这些统计数据来自哪里呢？spark内部通过LiveListenerBus实现了一种监听订阅的模式，application内部所有的变更状态通过发布变更事件，交由订阅这些变更事件的实现去处理（这里称之为spark listener）。处理完之后的最新状态将反应在sparkUI上。</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-1.jpeg" alt=""></p></li><li><p>从图中我们可以看出DAGSchedule是主要产生各类SparkListenerEvent的起源，SparkListenerEvent通过ListenerBus事件队列，期间定时器匹配将事件匹配到不同的SparkListener实现上去</p></li></ul><h4 id="Executor页面渲染"><a href="#Executor页面渲染" class="headerlink" title="Executor页面渲染"></a>Executor页面渲染</h4><ul><li><p>在SparkUI的初始化方法中可以看到绑定了我们在界面中见到的几个Tab，如Executors，stages，storage等</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-2.png" alt=""></p><p>跟进ExecutorsTab中看具体的页面渲染逻辑</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-3.png" alt=""></p><p>整个代码层次分明，页面渲染包括页面顶部通用的bar以及body里面具体的内容，这里将渲染页面顶部的逻辑模块化了；我们主要看的是executorspage.js这个文件，这里面是获取executor summary数据并渲染的主逻辑。在executorspage.js内部，发现为获取all-executors数据，发送了一个ajax请求</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-4.png" alt=""></p><p>这个allexecutors接口有我们想要的executors数据来源信息。全局搜索这个endpoint，发现在AbstractApplicationResource 声明定义了该接口实现</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-5.png" alt=""></p><p>意外的发现做了一个类似于请求存储的操作，跟进去发现是AppStatusStore</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-6.png" alt=""></p><p>查看类说明，发现这是一个spark 自身kv store的的访问封装实现</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-7.png" alt=""></p><p><img src="http://jacobs.wanhb.cn/images/sparkui-8.png" alt=""></p><p>追踪到这里，算是对数据来源钻到了尽头，可以知道最终sparkUI上executors summary数据是存在自身实现的kvstore里的</p></li><li><p>关于kvstore的由来，可以详细看这个<a href="https://issues.apache.org/jira/browse/SPARK-18085" target="_blank" rel="noopener">issule</a> 。大致的点和思路是：Spark History server在查看某一个application运行记录的时候需要从eventlog里面拿出数据渲染；对于少数几个任务来说，目前的实现没有问题，但是如果管理了大量的application，history server就会变的几乎不可用；于是思路是实现一套存储(基于LevelDB或Inmemory结合) 可供history server读写，能大幅提升其页面加载速度</p></li><li><p>现在我们需要关注一下executorAdded或者removed事件对kvstore里面的数据处理逻辑，看SparkListener中对executor增减接口的定义，追溯到AppStatusListener实现，这也恰好是改变AppStatusStore的入口</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-9.png" alt=""></p><p>可见当executor被remove的时候只是将状态置为false，并更新了kvstore里面的值，而不是将其删除，所以前端查询的时候如果发现executor状态不是active且没在blacklist里面的话，默认就把状态format称Dead了</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-10.png" alt=""></p><p><img src="http://jacobs.wanhb.cn/images/sparkui-11.png" alt=""></p></li></ul><h4 id="DynamicAllocation-实现机制"><a href="#DynamicAllocation-实现机制" class="headerlink" title="DynamicAllocation 实现机制"></a>DynamicAllocation 实现机制</h4><ul><li><p>这里再补充一下DynamicAllocation的底层实现分析。回到之前SparkListener里定义的两个事件处理接口：onExecutorAdded，onExecutorRemoved；其实不止AppStatusListener对这两个事件做了处理，还有ExecutorAllocationListener。这个监听器是触发ExecutorAllocationManager增删executors的入口</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-12.png" alt=""></p></li><li><p>可以看出里面都是调用的allocationManger里面的具体实现。在onExecutorAdded的callback处理逻辑中，会对新加入的executor做idle记录（onExecutorIdle中实现），先判断当前executor有没有缓存的blocks，走不同的计算timeout分支。其中<strong>cachedExecutorIdleTimeoutS</strong>默认是<strong>Integer.MAX_VALUE</strong> ，然后将记录存入hash结构(<executorid,ideltime>)里，方便<strong>ExecutorAllocationManager</strong>在定时任务下一个周期做检查排除过期的executor</executorid,ideltime></p><p><img src="http://jacobs.wanhb.cn/images/sparkui-13.png" alt=""></p><p>检查逻辑如下：</p><p><img src="http://jacobs.wanhb.cn/images/sparkui-14.png" alt=""></p></li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>从源码分析来看，确实主动和被动释放executor，在sparkUI上面对应的executor状态都会变为Dead。对于使用者来说，如果不清楚spark是否开启了dynamic allocation也确实会引起歧义。毕竟Dead总归是一种不好的状态，甚至逼迫着运维同学去分析一波日志。不知道spark以后的版本中是否会增加一个新的状态？比如引入Released之类的状态将主动和被动区分开，我想这样的话用户体验会更好。</li></ul>]]></content>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 学习 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Raft论文学习</title>
      <link href="/2019/05/01/Raft%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/05/01/Raft%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>分布式系统领域自然离不开一致性协议，而其中以Paxos和Raft最为著称。Paxos和Raft早两年有接触过，受限于当时的知识水平，对实现细节难免囫囵吞枣；最近决心专供分布式系统，于是重新拾起相关Paper开始拜读。以下是Raft 论文读后总结</p><h2 id="Raft-五大性质"><a href="#Raft-五大性质" class="headerlink" title="Raft 五大性质"></a>Raft 五大性质</h2><ul><li><strong>Election Safety</strong>: 在每一个term里，<em>至多</em>（有可能没有）只能有一个leader被选出</li><li><strong>Leader Append-Only</strong>: leader节点不会对自身的log entries 进行覆写/删除的操作；只是单纯的append</li><li><strong>Log Matching</strong>: 如果两个log entry 拥有相同的index和term，那么这两个entry是相等</li><li><strong>Leader Completeness</strong>: 在一个term中，如果log entry被commit了，那么这个entry 将会存在于所有的其他任期的leader中（<em>也是作为Candidate是否被选中的一个条件</em>）</li><li><strong>State Machine Safety</strong>:  如果一个节点apply 了一个log entry，那么带有相同index却不同的log entry是不能被其他任何一个节点所apply</li></ul><h2 id="Raft-组成部分"><a href="#Raft-组成部分" class="headerlink" title="Raft 组成部分"></a>Raft 组成部分</h2><ul><li>Raft 由 Leader，Follower以及Candidate三种角色组成，三者之间组成有限状态机，可在一定事件下互相切换，具体如下图<br><img src="https://pic4.zhimg.com/80/v2-393502082f95a7432687a6fbe19d7cdf_hd.jpg" alt=""></li><li>根据上图，角色对应的分工如下<ul><li>Follower<ul><li>响应candidates和leader的 rpc请求</li><li>如果leader在timeout之内未发送心跳，则主动切换为candidate发起新一轮选举</li></ul></li><li>Candidate：主要是选举<ul><li>将currentTerm +1，且投给自己，并发起Request Vote RPC给所有其他节点寻求投票</li><li>如果收到大多数节点投票，则变成leader，通知所有节点切换为follower</li><li>如果通过AppendEntries RPC说明新的leader选举成功，则将自己置为follower</li><li><em>可能出现都投自己的情况（极端）</em>：这种情况的处理机制是所有candidate 任意sleep 一段时间（<strong><em>150-300ms</em></strong>），再触发新一轮选举</li></ul></li><li>Leader:<ul><li>维持心跳，防止触发leader选举</li><li>如果接收到客户端append log请求，leader 会并发地向followers 发起AppendEntries Rpc请求，等大多数follower 节点都返回成功之后再将log entry本地commit,  并将结果最终结果返回给客户端；如果失败则retry，正常的请求处理流程如下图<br><img src="https://pic3.zhimg.com/80/v2-618a3dfc4c9169a6b486416c2c510516_hd.jpg" alt=""></li><li>在收到客户端append log 请求后，检测是否最新的log index大于nexIndex 中的值，如果是，则需要给follower 发送AppendEntries RPC请求<ul><li>请求成功：更新nextIndex和matchIndex</li><li>请求失败：一般是因为leader重选导致<em>数据不一致</em>，则减小nextIndex 重新发送AppendEntries RPC，如此往复，直到找到follower 与 leader 同步的最近一条log entry为止<br><img src="https://pic2.zhimg.com/80/v2-d81511e3cf92859cae0f37d2a05da2e5_hd.jpg" alt=""></li></ul></li><li>如果存在N， N&gt;CommitIndex，大多数matchIndex[follower]&gt;=N，且log[N].term == currentTerm，则将commitIndex 置为N</li></ul></li></ul></li></ul><h2 id="实现Raft的数据结构"><a href="#实现Raft的数据结构" class="headerlink" title="实现Raft的数据结构"></a>实现Raft的数据结构</h2><ul><li><p>消息状态划分</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Uncommit: 未提交转态（Client发到主节点，主节点还没有得到大多数从节点的提交回执）</span><br><span class="line">Commited: 已提交转态（从节点收到主节点的消息提交，还未收到确认报文）</span><br><span class="line">Applied: 已确认转态（从节点收到主节点的确认报文，或主节点已收到大多数从节点的提交回执）</span><br></pre></td></tr></table></figure></li><li><p>State ：每个节点的状态</p><ul><li>在所有节点上都有的</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//实际落盘的</span><br><span class="line">currentTerm：通过rpc接到的最新的任期，初始化为0，随着选举次数增加而增加</span><br><span class="line">votedFor: 保存着一次选举过程中所投的candidateId，为空表示还未投票</span><br><span class="line">log[]: log entries集合，每个entry由记录和所属任期组成 tuple2&lt;command,term&gt;</span><br><span class="line">//在内存中实时可见的</span><br><span class="line">commitIndex: 已确认被commit了的最高位的log entry index</span><br><span class="line">lastApplied: 被当前节点applied的最高位的log entry index</span><br></pre></td></tr></table></figure><ul><li>在leader 上的状态，每一次选举过后都会在新的leader上重新初始化</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nextIndex[]: 保存着每一个follower节点的下一个log entry index;初始化中leader last log index +1</span><br><span class="line">matchIndex[]: 保存着每一个follower已经被确认replicate成功的最高位的log entry index；初始化为0</span><br></pre></td></tr></table></figure><ul><li><p>RequestVote RPC 工作模式<br><img src="https://pic1.zhimg.com/80/v2-0b504b0909c97d306c936e4d8a422ee8_hd.jpg" alt=""></p></li><li><p>AppendEntries RPC工作模式</p><ul><li>由leader 发起log replicate，以及维护leader to follower 心跳，防止新一轮election触发<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">//rpc 请求参数</span><br><span class="line">term：leader term</span><br><span class="line">leaderId:</span><br><span class="line">pervLogIndex: 上一次apply过的 log 对应的Index</span><br><span class="line">prevLogTerm: 上一次apply过的log 对应的term</span><br><span class="line">entries[]: 要同步的log entries，之所以是数组是优化性能，减少rpc调用次数</span><br><span class="line">leaderCommit: leader最近一次提交的commitIndex</span><br><span class="line">//rpc 返回值</span><br><span class="line">term: follower 当前的term</span><br><span class="line">succss: 如果follower mactch了prevLogIndex和prevLogTerm返回true</span><br><span class="line">//replicate 处理逻辑</span><br><span class="line">如果term&lt; currentTerm，则返回false</span><br><span class="line">如果match 不上prevLogIndex和prevLogTerm 则返回fase</span><br><span class="line">如果当前节点存在相同index但是不同term的entry，则强制删掉该index之后所有的entry，从该节点往后同步leader log entry</span><br><span class="line">如果 leaderCommit &gt; commitIndex, 将commitIndex 设置为min(leaderCommit, index of last new entry)</span><br></pre></td></tr></table></figure></li></ul></li></ul><p><img src="https://pic2.zhimg.com/80/v2-01f3b77c9c02f8e2b949b57418f140e1_hd.jpg" alt=""></p><h2 id="Leader崩溃"><a href="#Leader崩溃" class="headerlink" title="Leader崩溃"></a>Leader崩溃</h2><h3 id="如何保证follower跟新leader的数据一致性"><a href="#如何保证follower跟新leader的数据一致性" class="headerlink" title="如何保证follower跟新leader的数据一致性"></a>如何保证follower跟新leader的数据一致性</h3><ul><li>问题：旧leader挂掉之后，follower通过心跳感知，并转为candidate，触发新一轮选举。新leader产生之后，leader和follower之间很可能存在数据不一致的情况：某些log entry在leader上不存在</li><li>Raft的做法是：leader会强制follower 完全复制自己的数据，这样会导致follower上的log entries 可能会被覆写删除（<strong>Kafka中partition leader与follower 之间的Sync参考了这一点</strong>）<br><img src="https://pic4.zhimg.com/80/v2-45bb7e6e89a1d8d5d421d29e3f3a3f5b_hd.jpg" alt=""><ul><li>如上图，通过不断的retry之后找到leader和follower之间一致的log entry；从那个entry之后开始同步（强行覆写）<h3 id="如何防止brain-split后log-entries正确性"><a href="#如何防止brain-split后log-entries正确性" class="headerlink" title="如何防止brain split后log entries正确性"></a>如何防止brain split后log entries正确性</h3></li></ul></li><li>问题：如果集群中某一个follower 由于网络问题，长时间没收到leader心跳，如果这时它选自己为leader，等到网络恢复后是不是会成为新的leader覆写之前被commit 的log entry？</li><li>Raft做法：增加被选为Leader的限制(<strong>参考性质*Leader Completeness</strong>)<ul><li>Raft 确保只有那些包含所有committed log entries（majority） 的candidates 才有资格被选为leader </li><li>实现：Vote RPC中包含了candidate 的log 信息，这样voter就可以通过对比自己的日志中log entry 的 index和term来判断candidate 是不是比自己日志更latest<h3 id="如何继续leader-crash之前的commit操作"><a href="#如何继续leader-crash之前的commit操作" class="headerlink" title="如何继续leader crash之前的commit操作"></a>如何继续leader crash之前的commit操作</h3></li></ul></li><li>这个问题存在的前提是新一轮leader election 被选为新leader的节点上保存了上一个leader 未commit成功的log entry；<strong>在raft协议中只确保commit 当前leader中的log entries会按照副本数机制实现(num of replicas &gt; num of node / 2  )</strong><br><img src="https://pic3.zhimg.com/80/v2-51183acb3121e6f1ad07bfc4b49e4ef6_hd.jpg" alt=""><ul><li>这种确保的是：如果一条log entry 被当前leader commit成功，那么可以认为之前所有的entries 都commit成功了（<strong>参考特性5 — Log Matching Property</strong> ），<strong>也不需将之前的log entry的term 改成current term</strong></li></ul></li></ul><h2 id="Follower-amp-amp-Candidate崩溃"><a href="#Follower-amp-amp-Candidate崩溃" class="headerlink" title="Follower&amp;&amp;Candidate崩溃"></a>Follower&amp;&amp;Candidate崩溃</h2><ul><li>follower 和 candidate 崩溃处理方式比较简单<ul><li>如果一个follower 或者 candidate 挂掉了，RequestVote 和 AppendEntries RPC 都会失败，处理的方式就是无限次的retry，只要服务重启，就能随着rpc 同步到最新的状态</li></ul></li></ul><h2 id="集群扩缩容"><a href="#集群扩缩容" class="headerlink" title="集群扩缩容"></a>集群扩缩容</h2><ul><li>目前我们讨论的都是在一组固定的节点上操作，但是在现实中存在因为节点的down掉以及扩容的需求，需要变更集群节点。 如果直接变更的话，可能会出现一段时间brain split的情况。最稳妥的方案就是将服务全部下线，扩容完成之后再重新上线，但是这过于低效<br><img src="https://pic4.zhimg.com/80/v2-fd51d6fa5b917864b632463c272b301b_hd.jpg" alt=""><ul><li>如图表示的是滚动升级的情况，逐个重启旧server，会存在新旧两个leader同时存在的情况（各自都赢得了所在集群大多数的vote）</li></ul></li><li>解决方案：<em>引入一种特殊类型的log entry</em>，专门用来做集群配置更替，把它叫做C (old,new)，当C(old,new)被commit之后集群进入 joint consensus（联合一致性），即<em>新旧集群共存</em>的状态。在这种状态下，需遵循的规则如下：<ul><li>Log entries将被replicate到新旧配置的所有server节点中</li><li>任何一个节点通过新旧任何一份配置都有权利在选举中成为leader</li><li>选举结果和log entry commitment的决定需要各自配置中的大多数节点认可</li></ul></li><li>讨论集群扩容的例子<br><img src="https://pic4.zhimg.com/80/v2-01972efd90a47cab1e3e3339a6c4d2e7_hd.jpg" alt=""><ul><li>第一阶段：逐台变更时，部分server上处于C(old,new) 状态，此时leader选举只能从C(old, new) 或 C(old) 中产生，具体取决于candidate是否接收到了C(old,new)  log entry；当C(old, new) 被最终committed，则只拥有C(new)和C(old) 的server将再无法被选举为leader（<strong>参考特性4 — Log Matching Property</strong>）</li><li>第二阶段：接着再引入一种log entry C(new) ，将它同步到所有节点，等C(new) 最终committed之后则集群切到了C(new)</li></ul></li><li>需注意的点<ul><li>新上的节点会存在相对于老集群数据落后的情况，需要一段时间的sync，以追上其他节点，这期间不做任何<em>投票</em>操作（此处可类比Doris 里面Observer的设计理念）</li><li>第二阶段结束时，下掉的节点可能不在新集群的配置里面，也就不会接收到心跳，这样可能触发下掉的server leader选举<ul><li>为防止扰乱集群可以规定：server如果在timeout允许的范围内正常的接收到了leader的心跳，则会忽略其他RequestVote Rpc请求</li></ul></li></ul></li></ul><h2 id="日志压缩"><a href="#日志压缩" class="headerlink" title="日志压缩"></a>日志压缩</h2><ul><li>日志如果不做压缩处理，理论上会无限期膨胀，期间可能很多重复多余的数据，浪费空间</li><li>最简单的做法就是利用snapshot，将系统整个的状态数据作为一个snapshot保存到stable storage上，这样在上一个时间点的snapshot就可以被删除了（FLink的 checkpoint 和Doris的metadata里面也是这么做的）<br><img src="https://pic3.zhimg.com/80/v2-8f6e62d58a0ac891021bc119c3f9f1de_hd.jpg" alt=""></li><li>一些其他的方式如：LSM Tree, log cleaning 等都可以</li></ul><h2 id="客户端设计的原则"><a href="#客户端设计的原则" class="headerlink" title="客户端设计的原则"></a>客户端设计的原则</h2><ul><li>首先客户端需要具备请求超时重发机制：请求random server会被reject，如果leader 挂掉触发选举也需要再一次的retry</li><li>Raft 对客户端的设计目标是要实现线性一致性语义，这样要求客户端每次command需要分配一个unique serial numer，在server端的state machine中会跟踪client最近一次的serial number，如果被serial number表示的command已经被执行完了则不会被再次执行（<strong>类似Doris 里面mini load Label的概念</strong>）</li><li><strong>只读订阅需求</strong>：（<strong>范例可了解Doris 元数据设计</strong>）为了降低leader节点的负载，可以允许client 请求follower节点读取数据；但是有一个缺点就是随着leader选举的过程，可能会读到过期的数据（被commited的数据没有被读到，这不满足线性一致性设计理念），针对这个, 有两种预防措施<ul><li>主节点选举成功之后，立即发一个空的log entry到所有节点，这样就触发了集群中所有follower节点向leader强制同步的过程</li><li>主节点在响应read-only请求之前必须确认自己是否已经过期，防止自身的信息处于过期的状态；<strong>确认方法是集群中大多数节点发送心跳</strong></li></ul></li></ul><h2 id="与Paxos的差异"><a href="#与Paxos的差异" class="headerlink" title="与Paxos的差异"></a>与Paxos的差异</h2><ul><li><a href="https://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">Paxos</a> 可以同时提交和处理多个提案，但是发生冲突时，理论上会有更高的延时（协商时间），而Raft算法会天生地把消息确定一个先后顺序。大幅减少了冲突的可能性</li></ul>]]></content>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
            <tag> 大数据 </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>【spark-tips】spark2.4.0触发的executor内存溢出排查</title>
      <link href="/2019/01/12/%E3%80%90spark-tips%E3%80%91spark2-4-0%E8%A7%A6%E5%8F%91%E7%9A%84executor%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E6%8E%92%E6%9F%A5/"/>
      <url>/2019/01/12/%E3%80%90spark-tips%E3%80%91spark2-4-0%E8%A7%A6%E5%8F%91%E7%9A%84executor%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E6%8E%92%E6%9F%A5/</url>
      <content type="html"><![CDATA[<h3 id="版本升级背景"><a href="#版本升级背景" class="headerlink" title="版本升级背景"></a>版本升级背景</h3><p>spark 2.4.0 最近刚发版，新增了很多令人振奋的特性。由于本司目前使用的是spark 2.3.0版本，本没打算这么快升级到2.4.0。无奈最近排查出的两个大bug迫使我们只能对spark进行升级。排查的两个bug如下：<br></p><ul><li><p>spark2.3.0 bug导致driver跑一段时间内存溢出，经过dump下来的堆转储文件发现占绝大内存的对象是spark自身的ElementTrackingStore。这是统计任务运行时资源占用情况的类，在每一个批次处理完之后都没有释放，导致driver内存溢出<br>  <img src="http://jacobs.wanhb.cn/images/memory.png" alt=""></p><ul><li>详情参考文章：<a href="https://www.cnblogs.com/bethunebtj/p/9103547.html" target="_blank" rel="noopener">导致driver OOM的一个SparkPlanGraphWrapper源码的bug</a></li></ul></li><li>spark streaming 2.3.0 + kafka 0.8.2.1 + zk管理offset 每次重启，会导致offsetrange的左区间莫名向右移动若干offset size，导致每个批次通过offsetrange从kafka消费的数据普遍会丢失部分数据，具体问题还在通过源码定位中</li></ul><p>第一个bug在spark 2.4.0中得到解决（<a href="https://issues.apache.org/jira/browse/SPARK-23670" target="_blank" rel="noopener">参考issue</a>），于是对spark进行了升级。所幸spark升级对spark on yarn这种运行方式来说非常解耦，只需要定义spark.jars依赖就行，yarn nodemanager会对依赖包进行下载。</p><h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>spark 版本升级之后，当天对在线任务观察，运行平稳，上一节提到的bug也修复了；但是第二天离线任务的运行却出现了问题：部分离线任务在做聚合运算的时候出现executor 集体内存溢出，任务执行失败</p><h4 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h4><ul><li><p>查看日志发现是内存溢出导致executor触发钩子异常退出</p><p>  <img src="http://jacobs.wanhb.cn/images/spark_1.png" alt=""></p></li><li><p>进一步发现在某个计算步骤，需要读入上一个步骤写入hdfs的数据，每个task处理的数据量比较大，且都放到了内存中（<em>导火线</em>）<br>  <img src="http://jacobs.wanhb.cn/images/spark_2.png" alt=""></p></li><li><p>接着因为要做各种聚合运算（reduceby, groupgy, join…）execution 的内存也不断增大，濒临内存的限制边缘 8G * 0.6(spark.memory.fraction) =4.8G，很容易就会来不及spill到磁盘，导致内存溢出<br>  <img src="http://jacobs.wanhb.cn/images/spark_2.png" alt=""></p></li><li><p>于是基本可以得到问题原因：从读入hdfs的源头去排查，为什么导致一个task处理的数据量过大；发现hdfs中上一步save到hdfs中的每一个part都是将近500M大小的parquet+snappy 压缩文件，而这种格式无法切分，导致一个map task只能对这400多M的文件照单全收，而由于我应用申请的配置是 <em>8 cores 8 mem / executor</em> 导致8个task同时读入大文件到executor jvm中，最终jvm报内存溢出异常<br>  <img src="http://jacobs.wanhb.cn/images/spark_4.png" alt=""></p></li></ul><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ul><li>限制executor 并行度，将cores 调小，内存适当调大</li><li>由于上一步写hdfs的操作并行度太小（只有40），重新调整并行度，让输出的每个part文件减小</li></ul>]]></content>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Flink实战总结</title>
      <link href="/2018/12/20/Flink%E5%AE%9E%E6%88%98%E6%80%BB%E7%BB%93/"/>
      <url>/2018/12/20/Flink%E5%AE%9E%E6%88%98%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink 近几年来一直备受业界瞩目，相对于同时期一夜成名的Spark来说，有种厚积薄发的味道。 当然，从根本上来看，也是因为这几年对于实时分布式计算引擎的需求日渐强烈，要求也越来越高（数据的latency，一致性）。而这也意味着以微批次来fake实时处理的Spark Streaming不再能满足实时处理系统的硬性要求(忽略spark continuous processing实现)。最近本司也正在考虑将实时处理任务从Spark Streaming迁移到Flink；于是就有了下面这篇实战总结文章。</p><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><ul><li>flink程序能实现在分布式的结合上进行各种转换操作，集合通常来自订阅的来源（文件，kafka,local,in-memory），结果被返回到sinks里（大多数写入分布式文件系统，或者标准输出）</li><li>DataSet and DataStream<ul><li>DataSet和DataStream在flink中都代表一种数据结构，是不可变且包含重复记录的集合。区别在于DataSet是有限的集合，而DataStream是无界的</li></ul></li><li>flink 配置interlij ideal 在本地运行调试<ul><li>只需要将flink依赖的包引入项目中即可启动项目<br><img src="http://jacobs.wanhb.cn/images/flink1.png" alt="HBase 架构图"></li></ul></li><li><strong>讲解Flink怎么序列化objects，怎么分配内存</strong><a href="https://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html" target="_blank" rel="noopener">Apache Flink: Juggling with Bits and Bytes</a></li></ul><h3 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a>DataStream</h3><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/datastream_api.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Flink DataStream API Programming Guide</a></li><li>datasource（数据源）: <ul><li>File-based: readTextFile, readFile…</li><li>Socket-based: socketTextStream</li><li>Collection-based: fromCollection, fromElements</li><li>custom: addSource, FlinkKafkaConsumer08 or other connectors</li></ul></li></ul><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Flink DataSet API Programming Guide</a></li></ul><h3 id="savepoint"><a href="#savepoint" class="headerlink" title="savepoint"></a>savepoint</h3><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/savepoints.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Savepoints</a></li><li>Savepoints are created, owned, and deleted by the user.</li><li>目前savepoint和checkpoint实现和format方式都相同（除了checkpoint选择了rocksdb作为state backend，这样format会有些微不同）</li><li>Operations：<ul><li>Triggering Savepoints： FsStateBackend or RocksDBStateBackend:</li><li>Trigger a Savepoint</li><li>Cancel Job with Savepoint<ul><li><code>bin/flink cancel -s [:targetDirectory] :jobId</code></li></ul></li><li>Resuming from Savepoints <ul><li><code>$ bin/flink run -s :savepointPath [:runArgs]</code></li></ul></li><li>Disposing Savepoints<ul><li><code>$ bin/flink savepoint -d :savepointPath</code></li></ul></li></ul></li></ul><h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h3><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/checkpoints.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Checkpoints</a></li><li>生命周期是由Flink管理，checkpoint的管理，创建以及释放统一通过Flink，而不需要用户干预</li><li><strong>Checkpoints are usually dropped（随应用退出被清除）</strong> after the job was terminated by the user (except if explicitly configured as retained Checkpoints)</li><li>checkpoint 优化 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/state/large_state_tuning.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Tuning Checkpoints and Large State</a><ul><li>state 双写：一份在distributed storage(HDFS)；一份在local</li><li>task-local recovery：默认是关闭的状态,可以通过<code>state.backend.local-recovery</code> 打开</li></ul></li></ul><h3 id="Barriers"><a href="#Barriers" class="headerlink" title="Barriers"></a>Barriers</h3><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-master/internals/stream_checkpointing.html" target="_blank" rel="noopener">Apache Flink 1.8-SNAPSHOT Documentation: Data Streaming Fault Tolerance</a></li></ul><h3 id="Window，waterMark，Trigger"><a href="#Window，waterMark，Trigger" class="headerlink" title="Window，waterMark，Trigger"></a>Window，waterMark，Trigger</h3><ul><li><a href="https://www.jianshu.com/p/a883262241ef" target="_blank" rel="noopener">Window，waterMark，Trigger介绍- 简书</a></li><li>window<ul><li>滚动窗口：分配器将每个元素分配到一个指定窗口大小的窗口中，并且不会重叠；TumblingEventTimeWindows.of(Time.seconds(5))</li><li>滑动窗口：滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率；因此可能出现窗口重叠，如果滑动参数小于滚动参数的话；SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))</li><li>会话窗口：通过session活动来对元素进行分组，跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况。当他在一个固定的时间周期内不再收到元素，即非活动间隔产生，那么窗口就会关闭；<ul><li>一个session窗口通过一个session间隔来配置，这个session间隔定义了非活跃周期的长度。当这个非活跃周期产生，那么当前的session将关闭并且后续的元素将被分配到新的session窗口中去。如：EventTimeSessionWindows.withGap(Time.minutes(10)</li></ul></li></ul></li><li>触发器(Triggers)<ul><li>触发器定义了一个窗口何时被窗口函数处理</li><li>EventTimeTrigger</li><li>ProcessingTimeTrigger</li><li>CountTrigger</li><li>PurgingTrigger</li></ul></li><li>驱逐器(Evictors)</li></ul><h2 id="任务提交与停止姿势"><a href="#任务提交与停止姿势" class="headerlink" title="任务提交与停止姿势"></a>任务提交与停止姿势</h2><ul><li><p>任务提交</p><ul><li><strong>启动命令详解</strong> :<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: YARN Setup</a></li><li><p>参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Usage:</span><br><span class="line">   Required</span><br><span class="line">     -n,--container &lt;arg&gt;   Number of YARN container to allocate (=Number of Task Managers)</span><br><span class="line">   Optional</span><br><span class="line">     -D &lt;arg&gt;                        Dynamic properties</span><br><span class="line">     -d,--detached                   Start detached</span><br><span class="line">     -jm,--jobManagerMemory &lt;arg&gt;    Memory for JobManager Container with optional unit (default: MB)</span><br><span class="line">     -nm,--name                      Set a custom name for the application on YARN</span><br><span class="line">     -q,--query                      Display available YARN resources (memory, cores)</span><br><span class="line">     -qu,--queue &lt;arg&gt;               Specify YARN queue.</span><br><span class="line">     -s,--slots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class="line">     -tm,--taskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with optional unit (default: MB)</span><br><span class="line">     -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths for HA mode</span><br></pre></td></tr></table></figure></li><li><p>提交到yarn-cluster上需要以 ::y:: 或者::yarn::作为前缀；如: <code>ynm=nm</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink run -c com.jacobs.jobs.realtime.wordcount.WindowWordCount target/real-time-jobs-1.0.0-SNAPSHOT.jar</span><br><span class="line"></span><br><span class="line">flink run -m yarn-cluster -ynm TestSinkUserLogStream -yn 4 -yjm 1024m -ytm 4096m -ys 4 -yqu feed.prod -c com.weibo.api.feed.dm.stream.TestFlinkStream /data1/dm-flink/feed-dm-flink-1.0.4-SNAPSHOT.jar</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>停止任务</strong></p><ul><li><strong>关闭或重启flink程序不能直接kill掉</strong>，这样会导致flink来不及制作checkpoint，而应该调用flink提供的cancel语意</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//重启正确姿势, with savepoint</span><br><span class="line">1. 调用cancel，cancel之前先触发savepoint</span><br><span class="line">bin/flink cancel -s [:targetDirectory] :jobId -yid: yarnAppId</span><br><span class="line">例子: flink cancel -s hdfs://vcp-yz-nameservice1/user/hcp/hcpsys/feed/flink-checkpoints/test-user-logs 97b4e67859af4bfb1b597355f1c846f3 -yid application_1542801635735_2121</span><br><span class="line">2. 从savepoint中恢复flink程序</span><br><span class="line">bin/flink run -s :savepointPath [:runArgs]</span><br><span class="line">例子: flink run -s hdfs://vcp-yz-nameservice1/user/hcp/hcpsys/feed/flink-checkpoints/test-user-logs/savepoint-97b4e6-22dd5890dd0c -m yarn-cluster -ynm TestSinkUserLogStream -yn 4 -yjm 1024m -ytm 4096m -ys 4 -yqu feed.prod -c com.weibo.api.feed.dm.stream.TestFlinkStream /data1/dm-flink/feed-dm-flink-1.0.4-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h2 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h2><h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><ul><li>standalone 启动cluster<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/flink-1.6.0/bin;./start-cluster.sh</span><br></pre></td></tr></table></figure></li></ul><h3 id="On-Yarn-Cluster"><a href="#On-Yarn-Cluster" class="headerlink" title="On Yarn Cluster"></a>On Yarn Cluster</h3><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: YARN</a></li><li><strong>参考文章</strong><a href="https://zhouhai02.com/post/flink-internals/flink1.6-flip6-flink-on-yarn-arch/" target="_blank" rel="noopener">Flink1.6 - flink on yarn分布式部署架构 - 深山含笑</a></li><li><p>架构图<br><img src="http://jacobs.wanhb.cn/images/flink2.png" alt="HBase 架构图"></p><ul><li>JobManager 和 ApplicationMaster  运行在同一个JVM里</li></ul></li><li><p><strong>on yarn 两种模式</strong></p><ul><li>session模式：允许运行多个作业在同一个Flink集群中，代价是作业之间没有资源隔离（同一个TM中可能跑多个不同作业的task）</li><li>per-job模式（生产环境）：per-job模式是指在yarn上为每一个Flink作业都分配一个单独的Flink集群，这样就解决了不同作业之间的资源隔离问题</li></ul></li><li><strong>摘录参考文章</strong>相比旧的Flink-on-YARN架构（Flink 1.5之前），新的yarn架构带来了以下的优势：<ul><li>client可以直接在yarn上面启动一个作业，不在像以前需要先启动一个固定大小的Flink集群然后把作业提交到这个Flink集群上</li><li>按需申请容器（指被同一个作业的不同算子所使用的容器可以有不同的CPU/Memory配置），没有被使用的容器将会被释放<br><img src="http://jacobs.wanhb.cn/images/flink3.png" alt="HBase 架构图"></li></ul></li><li>slot资源申请/分配流程分析</li><li>请求新TaskExecutor的slot分配<br><img src="http://jacobs.wanhb.cn/images/flink4.png" alt="HBase 架构图"></li><li>ResourceManager挂掉 ：不会挂掉task,不断尝试重新注册ResourceManager<strong>详细见参考文章</strong></li><li>TaskExecutor挂掉</li><li>JobMaster挂掉</li></ul><h2 id="资源分配相关？"><a href="#资源分配相关？" class="headerlink" title="资源分配相关？"></a>资源分配相关？</h2><ul><li>在operator中对并行度的设置将决定任务分配到几个task slot里面去</li></ul><h2 id="Flink程序运行流程分解"><a href="#Flink程序运行流程分解" class="headerlink" title="Flink程序运行流程分解"></a>Flink程序运行流程分解</h2><h3 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h3><ul><li><ol><li>Obtain an execution environment</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">getExecutionEnvironment()</span><br><span class="line">createLocalEnvironment()</span><br><span class="line">createRemoteEnvironment(host: String, port: Int, jarFiles: String*)</span><br></pre></td></tr></table></figure><ul><li><ol><li>Load/create the initial data</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val text: DataStream[String] = env.readTextFile(&quot;file:///path/to/file&quot;)</span><br></pre></td></tr></table></figure><ul><li><ol><li>Specify transformations on this data</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//create a new DataStream by converting every String in the original collection to an integer</span><br><span class="line">val mapped = input.map &#123; x =&gt; x.toInt &#125;</span><br></pre></td></tr></table></figure><ul><li><ol><li>Specify where to put the results of your computations</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">writeAsText(path: String)</span><br><span class="line">print()</span><br></pre></td></tr></table></figure><ul><li><ol><li>Trigger the program execution</li></ol></li></ul><h2 id="Flink-watermark机制"><a href="#Flink-watermark机制" class="headerlink" title="Flink watermark机制"></a>Flink watermark机制</h2><ul><li><p><strong>【重要】详细讲解watermark</strong>:  <a href="https://blog.csdn.net/lmalds/article/details/52704170" target="_blank" rel="noopener">Flink流计算编程—watermark（水位线）</a> </p><ul><li><p>window 触发的两个条件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1、watermark时间 &gt;= window_end_time</span><br><span class="line">2、在[window_start_time,window_end_time)中有数据存在</span><br></pre></td></tr></table></figure></li></ul></li><li><p><a href="https://zhuanlan.zhihu.com/p/20585530" target="_blank" rel="noopener">摘录：深入理解Flink核心技术 </a></p><ul><li>纵坐标为event_time，横坐标为processingTime，理想情况自然是两者一致，但实际情况肯定不可能<br><img src="http://jacobs.wanhb.cn/images/flink5.png" alt="HBase 架构图"></li></ul></li><li><a href="http://shiyanjun.cn/archives/1785.html" target="_blank" rel="noopener">摘录：使用EventTime与WaterMark进行流数据处理</a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> // 这块结合上图理解watermark的值</span><br><span class="line">@Override</span><br><span class="line">    public final Watermark getCurrentWatermark() &#123;</span><br><span class="line">        long potentialWM = currentMaxTimestamp - maxOutOfOrderness; // 当前最大事件时间戳，减去允许最大延迟到达时间</span><br><span class="line">        if (potentialWM &gt;= lastEmittedWatermark) &#123; // 检查上一次emit的WaterMark时间戳，如果比lastEmittedWatermark大则更新其值</span><br><span class="line">            lastEmittedWatermark = potentialWM;</span><br><span class="line">        &#125;</span><br><span class="line">        return new Watermark(lastEmittedWatermark);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><ul><li>Windowing, WaterMark,Trigger 三者依赖关系<ul><li>Windowing：就是负责该如何生成Window，比如Fixed Window、Slide Window，当你配置好生成Window的策略时，Window就会根据时间动态生成，最终得到一个一个的Window，包含一个时间范围：[起始时间, 结束时间)，它们是一个一个受限于该时间范围的事件记录的容器，每个Window会收集一堆记录，满足指定条件会触发Window内事件记录集合的计算处理</li><li>WaterMark：它其实不太好理解，可以将它定义为一个函数E=f(P)，当前处理系统的处理时间P，根据一定的策略f会映射到一个事件时间E，可见E在坐标系中的表现形式是一条曲线，根据f的不同曲线形状也不同。假设，处理时间12:00:00，我希望映射到事件时间11:59:30，这时对于延迟30秒以内（事件时范围11:59:30~12:00:00）的事件记录到达处理系统，都指派到时间范围包含处理时间12:00:00这个Window中。事件时间超过12:00:00的就会由Trigger去做补偿了。</li><li>Trigger：为了满足实际不同的业务需求，对上述事件记录指派给Window未能达到实际效果，而做出的一种补偿，比如事件记录在WaterMark时间戳之后到达事件处理系统，因为已经在对应的Window时间范围之后，我有很多选择：选择丢弃，选择是满足延迟3秒后还是指派给该Window，选择只接受对应的Window时间范围之后的5个事件记录，等等，这都是满足业务需要而制定的触发Window重新计算的策略，所以非常灵活。</li></ul></li></ul><h2 id="Sink-Connectors"><a href="#Sink-Connectors" class="headerlink" title="Sink Connectors"></a>Sink Connectors</h2><ul><li>Kafka </li><li>Elasticsearch</li><li>RabbitMQ</li><li>Rolling File Sink (HDFS)<ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/filesystem_sink.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: HDFS Connector</a></li></ul></li><li>Streaming File Sink <ul><li>partfile 有三种状态：in-progress, pending,finished；part file先被写成in-progress，一旦被关闭写入，会变成pending，当检查点成功之后，pending状态的文件将变成finished; </li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/connectors/streamfile_sink.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Streaming File Sink</a></li><li>Using Row-encoded Output Formats <ul><li>可以指定RollingPolicy 来滚动生成分区中的文件</li></ul></li><li>Using Bulk-encoded Output Formats<ul><li><strong>支持parquet，orc等文件格式</strong>，批量编码文件</li><li>通过BulkWriter.Factory定义不同的文件格式   <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/api/java/org/apache/flink/formats/parquet/avro/ParquetAvroWriters.html" target="_blank" rel="noopener">ParquetAvroWriters (flink 1.7-SNAPSHOT API)</a><br>/Users/lichao15/Documents/github/awesome-big-data/README.md        - <strong>源码：</strong> <a href="https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java" target="_blank" rel="noopener">flink/StreamingFileSink.java at master · apache/flink · GitHub</a></li><li>使用这种方式只能配合 <code>OnCheckpointRollingPolicy</code>  使用来滚动生成分区文件，通过设置 <code>env.enableCheckpointing(interval)</code>来设置文件滚动间隔</li><li><strong>Streaming to parquet in hdfs 出现问题，内存溢出导致job无限崩溃重启，大量part file</strong></li><li>如果失败，将从上一个检查点开始重新store，期间回滚in-progress中的文件，以确保不会重复保存上一个检查点之后的数据</li><li><strong>part文件过多问题</strong> <a href="https://stackoverflow.com/questions/52638193/streaming-to-parquet-files-not-happy-with-flink-1-6-1" target="_blank" rel="noopener">Streaming to parquet files not happy with flink 1.6.1 - Stack Overflow</a></li><li><strong>rolling parquet file 重点邮件</strong> <a href="http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Streaming-to-Parquet-Files-in-HDFS-td23492.html" target="_blank" rel="noopener">Apache Flink User Mailing List archive. - Streaming to Parquet Files in HDFS</a></li><li>注意压缩的时候内存溢出的情况，flink陷入无限的重启循环中<br><img src="http://jacobs.wanhb.cn/images/flink6.png" alt="HBase 架构图"></li></ul></li></ul></li></ul><h2 id="StreamingFileSink与Kafka-结合"><a href="#StreamingFileSink与Kafka-结合" class="headerlink" title="StreamingFileSink与Kafka 结合"></a>StreamingFileSink与Kafka 结合</h2><h3 id="如何做到exactly-once？"><a href="#如何做到exactly-once？" class="headerlink" title="如何做到exactly once？"></a>如何做到exactly once？</h3><ul><li><a href="https://flink.apache.org/features/2018/03/01/end-to-end-exactly-once-apache-flink.html" target="_blank" rel="noopener"> An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)</a><ul><li>二阶段提交</li></ul></li><li>partfile 有三种状态：in-progress, pending,finished；part file先被写成in-progress，一旦被关闭写入，会变成pending，当检查点成功之后，pending状态的文件将变成finished;</li><li>如果失败，将从上一个检查点开始重新store，期间回滚in-progress中的文件，以确保不会重复保存上一个检查点之后的数据<h3 id="flink如何控制kafka-offset提交与checkpoint-amp-amp-savepoint相结合"><a href="#flink如何控制kafka-offset提交与checkpoint-amp-amp-savepoint相结合" class="headerlink" title="flink如何控制kafka offset提交与checkpoint&amp;&amp;savepoint相结合"></a>flink如何控制kafka offset提交与checkpoint&amp;&amp;savepoint相结合</h3></li><li><a href="http://m.sohu.com/a/168546400_617676" target="_blank" rel="noopener">FlinkKafkaConsumer使用详解</a></li><li>关闭checkpoint(<strong>Checkpointingdisabled</strong>): <ul><li>此时， Flink Kafka Consumer依赖于它使用的具体的Kafka client的自动定期提交offset的行为，相应的设置是 Kafka properties中的 enable.auto.commit (或者 auto.commit.enable 对于Kafka 0.8) 以及 auto.commit.interval.ms</li></ul></li><li>开启checkpoint(<strong>Checkpointingenabled</strong>):<ul><li>在这种情况下，Flink Kafka Consumer会将offset存到checkpoint中</li><li><strong>制作完checkpoint 一并提交offsets</strong> 当checkpoint 处于completed的状态时（<strong>整个job的所有的operator都收到了这个checkpoint的barrier</strong>）。将offset记录起来并提交，从而保证exactly-once</li></ul></li></ul><ul><li>::exactly once的两个风险点：可结合savepoint来做::<ul><li><ol><li>异常退出的情况，没法来得及做checkpoint，而checkpoint间隔太长会导致丢失大量数据；可以通过airflow周期性手动触发savepoint恢复；封装hflink脚本<ul><li>解决思路是结合savepoint来做，通过<strong>airflow定时的触发savepoint</strong>操作，防止因checkpoint未及时做数据丢失</li><li>规定一分钟savepoint一次，这样即使分钟级别的数据丢失也是可以容忍</li></ul></li></ol></li><li><ol><li>第一点利用savepoint来做也有风险：在做savepoint的时候，如果异常退出，parfile未及时关闭导致数据丢失<ul><li><strong>暂时可以认为问题较小？</strong></li></ul></li></ol></li></ul></li></ul><h3 id="如何控制背压"><a href="#如何控制背压" class="headerlink" title="如何控制背压"></a><strong>如何控制背压</strong></h3><ul><li>如何做到挂很久之后重新启动时限制拉取的消息量？（类似spark.streaming.kafka.maxRatePerPartition）<ul><li>背压通过task slot 的stackTrace判断</li><li>可以在kafka source那层控制一次性消费量，类似于spark</li></ul></li></ul><h2 id="Flink-高性能部署"><a href="#Flink-高性能部署" class="headerlink" title="Flink 高性能部署"></a>Flink 高性能部署</h2><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-master/ops/jobmanager_high_availability.html" target="_blank" rel="noopener">Apache Flink 1.8-SNAPSHOT Documentation: JobManager High Availability (HA)</a></li></ul><h2 id="metric监控rest-api"><a href="#metric监控rest-api" class="headerlink" title="metric监控rest api"></a>metric监控rest api</h2><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-master/monitoring/rest_api.html" target="_blank" rel="noopener">Apache Flink 1.8-SNAPSHOT Documentation: Monitoring REST API</a></li></ul><h2 id="Restart-Strategies"><a href="#Restart-Strategies" class="headerlink" title="Restart Strategies"></a>Restart Strategies</h2><ul><li><strong>doc</strong> <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/restart_strategies.html" target="_blank" rel="noopener">Apache Flink 1.7 Documentation: Restart Strategies</a></li><li>Fixed Delay Restart Strategy</li><li>Failure Rate Restart Strategy</li><li>No Restart Strategy</li><li>Fallback Restart Strategy</li></ul>]]></content>
      
      
    </entry>
    
    <entry>
      <title>Spark实战总结</title>
      <link href="/2018/10/15/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/10/15/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Spark作为一款分布式计算查询引擎，在大数据领域逐渐扮演着越来越重要的作用。传统的MapReduce因计算模型缺陷导致在面对海量数据，复杂的计算场景下计算效率十分低下。于是Spark作为一种互补的即席查询实现方案被各大公司采用。下面是对Spark一些概念和使用的总结</p><h2 id="Spark-基础"><a href="#Spark-基础" class="headerlink" title="Spark 基础"></a>Spark 基础</h2><h3 id="Spark-的构成"><a href="#Spark-的构成" class="headerlink" title="Spark 的构成"></a>Spark 的构成</h3><ul><li>ClusterManager: 在standalone模式中即为，Master主节点，控制整个集群，监控worker。在yarn模式中为资源管理器</li><li>worker :从节点，负责控制计算节点，启动Executro和Driver。在yarn模式中NodeManager,负责计算节点的控制。</li><li>Driver：运行Application的main()函数并且创建SparkContext</li><li>Executor: 执行器，是为某Application运行在worker node上的一个进程，启动线程池运行任务上，每个Application拥有一组独立的executors</li><li>SparkContext: 整个应用程序的上下文，控制整个应用的生命周期</li><li>RDD：Spark的基本计算单元，一组RDD形成执行的有向无环图RDD Graph(DAG)</li><li>DAG Scheduler: 根据Job构建基于stage的DAG，并且提交stage给TaskScheduler</li><li>TaskScheduler: 可以将提交给它的stage 拆分为更多的task并分发给Executor执行</li><li>SparkEnv: 线程级别的上下文，存储运行时的重要组件的引用</li><li>DStream: 是一个RDD的序列，由若干RDD组成。在一个batchInterval中，会产生一个RDD，产生的数据统一塞入到这个RDD中，采用内存+磁盘的模式，尽可能放到内存中，当数据量太大时会spill到磁盘中</li></ul><h3 id="Spark-概念释义"><a href="#Spark-概念释义" class="headerlink" title="Spark 概念释义"></a>Spark 概念释义</h3><ul><li>Transformation返回值还是一个RDD。它使用了链式调用的设计模式，对一个RDD进行计算后，变换成另外一个RDD，然后这个RDD又可以进行另外一次转换。这个过程是分布式的。 Action返回值不是一个RDD。它要么是一个Scala的普通集合，要么是一个值，要么是空，最终或返回到Driver程序，或把RDD写入到文件系统中。</li><li>Action是返回值返回给driver或者存储到文件，是RDD到result的变换，Transformation是RDD到RDD的变换。只有action执行时，rdd才会被计算生成，这是rdd懒惰执行的根本所在。</li><li>Driver是我们提交Spark程序的节点，并且所有的reduce类型的操作都会汇总到Driver节点进行整合。节点之间会将map/reduce等操作函数传递一个独立副本到每一个节点，这些变量也会复制到每台机器上，而节点之间的运算是相互独立的，变量的更新并不会传递回Driver程序。</li><li>Spark中分布式执行的条件<ul><li>只要生成了task，就都是在executor中执行的，在driver中执行不会单独生成task</li><li>生成task的操作有: spark.read 读取文件，之后对文件做各种map, filter, reduce操作，都是针对partition而言的</li></ul></li></ul><h2 id="spark-工作机制"><a href="#spark-工作机制" class="headerlink" title="spark 工作机制"></a>spark 工作机制</h2><ul><li>一个Job被拆分成若干个Stage，每个Stage执行一些计算，产生一些中间结果。它们的目的是最终生成这个Job的计算结果。而每个Stage是一个task set，包含若干个task。Task是Spark中最小的工作单元，在一个executor上完成一个特定的事情。</li><li>除非用户指定持久化操作，否则转换过程中产生的中间数据在计算完毕后会被丢弃，即数据是非持久化的。</li><li>窄依赖:父RDD中的一个分区最多只会被子RDD中的一个分区使用，父RDD中，一个分区内的数据是不能被分割的，必须整个交付给子RDD中的一个分区。</li><li>宽依赖（Shuffle依赖）：父RDD中的分区可能会被多个子RDD分区使用。因为父RDD中一个分区内的数据会被分割，发送给子RDD的所有分区。因此Shuffle依赖也意味着父RDD与子RDD之间存在着Shuffle过程。</li></ul><h3 id="Spark作业"><a href="#Spark作业" class="headerlink" title="Spark作业"></a>Spark作业</h3><ul><li>Application: 用户自定义的Spark程序，用户提交之后，Spark为App分配资源程序转换并执行。</li><li>Driver Program: 运行Application的main函数并且创建SparkContext</li><li>RDD DAG： 当RDD遇到Action算子，将之前的所有算子形成一个有向无环图（DAG）。再在Spark中转化为Job,提交到集群进行执行，一个App中可以包含多个Job</li><li>Job： RDD Graph触发的作业，由spark Action算子触发，在SparkContext中通过runJob方法向spark提交Job</li><li>stage： 每个Job会根据RDD的宽依赖关系被切分很多stage ,每个stage包含一组相同的task，这一组task也叫taskset</li><li>Task: 一个分区对应一个Task,Task 执行RDD中对应stage中所包含的算子，Taksk 被封装好后放入Executor的线程池中执行。</li></ul><h2 id="spark调度原理"><a href="#spark调度原理" class="headerlink" title="spark调度原理"></a>spark调度原理</h2><h3 id="作业调度"><a href="#作业调度" class="headerlink" title="作业调度"></a>作业调度</h3><p>系统的设计很重要的一环便是资源调度。设计者将资源进行不同粒度的抽象建模，然后将资源统一放入调度器，通过一定的算法进行调度。</p><ul><li>spark的多种运行模式：Local模式，standalone模式、YARN模式，Mesos模式。</li></ul><h3 id="Standalone-VS-Yarn"><a href="#Standalone-VS-Yarn" class="headerlink" title="Standalone VS Yarn"></a>Standalone VS Yarn</h3><ul><li><p>角色对比</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">standalone:  yarn: </span><br><span class="line">client      client </span><br><span class="line">Master   ApplicationMaster </span><br><span class="line">Worker   ExecutorRunnable </span><br><span class="line">Scheduler   YarnClusterScheduler </span><br><span class="line">SchedulerBackend YarnClusterSchedulerBackend</span><br></pre></td></tr></table></figure></li><li><p>在yarn中application Master 与Application Driver 运行于同一个JVM进程中</p></li><li><p>standalone架构图</p><p>  <img src="http://jacobs.wanhb.cn/images/standalone.png" alt="standalone"></p></li><li><p>on yarn架构图</p><p>  <img src="http://jacobs.wanhb.cn/images/on%20yarn.png" alt="on yarn"></p></li></ul><h3 id="application调度"><a href="#application调度" class="headerlink" title="application调度"></a>application调度</h3><p>用户提交到spark中的作业集合，通过一定的算法对每个按一定次序分配集群中资源的过程。</p><ul><li>FIFO模式，用户先提交的作业1优先分配需要的资源，之后提交的作业再分配资源，依次类推。</li><li>Mesos: 粗粒度模式和细粒度模式</li><li><p>YARN模式：独占模式，可以控制应用分配资源</p><ul><li><p>yarn-cluster: 适用于生产环境。client将用户程序提交到到spark集群中就与spark集群断开联系了，此时client将不会发挥其他任何作用，仅仅负责提交。在此模式下。AM和driver是同一个东西，但官网上给的是driver运行在AM里，可以理解为AM包括了driver的功能就像Driver运行在AM里一样，此时的AM既能够向AM申请资源并进行分配，又能完成driver划分RDD提交task等工作</p></li><li><p>yarn-client: y适用于交互、调试，希望立即看到app的输出。Driver运行在客户端上，先有driver再用AM，此时driver负责RDD生成、task生成和分发，向AM申请资源等 ,AM负责向RM申请资源，其他的都由driver来完成</p></li></ul></li></ul><h3 id="Job调度"><a href="#Job调度" class="headerlink" title="Job调度"></a>Job调度</h3><p>Job调度就是在application内部的一组Job集合，在application分配到的资源量，通过一定的算法，对每个按一定次序分配Application中资源的过程。</p><ul><li>FIFO模式：先进先出模式</li><li>FAIR模式：spark在多个job之间以轮询的方式给任务进行资源分配，所有的任务拥有大致相当的优先级来共享集群的资源。这就意味着当一个长任务正在执行时，短任务仍可以分配到资源，提交并执行，并且获得不错的响应时间。</li></ul><h3 id="tasks延迟调度"><a href="#tasks延迟调度" class="headerlink" title="tasks延迟调度"></a>tasks延迟调度</h3><ul><li><p>数据本地性：尽量的避免数据在网络上的传输，传输任务为主，将任务传输到数据所在的节点</p></li><li><p>延时调度机制：拥有数据的节点当前正被其他的task占用，如果预测当前节点结束当前任务的时间要比移动数据的时间还要少，那么调度会等待，直到当前节点可用。否则移动数据到资源充足节点，分配任务执行。</p></li></ul><h2 id="spark-transformation和action的算子"><a href="#spark-transformation和action的算子" class="headerlink" title="spark transformation和action的算子"></a>spark transformation和action的算子</h2><h3 id="transformation"><a href="#transformation" class="headerlink" title="transformation"></a>transformation</h3><ul><li>map(func) 返回一个新的分布式数据集，由每个原元素经过func函数处理后的新元素组成 </li><li>filter(func) 返回一个新的数据集，由经过func函数处理后返回值为true的原元素组成 </li><li>flatMap(func) 类似于map，但是每一个输入元素，会被映射为0个或多个输出元素，(因此，func函数的返回值是一个seq，而不是单一元素) </li><li>mapPartitions(func) 类似于map，对RDD的每个分区起作用，在类型为T的RDD上运行时，func的函数类型必须是Iterator[T]=&gt;Iterator[U]</li><li>mapPartitionsWithIndex(func) 和mapPartitions类似，但func带有一个整数参数表上分区的索引值，在类型为T的RDD上运行时，func的函数参数类型必须是(int,Iterator[T])=&gt;Iterator[U] </li><li>sample(withReplacement,fraction,seed) 根据给定的随机种子seed，随机抽样出数量为fraction的数据 </li><li>pipe(command,[envVars]) 通过管道的方式对RDD的每个分区使用shell命令进行操作，返回对应的结果 </li><li>union(otherDataSet) 返回一个新的数据集，由原数据集合参数联合而成 </li><li>intersection(otherDataset) 求两个RDD的交集 </li><li>distinct([numtasks]) 返回一个包含源数据集中所有不重复元素的i新数据集 </li><li>groupByKey([numtasks]) 在一个由(K,v)对组成的数据集上调用，返回一个(K,Seq[V])对组成的数据集。默认情况下，输出结果的并行度依赖于父RDD的分区数目，如果想要对key进行聚合的话，使用reduceByKey或者combineByKey会有更好的性能 </li><li>reduceByKey(func,[numTasks]) 在一个(K,V)对的数据集上使用，返回一个(K,V)对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起，reduce任务的个数是可以通过第二个可选参数来配置的 </li><li>sortByKey([ascending],[numTasks]) 在类型为(K,V)的数据集上调用，返回以K为键进行排序的(K,V)对数据集，升序或者降序有boolean型的ascending参数决定 </li><li>join(otherDataset,[numTasks]) 在类型为(K,V)和(K,W)类型的数据集上调用，返回一个(K,(V,W))对，每个key中的所有元素都在一起的数据集 </li><li>cogroup(otherDataset,[numTasks]) 在类型为(K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为(K,Iterable[V],Iterable[W]) tuples </li><li>cartesian(otherDataset) 笛卡尔积，但在数据集T和U上调用时，返回一个(T,U)对的数据集，所有元素交互进行笛卡尔积 </li><li>coalesce(numPartitions) 对RDD中的分区减少指定的数目，通常在过滤完一个大的数据集之后进行此操作 </li><li>repartition(numpartitions) 将RDD中所有records平均划分到numparitions个partition中</li></ul><h3 id="action算子操作"><a href="#action算子操作" class="headerlink" title="action算子操作"></a>action算子操作</h3><ul><li>reduce(func) 通过函数func聚集数据集中的所有元素，这个函数必须是关联性的，确保可以被正确的并发执行 </li><li>collect() 在driver的程序中，以数组的形式，返回数据集的所有元素，这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用 </li><li>count() 返回数据集的元素个数 </li><li>first() 返回数据集的第一个元素(类似于take(1)) </li><li>take(n)  返回一个数组，由数据集的前n个元素组成。注意此操作目前并非并行执行的，而是driver程序所在机器 </li><li>takeSample(withReplacement,num,seed) 返回一个数组，在数据集中随机采样num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定的随机数生成器种子 </li><li>saveAsTextFile(path) 将数据集的元素，以textfile的形式保存到本地文件系统hdfs或者任何其他hadoop支持的文件系统，spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本 </li><li>takeOrderd(n,[ordering]) 排序后的limit(n) </li><li>saveAsSequenceFile(path) 将数据集的元素，以sequencefile的格式保存到指定的目录下，本地系统，hdfs或者任何其他hadoop支持的文件系统，RDD的元素必须由key-value对组成。并都实现了hadoop的writable接口或隐式可以转换为writable </li><li>saveAsObjectFile(path) 使用java的序列化方法保存到本地文件，可以被sparkContext.objectFile()加载 </li><li>countByKey()  对(K,V)类型的RDD有效，返回一个(K,Int)对的map，表示每一个可以对应的元素个数 </li><li>foreache(func) 在数据集的每一个元素上，运行函数func,t通常用于更新一个累加器变量，或者和外部存储系统做交互</li></ul><h2 id="Spark常用存储格式parquet-详解"><a href="#Spark常用存储格式parquet-详解" class="headerlink" title="Spark常用存储格式parquet 详解"></a>Spark常用存储格式parquet 详解</h2><ul><li><a href="https://juejin.im/entry/589932fab123db16a3ace2d1" target="_blank" rel="noopener">新型列式存储格式 Parquet 详解 - 后端 - 掘金</a></li><li><p>三个组成部分</p><ul><li>存储格式(storage format)</li><li>对象模型转换器(object model converters)</li><li>对象模型(object models) ：简单理解为数据在内存中的表示<br><img src="http://jacobs.wanhb.cn/images/parquet.png" alt="parquet"></li></ul></li><li><p>列式存储</p><ul><li>把某一列数据连续存储，每一行数据离散存储技术</li><li>带来的优化<ul><li>查询的时候不需要扫描全部的数据，而只需要读取每次查询涉及的列，这样可以将I/O消耗降低N倍，另外可以保存每一列的统计信息(min、max、sum等)，实现部分的谓词下推</li><li>由于每一列的成员都是同构的，可以针对不同的数据类型使用更高效的数据压缩算法，进一步减小I/O</li><li>由于每一列的成员的同构性，可以使用更加适合CPU pipeline的编码方式，减小CPU的缓存失效</li></ul></li></ul></li><li><p>数据模型</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">message AddressBook &#123;</span><br><span class="line">required string owner;</span><br><span class="line">repeated string ownerPhoneNumbers;</span><br><span class="line">repeated group contacts &#123;</span><br><span class="line">  required string name;</span><br><span class="line">  optional string phoneNumber;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>根被叫做message，有多个field</li><li>每个field包含三个属性:repetition, type, name<ul><li>repetition可以是required（出现1次）, optional（出现0次或1次），repeated（出现0次或者多次）。type可以是一个group或者一个primitive类型</li></ul></li><li>parquet数据类型不需要复杂的Map, List, Set等，而是使用repeated fields 和 groups来表示。例如List和Set可以被表示成一个repeated field,Map可以表示成一个包含有Key-value对的repeated group， 而且key是required的</li></ul></li><li><p>两个概念</p><ul><li>repetition level ：指明该值在路径中哪个repeated field重复<ul><li>针对的是repeted field的 。</li><li>它能用一个数字告诉我们在路径中的什么重复字段，此值重复了，以此来确定此值的位置</li><li>我们用深度0表示一个纪录的开头（虚拟的根节点），深度的计算忽略非重复字段（标签不是repeated的字段都不算在深度里）</li></ul></li><li>definition level：指明该列的路径上多少个可选field被定义了<ul><li>如果一个field是定义的，那么它的所有的父节点都是被定义的</li><li>从根节点开始遍历，当某一个field的路径上的节点开始是空的时候我们记录下当前的深度作为这个field的Definition Level</li><li>如果一个field的definition Level等于这个field的最大definition Level就说明这个field是有数据的</li><li><strong>注意</strong>：是指该路径上有定义的repeated field 和 optional field的个数，不包括required field，因为required field是必须有定义的</li></ul></li></ul></li><li><p>谓词下推：通过将一些过滤条件尽可能的在最底层执行可以减少每一层交互的数据量，从而提升性能</p><ul><li>例如”select count(1) from A Join B on A.id = B.id where A.a &gt; 10 and B.b &lt; 100″SQL查询中<ul><li>在处理Join操作之前需要首先对A和B执行TableScan操作，然后再进行Join，再执行过滤，最后计算聚合函数返回</li><li>但是如果把过滤条件A.a &gt; 10和B.b &lt; 100分别移到A表的TableScan和B表的TableScan的时候执行，可以大大降低Join操作的输入数据</li></ul></li><li>无论是行式存储还是列式存储，都可以在将过滤条件在读取一条记录之后执行以判断该记录是否需要返回给调用者，在Parquet做了更进一步的优化<ul><li>优化的方法时对每一个Row Group的每一个Column Chunk在存储的时候都计算对应的统计信息，包括该Column Chunk的最大值、最小值和空值个数。</li><li>通过这些统计值和该列的过滤条件可以判断该Row Group是否需要扫描。</li><li>另外Parquet未来还会增加诸如Bloom Filter和Index等优化数据，更加有效的完成谓词下推</li></ul></li></ul></li><li><p>映射下推：它意味着在获取表中原始数据时只需要扫描查询中需要的列</p><ul><li>在Parquet中原生就支持映射下推，执行查询的时候可以通过Configuration传递需要读取的列的信息</li><li>这些列必须是Schema的子集，映射每次会扫描一个Row Group的数据，然后一次性得将该Row Group里所有需要的列的Cloumn Chunk都读取到内存中，每次读取一个Row Group的数据能够大大降低随机读的次数，除此之外，Parquet在读取的时候会考虑列是否连续，如果某些需要的列是存储位置是连续的，那么一次读操作就可以把多个列的数据读取到内存</li></ul></li></ul><h2 id="Spark-Stremaing-相关"><a href="#Spark-Stremaing-相关" class="headerlink" title="Spark Stremaing 相关"></a>Spark Stremaing 相关</h2><h3 id="BlockRDD"><a href="#BlockRDD" class="headerlink" title="BlockRDD"></a>BlockRDD</h3><ul><li>由spark.streaming.blockInterval和duration决定有多少个BlockRdd</li><li><p>Receiver模式</p><ul><li>一个BatchDuration有几个block就会产生几个partition，可参考<a href="http://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html#approach-1-receiver-based-approach" target="_blank" rel="noopener">receiver bases approach</a></li><li>并行度由手动创建的receiver决定<br><img src="http://jacobs.wanhb.cn/images/receiver%E6%A8%A1%E5%BC%8F.png" alt="receiver模式"></li></ul></li><li><p>direct模式</p><ul><li>blockRDD不再对实际的分区数量起作用，而是会创建和kafka partitions 相同数量的RDD partitions，可参考<a href="http://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html#approach-2-direct-approach-no-receivers" target="_blank" rel="noopener">direct approach</a></li><li>在实际运行的时候通过下发到executor上的task，边拉取数据边处理，这样即使每个task执行失败，对应分区下面的offset也没有提交，也能通过重启task恢复<br>  <img src="http://jacobs.wanhb.cn/images/direct%E6%A8%A1%E5%BC%8F.png" alt="direct模式"></li></ul></li><li><p>消息消费速率限定</p><ul><li>开启背压模式：spark.streaming.backpressure.enabled=true<ul><li>此模式如果消息堆积严重，会一次性拉取kafka中所有堆积的消息进行处理。很可能会导致程序崩溃</li></ul></li><li><p>设置每个partition消费速率, spark.streaming.kafka.maxRatePerPartition</p><ul><li><p>对应每个batch拉取到的消息为: </p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">maxRatePerPartition*partitionNum*batch_interval</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h2 id="Spark-优化相关"><a href="#Spark-优化相关" class="headerlink" title="Spark 优化相关"></a>Spark 优化相关</h2><h3 id="优化建议"><a href="#优化建议" class="headerlink" title="优化建议"></a>优化建议</h3><ul><li>stage 的数量跟一个job中是否要进行shuffle有关，像reduceByKey，groupbyKey等等</li><li>尽量用broadcast和filter规避join操作</li><li><p>因为每次job partition数量过多，导致hive表中过多小文件产生，所以需要重新指定分区，有以下俩种方法：repartition(numPartitions:Int):RDD[T]和coalesce(numPartitions:Int，shuffle:Boolean=false):RDD[T]<br>他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区）</p><ul><li>N&lt;M。一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。</li><li>如果N&gt;M并且N和M相差不多，(假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false，在shuffl为false的情况下，如果M&gt;N时，coalesce为无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系。</li><li><p>如果N&gt;M并且两者相差悬殊，这时如果将shuffle设置为false，父子ＲＤＤ是窄依赖关系，他们同处在一个stage中，就可能造成Spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以讲shuffle设置为true。</p></li><li><p><strong>总之</strong>：如果shuffle为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDD的分区数变多的。</p></li></ul></li></ul><h3 id="参数调优"><a href="#参数调优" class="headerlink" title="参数调优"></a>参数调优</h3><ul><li><a href="http://tech.meituan.com/spark-tuning-basic.html" target="_blank" rel="noopener">美团Spark参数调优参考文章</a></li><li>最重要的是数据序列化和内存调优。对于大多数程序选择Kyro序列化器并持久化序列后的数据能解决常见的性能问题。</li><li><p>Executor</p><ul><li>每个节点可以起一个或多个Executor。每个Executor上的一个核只能同时执行一个task,如果一个Executor被分到了多个task只能排队依次执行</li><li>Executor内存主要分三块：<pre><code>- 1、让task执行我们自己编写的代码，默认占总内存的20%。- 2、让task通过shuffle过程拉取了上一个stage的task输出后，进行聚合等操作时，默认占用总内存20%；- 3、让RDD持久化使用，默认60%</code></pre></li><li>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。<pre><code>- 一个CPU core同一时间只能执行一个线程。- 每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</code></pre></li></ul></li><li><p>广播大变量</p><ul><li>当需要用到外部变量时，默认每个task都会存一份，这样会增加GC次数，使用广播变量能确保一个Executor中只有一份</li></ul></li><li><p>使用Kryo优化序列化性能(如果希望RDD序列化存储在内存中，面临GC问题的时候，优先使用序列化缓存技术)</p><ul><li>spark没有默认使用Kryo作为序列化类库，是因为Kryo要求注册所有需要序列化的自定义类型，这对开发比较麻烦</li></ul></li></ul><h2 id="Spark-内存管理"><a href="#Spark-内存管理" class="headerlink" title="Spark 内存管理"></a>Spark 内存管理</h2><ul><li><a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html" target="_blank" rel="noopener">参考文章：apache spark内存管理详解</a></li></ul><h3 id="堆内内存和堆外内存"><a href="#堆内内存和堆外内存" class="headerlink" title="堆内内存和堆外内存"></a>堆内内存和堆外内存</h3><ul><li>堆内：Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存<ul><li>缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存</li><li>执行 Shuffle 时占用的内存被规划为执行（Execution）内存<br>- 剩余部分： Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例</li><li>注意：在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常</li></ul></li><li>堆外：进一步优化内存的使用以及提高 Shuffle 时排序的效率， 可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据<ul><li>堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差</li></ul></li></ul><h3 id="存储管理"><a href="#存储管理" class="headerlink" title="存储管理"></a>存储管理</h3><ul><li><p>RDD 的持久化机制</p><ul><li>RDD 的持久化由 Spark 的 Storage 模块 [7] 负责，实现了 RDD 与物理存储的解耦合</li><li>Storage 模块负责管理 Spark 在计算过程中产生的数据，将那些在内存或磁盘、在本地或远程存取数据的功能封装了起来</li><li>在具体实现时 Driver 端和 Executor 端的 Storage 模块构成了主从式的架构，即 Driver 端的 BlockManager 为 Master，Executor 端的 BlockManager 为 Slave。</li><li>Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID ）。</li><li>Master 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Slave 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令，例如新增或删除一个 RDD</li></ul></li><li><p>RDD 缓存的过程</p><ul><li>RDD 在缓存到存储内存之后，Partition 被转换成 Block, 其中Record在堆内占有一块连续的空间</li><li>将Partition由不连续的存储空间转换为连续存储空间的过程，Spark称之为”展开”（Unroll）</li><li>Block 有序列化和非序列化两种存储格式</li><li>用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成</li></ul></li><li><p>淘汰规则</p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存</li><li>新旧 Block 不能属于同一个 RDD，避免循环淘汰</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题</li><li>遍历 LinkedHashMap 中 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。其中 LRU 是 LinkedHashMap 的特性。</li></ul></li><li><p>Spark中执行内存管理</p><ul><li>shuffle write:<ul><li>若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</li><li>若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。</li></ul></li><li>shuffle read:<ul><li>在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter 处理，占用堆内执行空间。</li></ul></li><li>Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制</li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 学习 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>airflow实战总结</title>
      <link href="/2018/08/30/airflow%E5%AE%9E%E6%88%98%E6%80%BB%E7%BB%93/"/>
      <url>/2018/08/30/airflow%E5%AE%9E%E6%88%98%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>airflow是一款开源的，分布式任务调度框架，它将一个具有上下级依赖关系的工作流，组装成一个有向无环图。</p><ul><li>特点:<ul><li>分布式任务调度：允许一个工作流的task在多台worker上同时执行</li><li>可构建任务依赖：以有向无环图的方式构建任务依赖关系</li><li>task原子性：工作流上每个task都是原子可重试的，一个工作流某个环节的task失败可自动或手动进行重试，不必从头开始任务</li></ul></li><li><p>工作流示意图</p><p>  <img src="https://pic4.zhimg.com/v2-fbd8d77be2eda3c9766c300359e8eba3_1200x500.jpg" alt="airflow-dags"></p><ul><li>一个dag表示一个定时的工作流，包含一个或者多个具有依赖关系的task</li></ul></li><li><p>task依赖图</p><p>  <img src="https://pic1.zhimg.com/80/v2-57deb1228a73c290c666539bc56ee8ac_hd.jpg" alt="airflow-tasks"></p></li><li><p>架构图及集群角色</p><p>  <img src="https://pic1.zhimg.com/80/v2-35a160b63e7389fe12f451e299ab0c00_hd.jpg" alt="airflow-infra"></p><ul><li>webserver : 提供web端服务，以及会定时生成子进程去扫描对应的目录下的dags，并更新数据库</li><li>scheduler : 任务调度服务，根据dags生成任务，并提交到消息中间件队列中 (redis或rabbitMq)</li><li>celery worker : 分布在不同的机器上，作为任务真正的的执行节点。通过监听消息中间件: redis或rabbitMq 领取任务</li><li>flower : 监控worker进程的存活性，启动或关闭worker进程，查看运行的task</li></ul></li></ul><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><ul><li><p>构建docker镜像</p><ul><li><p>采用的airflow是未发行的1.10.0版本，原因是从1.10.0开始，支持时区的设置，而不是统一的UTC</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">//self.registry.domain 为docker私有镜像仓库</span><br><span class="line">//self.mvn.registry.com maven 私有镜像仓库</span><br><span class="line">//data0 为数据目录，data1为日志目录，运维统一配置日志清楚策略</span><br><span class="line">#docker build --network host -t self.registry.domain/airflow_base_1.10.7:1.0.0 .</span><br><span class="line">FROM self.registry.domain/airflow/centos_base_7.4.1708:1.0.0</span><br><span class="line">LABEL AIRFLOW=1.10.7</span><br><span class="line"></span><br><span class="line">ARG CELERY_REDIS=4.1.1</span><br><span class="line">ARG DOCKER_VERSION=1.13.1</span><br><span class="line">ARG AIRFLOW_VERSION=1.10.7</span><br><span class="line"></span><br><span class="line">ADD sbin /data0/airflow/sbin</span><br><span class="line"></span><br><span class="line">ENV SLUGIFY_USES_TEXT_UNIDECODE=yes \</span><br><span class="line">    #如果构建镜像的机器需要代理才能连接外网的话，配置https_proxy</span><br><span class="line">    https_proxy=https://ip:port </span><br><span class="line"></span><br><span class="line">RUN curl http://self.mvn.registry.com/python/python-3.5.6.jar -o /tmp/Python-3.5.6.tgz &amp;&amp; \</span><br><span class="line">    curl http://self.mvn.registry.com/airflow/$&#123;AIRFLOW_VERSION&#125;/airflow-$&#123;AIRFLOW_VERSION&#125;.jar -o /tmp/incubator-airflow-$&#123;AIRFLOW_VERSION&#125;.tar.gz &amp;&amp; \</span><br><span class="line">    curl http:/self.mvn.registry.com/docker/$&#123;DOCKER_VERSION&#125;/docker-$&#123;DOCKER_VERSION&#125;.jar -o /tmp/docker-$&#123;DOCKER_VERSION&#125;.tar.gz &amp;&amp; \</span><br><span class="line">    tar zxf /tmp/docker-$&#123;DOCKER_VERSION&#125;.tar.gz -C /data0/software &amp;&amp; \</span><br><span class="line">    tar zxf /tmp/Python-3.5.6.tgz -C /data0/software &amp;&amp; \</span><br><span class="line">    tar zxf /tmp/incubator-airflow-$&#123;AIRFLOW_VERSION&#125;.tar.gz -C /data0/software &amp;&amp; \</span><br><span class="line">    yum install -y libtool-ltdl policycoreutils-python &amp;&amp; \</span><br><span class="line">    rpm -ivh --force --nodeps /data0/software/docker-$&#123;DOCKER_VERSION&#125;/docker-engine-selinux-$&#123;DOCKER_VERSION&#125;-1.el7.centos.noarch.rpm &amp;&amp; \</span><br><span class="line">    rpm -ivh --force --nodeps /data0/software/docker-$&#123;DOCKER_VERSION&#125;/docker-engine-$&#123;DOCKER_VERSION&#125;-1.el7.centos.x86_64.rpm &amp;&amp; \</span><br><span class="line">    yum -y install gcc &amp;&amp; yum -y install gcc-c++ &amp;&amp; yum -y install make &amp;&amp; \</span><br><span class="line">    yum -y install zlib-devel mysql-devel python-devel cyrus-sasl-devel cyrus-sasl-lib libxml2-devel libxslt-devel &amp;&amp; \</span><br><span class="line">    cd /data0/software/Python-3.5.6 &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make install &amp;&amp; \</span><br><span class="line">    ln -sf /usr/local/bin/pip3 /usr/local/bin/pip &amp;&amp; \</span><br><span class="line">    ln -sf /usr/local/bin/python3 /usr/local/bin/python &amp;&amp; \</span><br><span class="line">    cd /data0/software/incubator-airflow-$&#123;AIRFLOW_VERSION&#125; &amp;&amp; python setup.py install &amp;&amp; \</span><br><span class="line">    pip install -i https://pypi.douban.com/simple/ apache-airflow[crypto,celery,hive,jdbc,mysql,hdfs,password,redis,devel_hadoop] &amp;&amp; \</span><br><span class="line">    pip install -i https://pypi.douban.com/simple/ celery[redis]==$CELERY_REDIS &amp;&amp; \</span><br><span class="line">    pip install -i https://pypi.douban.com/simple/ docutils &amp;&amp; \</span><br><span class="line">    ln -sf /usr/local/lib/python3.5/site-packages/apache_airflow-1.10.0-py3.5.egg/airflow /data0/software/airflow &amp;&amp; \</span><br><span class="line">    mkdir -p /data0/airflow/bin &amp;&amp; \</span><br><span class="line">    ln -sf /data0/airflow/sbin/airflow-200.sh /data0/airflow/bin/200.sh &amp;&amp; \</span><br><span class="line">    ln -sf /data0/airflow/sbin/airflow-503.sh /data0/airflow/bin/503.sh &amp;&amp; \</span><br><span class="line">    chown -R root:root /data0/software/ &amp;&amp; \</span><br><span class="line">    chown -R root:root /data0/airflow/ &amp;&amp; \</span><br><span class="line">    chmod -R 775 /data0/airflow/sbin/* &amp;&amp; \</span><br><span class="line">    chmod -R 775 /data0/airflow/bin/* &amp;&amp; \</span><br><span class="line">    echo &apos;source /data0/airflow/sbin/init-airflow.sh&apos; &gt;&gt; ~/.bashrc &amp;&amp; \</span><br><span class="line">    rm -rf /tmp/* /data0/software/Python-3.5.6 /data0/software/incubator-airflow-$&#123;AIRFLOW_VERSION&#125; /data0/software/docker-$&#123;DOCKER_VERSION&#125;</span><br><span class="line">    </span><br><span class="line">ENV PATH=$PATH:/data0/software/jdk/bin:/data0/software/airflow/bin:/data0/airflow/sbin/:/data0/airflow/sbin/airflow/:/data0/airflow/bin/</span><br><span class="line"></span><br><span class="line">WORKDIR /data0/airflow/bin/</span><br></pre></td></tr></table></figure></li><li><p>通过docker 启动容器的话需要暴露几个端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">webserver: 8081</span><br><span class="line">worker: 8793</span><br><span class="line">flower: 5555</span><br><span class="line">//启动示例</span><br><span class="line">docker run --name airflow -it -d --privileged --net=host -p 8081:8081 -p 5555:5555 -p 8793:8793 -v /var/run/docker.sock:/var/run/docker.sock -v /data1:/data1 -v /data0/airflow:/data0/airflow self.registry.domain/airflow_1.10.7:1.0.0</span><br></pre></td></tr></table></figure></li></ul></li><li><p>airflow 升级到未release的1.10.0的版本</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//如果之前用的是低版本的话，需要执行</span><br><span class="line">airflow upgradedb 来更新迁移数据库的schema</span><br><span class="line">//执行之前首先需要set mysql property</span><br><span class="line">set global explicit_defaults_for_timestamp=1 //会提示is readonly variable</span><br><span class="line">需要在my.cnf中添加这个设置:explicit_defaults_for_timestamp=1 并重启mysql</span><br><span class="line">//update celery几个设置</span><br><span class="line">celeryd_concurrency -&gt; worker_concurrency</span><br><span class="line">celery_result_backend -&gt; result_backend</span><br></pre></td></tr></table></figure><ul><li><p>修改时区，以及界面上执行时间的显示(airlfow 默认界面上还是按照UTC显示)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//需要update configuration</span><br><span class="line">default_timezone = Etc/GMT-8</span><br><span class="line"></span><br><span class="line">//修改dags.html中的显示时间，使得界面上看起来方便</span><br><span class="line">// jinjia2 传入转换函数，在views.py 的homeview的render中 </span><br><span class="line">//（方法验证有点问题，再优化）</span><br><span class="line">def utc2local(utc):</span><br><span class="line">    epoch = time.mktime(utc.timetuple())</span><br><span class="line">    offset = datetime.fromtimestamp(epoch) - datetime.utcfromtimestamp(epoch)</span><br><span class="line">    return utc + offset</span><br><span class="line">utc2local(last_run.execution_date).strftime(&quot;%Y-%m-%d %H:%M&quot;)</span><br><span class="line">utc2local(last_run.start_date).strftime(&quot;%Y-%m-%d %H:%M&quot;)`</span><br></pre></td></tr></table></figure></li></ul></li><li><p>airflow plugins 定制化开发</p><ul><li><a href="https://airflow.apache.org/plugins.html" target="_blank" rel="noopener">官方文档</a></li><li>plugin 这个没法传给worker，还是得重新分发到各个worker节点，建议打入airflow基础镜像中</li><li>增加operator时需要重启webserver和scheduler</li></ul></li><li><p>由于dag的删除现在官方没有暴露直接的api,而完整的删除又牵扯到多个表，总结出删除dag的sql如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set @dag_id = &apos;BAD_DAG&apos;;</span><br><span class="line">delete from airflow.xcom where dag_id = @dag_id;</span><br><span class="line">delete from airflow.task_instance where dag_id = @dag_id;</span><br><span class="line">delete from airflow.sla_miss where dag_id = @dag_id;</span><br><span class="line">delete from airflow.log where dag_id = @dag_id;</span><br><span class="line">delete from airflow.job where dag_id = @dag_id;</span><br><span class="line">delete from airflow.dag_run where dag_id = @dag_id;</span><br><span class="line">delete from airflow.dag where dag_id = @dag_id;</span><br></pre></td></tr></table></figure></li><li><p>自己实现的200和503脚本，用于集群统一的上下线操作</p><ul><li><p>200脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">function usage() &#123;</span><br><span class="line">    echo -e &quot;\n A tool used for starting airflow services</span><br><span class="line">Usage: 200.sh &#123;webserver|worker|scheduler|flower&#125;</span><br><span class="line">&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PORT=8081</span><br><span class="line">ROLE=webserver</span><br><span class="line">ENV_ARGS=&quot;&quot;</span><br><span class="line">check_alive() &#123;</span><br><span class="line">    PID=`netstat -nlpt | grep $PORT | awk &apos;&#123;print $7&#125;&apos; | awk -F &quot;/&quot; &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">    [ -n &quot;$PID&quot; ] &amp;&amp; return 0 || return 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">check_scheduler_alive() &#123;</span><br><span class="line">    PIDS=`ps -ef | grep &quot;/usr/local/bin/airflow scheduler&quot; | grep &quot;python&quot; | awk &apos;&#123;print $2&#125;&apos;`</span><br><span class="line">    [ -n &quot;$PIDS&quot; ] &amp;&amp; return 0 || return 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function get_host_ip()&#123;</span><br><span class="line">    local host=$(ifconfig | grep &quot;inet &quot; | grep &quot;\-\-&gt;&quot; | awk &apos;&#123;print $2&#125;&apos; | tail -1)</span><br><span class="line">    if [[ -z &quot;$host&quot; ]]; then</span><br><span class="line">        host=$(ifconfig | grep &quot;inet &quot; | grep &quot;broadcast&quot; | awk &apos;&#123;print $2&#125;&apos; | tail -1)</span><br><span class="line">    fi</span><br><span class="line">    echo &quot;$&#123;host&#125;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">start_service() &#123;</span><br><span class="line">    if [ $ROLE = &apos;scheduler&apos; ];then</span><br><span class="line">        check_scheduler_alive</span><br><span class="line">    else</span><br><span class="line">        check_alive</span><br><span class="line">    fi</span><br><span class="line">    if [ $? -ne 0 ];then</span><br><span class="line">        nohup airflow $ROLE $ENV_ARGS &gt; $BASE_LOG_DIR/$ROLE/$ROLE.log 2&gt;&amp;1 &amp;</span><br><span class="line">        sleep 5</span><br><span class="line">        if [ $ROLE = &apos;scheduler&apos; ];then</span><br><span class="line">            check_scheduler_alive</span><br><span class="line">        else</span><br><span class="line">            check_alive</span><br><span class="line">        fi</span><br><span class="line">        if [ $? -ne 0 ];then</span><br><span class="line">            echo &quot;service start error&quot;</span><br><span class="line">            exit 1</span><br><span class="line">        else</span><br><span class="line">            echo &quot;service start success&quot;</span><br><span class="line">            exit 0</span><br><span class="line">        fi</span><br><span class="line">    else</span><br><span class="line">        echo &quot;service alreay started&quot;</span><br><span class="line">        exit 0</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">    if [ -z &quot;$&#123;POOL&#125;&quot; ]; then</span><br><span class="line">        echo &quot;the environment variable POOL cannot be empty&quot;</span><br><span class="line">        exit 1</span><br><span class="line">    fi</span><br><span class="line">    source /data0/hcp/sbin/init-hcp.sh</span><br><span class="line">    case &quot;$1&quot; in</span><br><span class="line">        webserver)</span><br><span class="line">            echo &quot;starting airflow webserver&quot;</span><br><span class="line">            ROLE=webserver</span><br><span class="line">            PORT=8081</span><br><span class="line">            start_service</span><br><span class="line">            ;;</span><br><span class="line">        worker)</span><br><span class="line">            echo &quot;starting airflow worker&quot;</span><br><span class="line">            ROLE=worker</span><br><span class="line">            PORT=8793</span><br><span class="line">            local host_ip=$(get_host_ip)</span><br><span class="line">            ENV_ARGS=&quot;-cn $&#123;host_ip&#125;@$&#123;host_ip&#125;&quot;</span><br><span class="line">            start_service</span><br><span class="line">            ;;</span><br><span class="line">        flower)</span><br><span class="line">            echo &quot;starting airflow flower&quot;</span><br><span class="line">            ROLE=flower</span><br><span class="line">            PORT=5555</span><br><span class="line">            start_service</span><br><span class="line">            ;;</span><br><span class="line">        scheduler)</span><br><span class="line">            echo &quot;starting airflow scheduler&quot;</span><br><span class="line">            ROLE=scheduler</span><br><span class="line">            start_service</span><br><span class="line">            ;;     </span><br><span class="line">        *)</span><br><span class="line">            usage</span><br><span class="line">            exit 1</span><br><span class="line">    esac</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br></pre></td></tr></table></figure></li><li><p>503脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">function usage() &#123;</span><br><span class="line">    echo -e &quot;\n A tool used for stop airflow services</span><br><span class="line">Usage: 200.sh &#123;webserver|worker|scheduler|flower&#125;</span><br><span class="line">&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function get_host_ip()&#123;</span><br><span class="line">    local host=$(ifconfig | grep &quot;inet &quot; | grep &quot;\-\-&gt;&quot; | awk &apos;&#123;print $2&#125;&apos; | tail -1)</span><br><span class="line">    if [[ -z &quot;$host&quot; ]]; then</span><br><span class="line">        host=$(ifconfig | grep &quot;inet &quot; | grep &quot;broadcast&quot; | awk &apos;&#123;print $2&#125;&apos; | tail -1)</span><br><span class="line">    fi</span><br><span class="line">    echo &quot;$&#123;host&#125;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">    if [ -z &quot;$&#123;POOL&#125;&quot; ]; then</span><br><span class="line">        echo &quot;the environment variable POOL cannot be empty&quot;</span><br><span class="line">        exit 1</span><br><span class="line">    fi</span><br><span class="line">    source /data0/hcp/sbin/init-hcp.sh</span><br><span class="line">    case &quot;$1&quot; in</span><br><span class="line">        webserver)</span><br><span class="line">            echo &quot;stopping airflow webserver&quot;</span><br><span class="line">            cat $AIRFLOW_HOME/airflow-webserver.pid | xargs kill -9</span><br><span class="line">            ;;</span><br><span class="line">        worker)</span><br><span class="line">            echo &quot;stopping airflow worker&quot;</span><br><span class="line">            PORT=8793</span><br><span class="line">            PID=`netstat -nlpt | grep $PORT | awk &apos;&#123;print $7&#125;&apos; | awk -F &quot;/&quot; &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">            kill -9 $PID</span><br><span class="line">            local host_ip=$(get_host_ip)</span><br><span class="line">            ps -ef | grep celeryd | grep $&#123;host_ip&#125;@$&#123;host_ip&#125; | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9</span><br><span class="line">            ;;</span><br><span class="line">        flower)</span><br><span class="line">            echo &quot;stopping airflow flower&quot;</span><br><span class="line">            PORT=5555</span><br><span class="line">            PID=`netstat -nlpt | grep $PORT | awk &apos;&#123;print $7&#125;&apos; | awk -F &quot;/&quot; &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">            kill -9 $PID</span><br><span class="line">            start_service</span><br><span class="line">            ;;</span><br><span class="line">        scheduler)</span><br><span class="line">            echo &quot;stopping airflow scheduler&quot;</span><br><span class="line">            PID=`ps -ef | grep &quot;/usr/local/bin/airflow scheduler&quot; | grep &quot;python&quot; | awk &apos;&#123;print $2&#125;&apos;`</span><br><span class="line">            kill -9 $PID</span><br><span class="line">            ;;     </span><br><span class="line">        *)</span><br><span class="line">            usage</span><br><span class="line">            exit 1</span><br><span class="line">    esac</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="遇到的坑以及定制化解决方案"><a href="#遇到的坑以及定制化解决方案" class="headerlink" title="遇到的坑以及定制化解决方案"></a>遇到的坑以及定制化解决方案</h2><ul><li><p>问题1: airflow worker 角色不能使用根用户启动</p><ul><li><p>原因：不能用根用户启动的根本原因，在于airflow的worker直接用的celery，而celery 源码中有参数默认不能使用ROOT启动，否则将报错, <a href="http://docs.celeryproject.org/en/latest/_modules/celery/platforms.html" target="_blank" rel="noopener">源码链接</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">C_FORCE_ROOT = os.environ.get(&apos;C_FORCE_ROOT&apos;, False)</span><br><span class="line"></span><br><span class="line">ROOT_DISALLOWED = &quot;&quot;&quot;\</span><br><span class="line">Running a worker with superuser privileges when the</span><br><span class="line">worker accepts messages serialized with pickle is a very bad idea!</span><br><span class="line"></span><br><span class="line">If you really want to continue then you have to set the C_FORCE_ROOT</span><br><span class="line">environment variable (but please think about this before you do).</span><br><span class="line"></span><br><span class="line">User information: uid=&#123;uid&#125; euid=&#123;euid&#125; gid=&#123;gid&#125; egid=&#123;egid&#125;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">ROOT_DISCOURAGED = &quot;&quot;&quot;\</span><br><span class="line">You&apos;re running the worker with superuser privileges: this is</span><br><span class="line">absolutely not recommended!</span><br><span class="line"></span><br><span class="line">Please specify a different user using the --uid option.</span><br><span class="line"></span><br><span class="line">User information: uid=&#123;uid&#125; euid=&#123;euid&#125; gid=&#123;gid&#125; egid=&#123;egid&#125;</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure></li><li><p>解决方案一：修改airlfow源码，在celery_executor.py中强制设置C_FORCE_ROOT</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from celery import Celery, platforms </span><br><span class="line">在app = Celery(…)后新增 </span><br><span class="line">platforms.C_FORCE_ROOT = True</span><br><span class="line">重启即可</span><br></pre></td></tr></table></figure></li><li><p>解决方案二：在容器初始化环境变量的时候，设置C_FORCE_ROOT参数，以零侵入的方式解决问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#强制celery worker运行采用root模式</span><br><span class="line">export C_FORCE_ROOT=True</span><br></pre></td></tr></table></figure></li></ul></li><li><p>问题2: docker in docker</p><ul><li>在dags中以docker方式调度任务时，为了container的轻量话，不做重型的docker pull等操作，我们利用了docker cs架构的设计理念，只需要将宿主机的/var/run/docker.sock文件挂载到容器目录下即可 <a href="http://wangbaiyuan.cn/docker-in-docker.html#prettyPhoto" target="_blank" rel="noopener">docker in docker 资料</a></li></ul></li><li><p>问题3: 由于我们运行airlfow的机器是高配机器切分的虚机，host并非是传统的ip段，多节点执行后无法在master节点上通过worker节点提供的日志服务获取执行日志</p><ul><li><p>查看celery源码(celery/celery/worker/worker.py)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from celery.utils.nodenames import default_nodename, worker_direct</span><br><span class="line">self.hostname = default_nodename(hostname)</span><br><span class="line">// 查看default_nodename方法</span><br><span class="line">def default_nodename(hostname):</span><br><span class="line">    &quot;&quot;&quot;Return the default nodename for this process.&quot;&quot;&quot;</span><br><span class="line">    name, host = nodesplit(hostname or &apos;&apos;)</span><br><span class="line">    return nodename(name or NODENAME_DEFAULT, host or gethostname())</span><br><span class="line"></span><br><span class="line">//默认在worker.py 的构造方法中没有传入hostname 所以在celery nodenames.py中default_nodename方法里面调用了gethostname</span><br><span class="line">//可以看到gethostname的实现，调用了socket.gethostname，这个直接得到了虚拟机的host</span><br><span class="line">gethostname = memoize(1, Cache=dict)(socket.gethostname)</span><br></pre></td></tr></table></figure></li><li><p>解决方案：发现airflow worker的启动命令中其实提供了设置celery host name的参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">airflow worker -cn=ip@ip</span><br></pre></td></tr></table></figure></li></ul></li><li><p>问题4: 多个worker节点进行调度反序列化dag执行的时候，报找不到module的错误</p><ul><li><p>当时考虑到文件更新的一致性，采用所有worker统一执行master下发的序列化dag的方案，而不依赖worker节点上实际的dag文件，开启这一特性操作如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">worker节点上： airflow worker -cn=ip@ip -p //-p为开关参数，意思是以master序列化的dag作为执行文件，而不是本地dag目录中的文件</span><br><span class="line">master节点上： airflow scheduler -p</span><br></pre></td></tr></table></figure></li><li><p>错误原因在于远程的worker节点上不存在实际的dag文件，反序列化的时候对于当时在dag中定义的函数或对象找不到module_name</p></li><li>解决方案一：在所有的worker节点上同时发布dags目录，缺点是dags一致性成问题</li><li><p>解决方案二：修改源码中序列化与反序列化的逻辑，主体思路还是替换掉不存在的module为main。修改如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">//models.py 文件，对 class DagPickle(Base) 定义修改</span><br><span class="line">import dill</span><br><span class="line">class DagPickle(Base):</span><br><span class="line">id = Column(Integer, primary_key=True)</span><br><span class="line"># 修改前: pickle = Column(PickleType(pickler=dill))</span><br><span class="line">pickle = Column(LargeBinary)</span><br><span class="line">created_dttm = Column(UtcDateTime, default=timezone.utcnow)</span><br><span class="line">pickle_hash = Column(Text)</span><br><span class="line"></span><br><span class="line">__tablename__ = &quot;dag_pickle&quot;</span><br><span class="line">def __init__(self, dag):</span><br><span class="line">    self.dag_id = dag.dag_id</span><br><span class="line">    if hasattr(dag, &apos;template_env&apos;):</span><br><span class="line">        dag.template_env = None</span><br><span class="line">    self.pickle_hash = hash(dag)</span><br><span class="line">    raw = dill.dumps(dag)</span><br><span class="line">    # 修改前: self.pickle = dag</span><br><span class="line">    reg_str = &apos;unusual_prefix_\w*&#123;0&#125;&apos;.format(dag.dag_id)</span><br><span class="line">    result = re.sub(str.encode(reg_str), b&apos;__main__&apos;, raw)</span><br><span class="line">    self.pickle =result</span><br><span class="line"></span><br><span class="line">//cli.py 文件反序列化逻辑 run(args, dag=None) 函数</span><br><span class="line">// 直接通过dill来反序列化二进制文件，而不是通过PickleType 的result_processor做中转</span><br><span class="line">修改前: dag = dag_pickle.pickle</span><br><span class="line">修改后：dag = dill.loads(dag_pickle.pickle)</span><br></pre></td></tr></table></figure></li><li><p>解决方案三：源码零侵入，使用python的types.FunctionType重新创建一个不带module的function，这样序列化与反序列化的时候不会有问题（待验证)</p><ul><li>注意，使用types.FunctionType的方式装饰函数时，由于所有的引用都会从golbals里面找，所以对于module的导入，建议在被装饰的函数里面变成local的方式引入</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//这里把globals()传入是为了把builtlins等一些模块传入，省事</span><br><span class="line">new_func = types.FunctionType((lambda df: df.iloc[:, 0].size == xx).__code__, globals())</span><br></pre></td></tr></table></figure></li></ul></li><li><p>问题5：由于airflow在master查看task执行日志是通过各个节点的http服务获取的，但是存入task_instance表中的host_name不是ip，可见获取hostname的方式有问题.</p><ul><li><p>解决方案：修改airflow/utils/net.py 中get_hostname函数，添加优先获取环境变量中设置的hostname的逻辑</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">//models.py TaskInstance</span><br><span class="line">self.hostname = get_hostname()</span><br><span class="line">//net.py 在get_hostname里面加入一个获取环境变量的逻辑</span><br><span class="line">import os</span><br><span class="line">def get_hostname():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Fetch the hostname using the callable from the config or using</span><br><span class="line">    `socket.getfqdn` as a fallback.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 尝试获取环境变量</span><br><span class="line">    if &apos;AIRFLOW_HOST_NAME&apos; in os.environ:</span><br><span class="line">        return os.environ[&apos;AIRFLOW_HOST_NAME&apos;]</span><br><span class="line">    # First we attempt to fetch the callable path from the config.</span><br><span class="line">    try:</span><br><span class="line">        callable_path = conf.get(&apos;core&apos;, &apos;hostname_callable&apos;)</span><br><span class="line">    except AirflowConfigException:</span><br><span class="line">        callable_path = None</span><br><span class="line"></span><br><span class="line">    # Then we handle the case when the config is missing or empty. This is the</span><br><span class="line">    # default behavior.</span><br><span class="line">    if not callable_path:</span><br><span class="line">        return socket.getfqdn()</span><br><span class="line"></span><br><span class="line">    # Since we have a callable path, we try to import and run it next.</span><br><span class="line">    module_path, attr_name = callable_path.split(&apos;:&apos;)</span><br><span class="line">    module = importlib.import_module(module_path)</span><br><span class="line">    callable = getattr(module, attr_name)</span><br><span class="line">    return callable()</span><br></pre></td></tr></table></figure></li></ul></li></ul>]]></content>
      
      
    </entry>
    
    <entry>
      <title>impala集群搭建</title>
      <link href="/2018/08/12/impala%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/08/12/impala%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>说起Impala，很多人都不会陌生。它区别于MapReduce 中间结果溢写，跨节点数据获取的低效，采用MPP 查询引擎，各查询节点并发执行查询语句，并将生成的查询结果汇总输出。<br>近期开始真正的使用impala，之前只是小玩过已经集成好的环境，并没有真正的从0到1的去构建Impala集群。基于我司所有的大数据组件都是采用容器的方式部署以便统一管理，我们需要先构建Impala镜像</p><h2 id="impala-相关介绍"><a href="#impala-相关介绍" class="headerlink" title="impala 相关介绍"></a>impala 相关介绍</h2><ul><li>impala是由cloudera公司主导开发的一款大数据实时查询的分析工具，区别于Hive底层传统的MapReduce批处理方式，采用MPP查询引擎架构，相比于Hive 能带来查询性能30-90倍的提升</li><li><p>特点</p><ul><li>查询速度快: 底层MPP查询引擎。基于内存计算，中间结果不写入磁盘，由coordinator汇总数据结果</li><li>灵活性高：可以兼容存储在HDFS上的原生数据，也可以兼容优化处理过的压缩数据，与Hive共用metastore兼容从Hive上导入的所有sql语句</li><li>可伸缩性：可以很好的和一些BI工具去配合使用，如Microstrategy、Tableau、Qlikview等。</li></ul></li><li><p>架构</p></li></ul><p><img src="http://jacobs.wanhb.cn/images/impala-architecture.png" alt="impala-architecture"></p><ul><li><p>集群角色</p><ul><li><p>impalad(query planner,coordinator, exec engine)：</p><ul><li>分布在datanode节点上，接受客户端的查询请求</li><li>接收查询请求的impalad 会作为本次查询的coordinator，生成查询计划树，并分发给具有相应数据的impalad执行</li><li>汇总各个impalad上执行的查询结果，返回给客户端</li></ul></li><li>statestore<ul><li>跟踪集群中impalad的健康状态及信息位置，并把健康状况同步到所有的impalad进程节点</li></ul></li><li>catalog<ul><li>将元数据的变化通知给集群的各个节点，减少refresh和invalidate metadata语句使用</li></ul></li></ul></li></ul><h2 id="镜像构建篇"><a href="#镜像构建篇" class="headerlink" title="镜像构建篇"></a>镜像构建篇</h2><h3 id="hive镜像构建"><a href="#hive镜像构建" class="headerlink" title="hive镜像构建"></a>hive镜像构建</h3><ul><li>因为Impala依赖hive metastore，所以在构建impala镜像之前，先要构建hive镜像</li><li><p>构建过程</p><ul><li>impala 不支持 hive 2.x以上的系列，于是选择1.1.0版本，在cloudera官网下载编译好的tarball</li><li>原本的hive lib 中缺少连接metastore的mysql jdbc驱动，自己下载jdbc connector jar放入lib目录下即可</li><li>由于我hive 用的cdh的版本，而hadoop用的是apache的版本，导致真正运行hive的时候会找不到mapreduce指定的类，为此lib目录下需加入hadoop-core-2.6.0-mr1-cdh5.9.1.jar</li><li><p>使用自带的schematool 创建元数据表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./schematool -initSchema -dbType mysql</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="impala镜像构建"><a href="#impala镜像构建" class="headerlink" title="impala镜像构建"></a>impala镜像构建</h3><ul><li>impala 我使用的是2.12.0的版本，这个版本cloudera官方没有提供tarball文件，只提供的rpm包。考虑到自身编译impala 成本比较大，于是采用rpm 安装的方法</li><li>详细的安装流程参考链接 <a href="http://lxw1234.com/archives/2017/06/862.htm" target="_blank" rel="noopener">Impala安装配置–RPM方式</a> 我这边只记录一下安装过程中遇到的坑</li><li><p>坑一：跟hive一样，由于我使用的hadoop是apache开源版本，有些class对应的包中没有</p><ul><li>软链hadoop-core-2.6.0-mr1-cdh5.9.1.jar到impala/lib中</li><li><p>需要对服务的启动文件catalogd, impalad, stastored改动</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for JAR_FILE in $&#123;IMPALA_HOME&#125;/lib/*.jar; do</span><br><span class="line"> export CLASSPATH=&quot;$&#123;JAR_FILE&#125;:$&#123;CLASSPATH&#125;&quot;</span><br><span class="line">done</span><br><span class="line">#hadoop share lib</span><br><span class="line">for HADOOP_JAR_FILE in $HADOOP_HOME/share/hadoop/tools/lib/*.jar; do</span><br><span class="line"> export CLASSPATH=&quot;$&#123;HADOOP_JAR_FILE&#125;:$&#123;CLASSPATH&#125;&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p>坑二：软链jdbc driver到impala/lib 并修改catalogd，其他启动文件相应都要修改</p></li><li><p>坑三：启动catalogd,impala-server时报错</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">E0804 16:42:09.008862   543 MetaStoreUtils.java:1274] Got exception: java.io.IOException No FileSystem for scheme: hdfs</span><br><span class="line">Java exception follows:</span><br><span class="line">java.io.IOException: No FileSystem for scheme: hdfs</span><br></pre></td></tr></table></figure></li></ul></li></ul><pre><code>    core-site.xml中加入配置    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;fs.file.impl&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;org.apache.hadoop.fs.LocalFileSystem&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;The FileSystem for file: uris.&lt;/description&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;fs.hdfs.impl&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;org.apache.hadoop.hdfs.DistributedFileSystem&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;The FileSystem for hdfs: uris.&lt;/description&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure>- 坑四 (class org.apache.hdfs.DistributedFileSystem not found)：    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在hadoop2.8.3的版本中org.apache.hadoop.hdfs.DistributedFileSystem can be found in hadoop-hdfs-client jar. </span><br><span class="line">只需要将包引入impala/lib 目录下即可</span><br></pre></td></tr></table></figure>- 坑五：HBase client各种依赖包没有    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ln -s $HBASE_HOME/lib/hbase-shaded-miscellaneous-2.1.0.jar hbase-shaded-miscellaneous.jar</span><br><span class="line">ln -s $HBASE_HOME/lib/hbase-shaded-protobuf-2.1.0.jar hbase-shaded-protobuf.jar</span><br><span class="line">ln -s $HBASE_HOME/lib/commons-lang3-3.6.jar commons-lang3.jar</span><br><span class="line">以</span><br></pre></td></tr></table></figure>- 坑六: 启动impala-server提示sasl plugin not found    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install cyrus-sasl* </span><br><span class="line">restart impala service</span><br></pre></td></tr></table></figure></code></pre><h2 id="impala-集群搭建"><a href="#impala-集群搭建" class="headerlink" title="impala 集群搭建"></a>impala 集群搭建</h2><h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><ul><li>20 个 impalad服务 与datanode混部，以最大化利用分布式本地查询的优势</li><li>catalog, statestore 与namenode节点混部以减少namenode网络IO带来的影响</li><li>全部服务部署docker化，脚本化，提供200与503脚本以便统一批量操作服务启动终止</li><li><p>200启动脚本是对impala提供的原生的启动命令的封装，包括监听运行进程的存活，便于对集群机器批量操作</p></li><li><p>200脚本封装代码</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">EXEC_FILE=/impala/impala-server</span><br><span class="line">PROGRESS=/usr/lib/impala/bin/impalad</span><br><span class="line"></span><br><span class="line">function usage() &#123;</span><br><span class="line">    echo -e &quot;\n A tool used for starting impala services</span><br><span class="line">Usage: 200.sh &#123;statestore|catalog|server&#125;</span><br><span class="line">&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">check_alive() &#123;</span><br><span class="line">    PID=`ps -ef | grep $IMPALA_USER_NAME | grep &quot;$PROGRESS&quot; | awk &apos;&#123;print $2&#125;&apos;`</span><br><span class="line">    [ -n &quot;$PID&quot; ] &amp;&amp; return 0 || return 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">start_service() &#123;</span><br><span class="line">    if [ ! -f $EXEC_FILE ];then</span><br><span class="line">        echo &quot;file not exists&quot;</span><br><span class="line">        exit 1</span><br><span class="line">    fi</span><br><span class="line">    check_alive</span><br><span class="line">    if [ $? -ne 0 ];then</span><br><span class="line">        $EXEC_FILE restart</span><br><span class="line">        sleep 10</span><br><span class="line">        check_alive</span><br><span class="line">        if [ $? -ne 0 ];then</span><br><span class="line">            echo &quot;service start error&quot;</span><br><span class="line">            exit 1</span><br><span class="line">        else</span><br><span class="line">            echo &quot;service start success&quot;</span><br><span class="line">            exit 0</span><br><span class="line">        fi</span><br><span class="line">    else</span><br><span class="line">        echo &quot;service alreay started&quot;</span><br><span class="line">        exit 0</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">    # $SERVICE_POOL是对应的服务池，通过docker run -e 参数传入</span><br><span class="line">    [[ -f /data0/hcp/conf/pools/$&#123;SERVICE_POOL&#125;/init-env.sh ]] &amp;&amp; source /data0/hcp/conf/pools/$&#123;HCP_POOL&#125;/init-env.sh</span><br><span class="line">    [[ -f /data0/hcp/sbin/impala/init-impala.sh ]] &amp;&amp; source /data0/hcp/sbin/impala/init-impala.sh</span><br><span class="line">    case &quot;$1&quot; in</span><br><span class="line">    statestore)</span><br><span class="line">            echo &quot;starting impala statestore&quot;</span><br><span class="line">        EXEC_FILE=/data0/hcp/sbin/impala/impala-state-store</span><br><span class="line">            PROGRESS=/usr/lib/impala/sbin/statestored</span><br><span class="line">            start_service</span><br><span class="line">        ;;</span><br><span class="line">        server)</span><br><span class="line">            echo &quot;starting impala server&quot;</span><br><span class="line">            EXEC_FILE=/data0/hcp/sbin/impala/impala-server</span><br><span class="line">            PROGRESS=/usr/lib/impala/sbin/impalad</span><br><span class="line">            start_service</span><br><span class="line">        ;;</span><br><span class="line">        catalog)</span><br><span class="line">            echo &quot;starting impala catalog&quot;</span><br><span class="line">            EXEC_FILE=/data0/hcp/sbin/impala/impala-catalog</span><br><span class="line">            PROGRESS=/usr/lib/impala/sbin/catalogd</span><br><span class="line">            start_service</span><br><span class="line">        ;;            </span><br><span class="line">        *)</span><br><span class="line">            usage</span><br><span class="line">            exit 1</span><br><span class="line">    esac</span><br><span class="line">&#125;</span><br><span class="line">main &quot;$@&quot;</span><br></pre></td></tr></table></figure><ul><li>503脚本设计思路类似</li></ul>]]></content>
      
      
    </entry>
    
    <entry>
      <title>okhttp support 100-continue for palo</title>
      <link href="/2018/04/24/okhttp-support-100-continue-for-palo/"/>
      <url>/2018/04/24/okhttp-support-100-continue-for-palo/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>虽然百度的Palo是个很强大的，基于MPP Search Engine的OLAP框架，但是由于处于开源的早期阶段，各方面都不是很完善。其中，Palo集群的稳定性对于日渐依赖Palo的核心的业务来说显得尤为重要。最近也一直在做Palo稳定性建设相关的工作。在对全链路监控这块，自然而然地想到对业务中使用频繁的http-mini-load接口进行SDK封装，以实现对请求进行失败重试以及失败率的监控报警的功能。</p><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><ul><li><p>在实际的SDK封装中，用到了流行的okhttp，发送请求如下:</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PaloHttpUtil paloHttpUtil = PaloHttpUtil.builder().build();</span><br><span class="line">String bodyStr = &quot;1 1 2018-04-12 20:13:00 4101628087476389 1\n1 1 2018-04-12 20:13:00 4205819141030267 2&quot;;</span><br><span class="line">System.out.println(paloHttpUtil</span><br><span class="line">        .put(String.format(&quot;http://xxxx:8030/api/feed/comment_trend/_load?&quot; +</span><br><span class="line">                &quot;label=comment_trend_load_%s&amp;columns=trend_type,user_type,timestamp,mid,count&quot;, prefix))</span><br><span class="line">        .auth(&quot;feed&quot;, &quot;feed&quot;)</span><br><span class="line">        .header(&quot;Expect&quot;, &quot;100-continue&quot;)</span><br><span class="line">        .body(bodyStr, ContentType.WILDCARD)</span><br><span class="line">        .asyncSend(3, 10000, TimeUnit.MILLISECONDS)</span><br><span class="line">        .string()</span><br></pre></td></tr></table></figure><p>  response: code 307 Temporary {“Status”:”Failed”} 也就是说发生了重定向。对于服务器为何不在一个request中直接接收PUT的数据，这块贴一下100-continue的定义。</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">100 (Continue/继续) :如果服务器收到头信息中带有100-continue的请求，这是指客户端询问是否可以在后续的请求中发送附件。</span><br><span class="line">在这种情况下，服务器用100(SC_CONTINUE)允许客户端继续或用417 (Expectation Failed)告诉客户端不同意接受附件。这个状态码是 HTTP 1.1中新加入的。</span><br></pre></td></tr></table></figure></li></ul><pre><code>至于为什么发生了重定向先不考虑，先研究一下为什么okhttp不支持307重定向。</code></pre><h4 id="源码追踪"><a href="#源码追踪" class="headerlink" title="源码追踪"></a>源码追踪</h4><ul><li>我们通过debug深入源码看看在哪一步处理的307重定向</li></ul><p><img src="https://pic2.zhimg.com/80/v2-9dca7e4ac92cae2de9844b1cf5565825_hd.jpg" alt=""></p><p>由图，我们可以看到对于request/response的处理，okhttp采取了插件的形式，类似于Spring AOP 源码中切面invoke方法的处理方式。这种插件的方式意味着我们可以定制化请求处理逻辑。借官方原图：</p><p><img src="https://pic2.zhimg.com/80/v2-c66ef8de0a698b2b2a7eb99856790ea1_hd.jpg" alt=""></p><ul><li><p>由于interceptor里面是可以执行重试逻辑或直接返回response，所以，我们再深入看看在哪个Interceptor里直接返回了response。断点打到RetryAndFollowUpInterceptor里如下的代码块：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Request followUp = this.followUpRequest(response, streamAllocation.route());</span><br><span class="line">//这里followUP返回null</span><br><span class="line">           if (followUp == null) &#123;</span><br><span class="line">               if (!this.forWebSocket) &#123;</span><br><span class="line">                   streamAllocation.release();</span><br><span class="line">               &#125;</span><br><span class="line"></span><br><span class="line">               return response;</span><br><span class="line">           &#125;</span><br></pre></td></tr></table></figure><p>  由于followUp返回了null，导致response直接返回。说明当前的redirect策略不支持307重定向，再深入具体的重定向策略followUpRequest</p><p>  <img src="https://pic2.zhimg.com/80/v2-f1c8c432fbd725b19055ef3dcf8228a1_hd.jpg" alt=""></p><p>  发现307，308如果request method不等于GET且不为HEAD时直接返回了null，由此对于307 的PUT重定向操作okhttp是不支持的</p></li></ul><h2 id="问题的解决"><a href="#问题的解决" class="headerlink" title="问题的解决"></a>问题的解决</h2><h4 id="okhttp-添加自定义redirect-interceptor"><a href="#okhttp-添加自定义redirect-interceptor" class="headerlink" title="okhttp 添加自定义redirect interceptor"></a>okhttp 添加自定义redirect interceptor</h4><ul><li><p>前面提到，我们可以往okhttpClient里面添加自定义的interceptor来达到对request/response灵活劫持的目的。于是考虑加一个支持palo put 307 重定向的redirect interceptor.</p></li><li><p>大致的策略还是跟RetryAndFollowUpInterceptor一样，对followUpRequest的方法做了修改</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">case 300:</span><br><span class="line">  case 301:</span><br><span class="line">case 302:</span><br><span class="line">case 303:</span><br><span class="line">case 307:</span><br></pre></td></tr></table></figure><p>  将307放到了300-303并列的位置，进入redirect逻辑。去掉将method统一替换成GET的逻辑:</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if (HttpMethod.redirectsToGet(method)) &#123;</span><br><span class="line">requestBuilder.method(&quot;GET&quot;, (RequestBody)null);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">RequestBody requestBody = maintainBody ?userResponse.request().body() : null;</span><br><span class="line">requestBuilder.method(method, requestBody);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  改为:</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">boolean maintainBody = HttpMethod.requiresRequestBody(method);</span><br><span class="line">                                   RequestBody requestBody = maintainBody ? userResponse.request().body() : null;</span><br><span class="line">                                   requestBuilder.method(method, requestBody);</span><br></pre></td></tr></table></figure><p>  为了带上用户名密码，去掉逻辑:</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> if (!this.sameConnection(userResponse, url))&#123;</span><br><span class="line"> requestBuilder.removeHeader(&quot;Authorization&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li>至此，Palo http-mini-load put 307 Temporary 重定向问题得到了解决</li></ul><h4 id="使用apache-httpcomponents"><a href="#使用apache-httpcomponents" class="headerlink" title="使用apache httpcomponents"></a>使用apache httpcomponents</h4><ul><li><p>在apache httpcomponents中，可以设置redirectStrategy，来达到重定向的策略，且不受http code的约束</p><p>  <img src="https://pic1.zhimg.com/80/v2-4f64e67306feb76dcc66ffea25931da8_hd.jpg" alt=""></p><p>  可以看到本身的redirect机制还是比较强大的</p></li><li><p>不过鉴于本人习惯用okhttp，且用自定义的interceptro也能解决问题，所以暂时没有采用这种方法。</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>okhttp 不支持307除get意外其他request method重定向的原因不得而知。不过对于开源的组件，也不必要满足各种各样奇怪的胃口，对于需求的定制化留好可扩展接口就行。</li></ul>]]></content>
      
      
        <tags>
            
            <tag> kylin - Java - 源码 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>JVM知识总结</title>
      <link href="/2018/03/26/JVM%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
      <url>/2018/03/26/JVM%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h2 id="HotSpot-JVM-Architecture"><a href="#HotSpot-JVM-Architecture" class="headerlink" title="HotSpot JVM Architecture"></a>HotSpot JVM Architecture</h2><p><img src="http://jacobs.wanhb.cn/images/hotspotjvm-1.png" alt="HotSpot JVM Architecture"></p><h2 id="JVM运行时数据区域"><a href="#JVM运行时数据区域" class="headerlink" title="JVM运行时数据区域"></a>JVM运行时数据区域</h2><ul><li>程序计数器 (program counter registers)</li><li>java虚拟机栈：线程私有，生命周期与线程相同。每个方法在运行期间都会创建一个栈帧用于存储局部变量、操作数栈、动态链接、方法出口等信息。<ul><li>经常有人把java内存区分为堆内存和栈内存，这种分法比较粗糙。其中所指的栈就是虚拟机栈。或者说是虚拟机栈中局部变量表部分。</li><li>局部变量表存放了编译期间可知的各种基本数据类型(boolean byte char,…)、对象引用和returnAddress类型</li></ul></li><li>本地方法栈 为虚拟机提供Native方法服务</li><li>java堆： 是JVM所管理的内存中最大的一块。是被所有线程共享的一块内存区域。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配（随着JIT编译器的发展与逃逸分析技术的成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，导致在堆上分配对象不是那么绝对）。</li><li><p>方法区： 与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。</p><ul><li>对于HotSpot熟悉的人员来说，很多人更愿意把方法区称为“永久代”本质上并不等价，仅仅是因为HotSpot虚拟机设计团队选择把GC分代收集扩展到方法区，或者说使用永久代来实现方法区而已。但是这种实现方式更容易遇到内存溢出的问题，现在HotSpot虚拟机也放弃永久代逐步改为采用Native Memory来实现方法区的规划了（如：把原本放在永久代的字符串常量池移除）</li><li>JVM规范对方法区限制非常宽松，但也不是永久存在，还是会存在类的卸载，常量池的回收问题</li></ul></li><li><p>运行时常量池：方法区的一部分，Class文件中出了有类的版本、字段、方法、接口等描述信息外，还有常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。</p><ul><li>运行期间也可以将新的常量放入池中，如String类的intern()方法（目前被废除）</li></ul></li><li><p>直接内存(Direct Memory)：不是JVM规范定义的内存区域，但是也被频繁使用，而且也可能导致内存溢出异常。在JDK1.4新加入了NIO 类，引入了一种基于通道与缓冲区Buffer的IO方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。显然，这部分内存不会受到Java堆大小的限制，但是还是受到本机总内存的限制</p></li></ul><h2 id="对象创建"><a href="#对象创建" class="headerlink" title="对象创建"></a>对象创建</h2><h3 id="分配实例内存的时候可能会出现并发问题的解决方案："><a href="#分配实例内存的时候可能会出现并发问题的解决方案：" class="headerlink" title="分配实例内存的时候可能会出现并发问题的解决方案："></a>分配实例内存的时候可能会出现并发问题的解决方案：</h3><ul><li>对分配内存空间的动作进行同步处理———实际上JVM采用CAS配上失败重试的方式保证更新操作的原子性</li><li>把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存称为本地线程分配缓冲（TLAB）。哪个线程要分配内存就在哪个TLAB上分配，只有TLAB用完并分配新的的时候才需要同步锁定。虚拟机是否使用TLAB，可以通过-XX：+／-UseTLAB参数来设定</li></ul><h3 id="GC-算法："><a href="#GC-算法：" class="headerlink" title="GC 算法："></a>GC 算法：</h3><ul><li>可达性分析算法（不算GC算法，仅仅是思想）:</li><li>GC Roots的对象包括下面几种：<ul><li>1、虚拟机栈（栈帧中的本地变量表）中引用的对象</li><li>2、方法区中类静态属性引用的对象</li><li>3、方法区中常量引用的对象</li><li>4、本地方法栈中JNI（即一般说的Native方法）引用的对象。</li></ul></li></ul><ul><li>引用计数法：</li><li>标记清除（Mark-Sweep)：</li><li>复制算法 (Copying)</li><li>标记-压缩算法 (Mark-Compact)</li><li>增量算法 (Incremental Collecting)</li></ul><h3 id="JVM-垃圾回收器分类"><a href="#JVM-垃圾回收器分类" class="headerlink" title="JVM 垃圾回收器分类"></a>JVM 垃圾回收器分类</h3><ul><li>Young generation:<ul><li>Serial</li><li>ParNew</li><li>Parallel Scavenge</li></ul></li><li>Tenured generation:<ul><li>CMS</li><li>Parallel Old</li><li>Serial Old</li></ul></li><li><p>All</p><ul><li>G1</li></ul></li><li><p>新生代串行收集器（GC日志中:Default New Generation）： 第一，它仅仅使用单线程进行垃圾回收；第二，它独占式的垃圾回收。复制算法</p></li><li>老年代串行收集器： 标记-压缩算法。</li><li>-XX:+UseParNewGC 参数设置，表示新生代使用并行收集器，老年代使用串行收集器</li><li>-XX:+UseParallelGC 参数设置，表示新生代和老年代均使用并行回收收集器</li><li>新生代并行回收 (Parallel Scavenge) 收集器，对应的GC日志中新生代的名称为PSYoungGen</li><li>老年代并行回收收集器</li><li><p>CMS 收集器: 它使用的是标记-清除算法，同时它又是一个使用多线程并发回收的垃圾收集器。</p><ul><li>主要步骤有：初始标记、并发标记、重新标记、并发清除和并发重置。其中初始标记和重新标记是独占系统资源的，而并发标记、并发清除和并发重置是可以和用户线程一起执行的。因此，从整体上来说，CMS 收集不是独占式的，它可以在应用程序运行过程中进行垃圾回收。</li><li>标记-清除算法将会造成大量内存碎片，离散的可用空间无法分配较大的对象。在这种情况下，即使堆内存仍然有较大的剩余空间，也可能会被迫进行一次垃圾回收，以换取一块可用的连续内存，这种现象对系统性能是相当不利的，为了解决这个问题，CMS 收集器还提供了几个用于内存压缩整理的算法。</li><li>-XX:+UseCMSCompactAtFullCollection 参数可以使 CMS 在垃圾收集完成后，进行一次内存碎片整理。内存碎片的整理并不是并发进行的。</li><li><p>-XX:CMSFullGCsBeforeCompaction 参数可以用于设定进行多少次 CMS 回收后，进行一次内存压缩。使用CMS收集器后，默认新老年代分别使用ParNew和CMS收集器进行垃圾回收</p></li><li><p>G1 收集器 (Garbage First) : 与 CMS 收集器相比，G1 收集器是基于标记-压缩算法的</p><ul><li>G1 收集器还可以进行非常精确的停顿控制。它可以让开发人员指定当停顿时长为 M 时，垃圾回收时间不超过 N。</li><li>使用参数-XX:+UnlockExperimentalVMOptions –XX:+UseG1GC 来启用 G1 回收器，设置 G1 回收器的目标停顿时间：-XX:MaxGCPauseMills=20,-XX:GCPauseIntervalMills=200。</li></ul></li></ul></li></ul><h3 id="JVM内存分配情况"><a href="#JVM内存分配情况" class="headerlink" title="JVM内存分配情况"></a>JVM内存分配情况</h3><ul><li><a href="http://trustmeiamadeveloper.com/2016/03/18/where-is-my-memory-java/" target="_blank" rel="noopener">参考链接</a></li><li>容器内存 = 文件缓存 + Java 应用内存<br>Java 应用内存 = 堆内存 + MetaSpace + 堆外内存<br>堆外内存 = 线程栈 + 缓冲区（例如 NIO）等等</li><li>所以按照目前的经验来看，堆内存至少应该小于 2/3 的容器内存（？）</li></ul><h3 id="happens-before规则"><a href="#happens-before规则" class="headerlink" title="happens-before规则"></a>happens-before规则</h3><ul><li>定义<ul><li>如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。</li><li>两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这种重排序并不非法（也就是说，JMM允许这种重排序）</li></ul></li><li>程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。</li><li>监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。</li><li>volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。</li><li>传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。</li><li>start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。</li><li>join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。</li><li>程序中断规则：对线程interrupted()方法的调用先行于被中断线程的代码检测到中断时间的发生。</li><li>对象finalize规则：一个对象的初始化完成（构造函数执行结束）先行于发生它的finalize()方法的开始。</li></ul><h3 id="对象转入老年代的情况："><a href="#对象转入老年代的情况：" class="headerlink" title="对象转入老年代的情况："></a>对象转入老年代的情况：</h3><ul><li>1、经过了正常的MinoGC过程数次之后（可以通过-XX:MaxTenuringThreshold配置）晋升到老年代</li><li>2、大于一定值的对象直接进入老年代（ 通过-XX:PretenureSizeThreshold配置）</li><li>3、空间分配担保：MinorGC之前，jvm会检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果成立，那么MinorGC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于则尝试一次MinorGC，尽管会有风险，如果小于。或者HandlePromotionFailure设置不允许冒险，那这时候也需要改为进行一次Full GC.</li><li>因为新生代是使用复制收集算法，但是为了内存利用率，只使用其中一个Survivor空间作为轮换备份，因此当出现大量对象在MinorGC后仍然存活。就需要老年代进行分配担保。把Survivor无法容纳的对象直接进入老年代。前提是老年代还有容纳这些对象的剩余空间，一共有多少对象会存活下来在实际完成内存回收之前是无法明确知道的，所以只好取之前每一次回收晋升到老年代对象容量的平均大小值作为经验值，与老年代的剩余空间比较，决定是否进行FullGc来让老年代腾出空间。如果允许担保失败(HandlePromotionFailure)并且老年代最大可用连续空间大于历次晋升到老年代对象的平均大小，将尝试进行一次MinorGC，如果小于或者不允许担保失败则会进行一次 FullGC，所以一般会将HandlePromotionFailure打开，以防止FullGC频率过高</li></ul><h3 id="一些JVM工具"><a href="#一些JVM工具" class="headerlink" title="一些JVM工具"></a>一些JVM工具</h3><ul><li><p>jps : 虚拟机进程状况工具</p><ul><li><a href="http://www.importnew.com/?p=18132&amp;preview=true" target="_blank" rel="noopener">详细参考文章</a></li><li>-q 只输出LVMID,省略主类名称</li><li>-m 输出虚拟机进程启动时传递给主类main()函数的参数</li><li>-l 输出主类的全名，如果进程执行的是Jar包，输出jar路径</li><li>-v 输出虚拟机进程启动时JVM参数</li></ul></li><li><p>jstat: 虚拟机统计信息监视工具，可以显示本地或者远程虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据</p><ul><li><a href="http://www.importnew.com/18202.html" target="_blank" rel="noopener">详细参考文章</a></li><li>-class pid 监视类装载、卸载数量、总空间以及类装载所耗费时间</li><li>-gc pid 监视Java堆状况，包括Eden区，两个Survivor区、老年代、永久代容量、已用空间、GC时间合计等信息</li><li>-gccapacity pid 可以显示，VM内存中三代（young,old,perm）对象的使用和占用大小</li><li>-gcnew pid 年轻代对象的信息</li><li>-gcold 年老代对象信息</li></ul></li><li><p>jinfo: java配置信息工具</p><ul><li>实时地查看和调整虚拟机各项参数，未被显示指定的参数的系统默认值</li><li>jinfo -flag CMSInitiatingOccupancyFraction 14444</li></ul></li><li><p>jmap java内存映象工具 用于生成堆转储快照（一般称为heapdump或dump文件）如果不使用jmap命令，还可以使用暴力的手段如：-XX:+HeapDumpOnOutOfMemoryError参数，可以让虚拟机在 OOM异常之后自动生成dump文件。通过-XX:+HeapDumpOnCtrlBreak参数则可以使用[Ctrl]+[Break]键让虚拟机生成dump文件。</p><ul><li><a href="http://www.importnew.com/18196.html" target="_blank" rel="noopener">详细参考文章</a></li><li>jmap 的作用不仅仅是为了获取dump文件，还可以查询finalize执行队列、Java堆和永久代的详细信息，如空间使用率、当前用的是哪种收集器等。</li><li><p>-dump 生成java堆转储快照 -dump:[live, ]format=b, file=,其中live子参数说明是否只dump出存活的对象。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jmap -dump:format=b,file=20190108.dump [pid]</span><br></pre></td></tr></table></figure></li><li><p>-heap 显示java堆详细信息，如使用那种回收器、参数配置、分代状况等／</p></li><li>-histo 显示堆中对象的统计信息，包括类，实例数量，合计容量</li><li>-F 当虚拟机进程对-dump选项没有响应时，可使用这个选项强制生成dump快照。</li><li>例子：jmap -dump:format=b,file=exlipse.bin 3500 (3500 是通过jps命令查询到的LVMID)</li></ul></li><li><p>jhat: 虚拟机堆转储快照分析工具</p><ul><li>sun JDK提供jhat 与jmap 搭配使用，来分析jmap 生成的堆转储快照。jhat内置了一个微型的Http/Html服务器，生成dump文件的分析结果后，可以在浏览器中查看之后可以用visualVM来分析dump文件</li></ul></li><li><p>jstack: java堆栈跟踪工具，用于生成虚拟机当前时刻的线程快照。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环</p><ul><li><a href="http://www.importnew.com/18176.html" target="_blank" rel="noopener">详细参考文章</a></li></ul></li></ul><h3 id="垃圾收集器参数总结："><a href="#垃圾收集器参数总结：" class="headerlink" title="垃圾收集器参数总结："></a>垃圾收集器参数总结：</h3><ul><li>UseSerialGC ：虚拟机运行在Client模式下的默认值，打开此开关后，使用Serial+Serial Old 的收集器组合进行内存回收</li><li>UseParNewGC: 打开此开关后使用ParNew+SerialOld的收集器组合进行内存回收</li><li>UseConcMarkSweepGC：打开此开关后，使用ParNew+CMS+Serial Old的收集器组合进行内存回收，Serial Old收集器将作为CMS收集器出现Concurrent Mode Failure失败后的后备收集器使用</li><li>UseparallelGC 虚拟机运行在Server模式下的默认值，打开此开关后，使用Parallel Scavenge+Serial Old (PS MarkSweep) 的收集器组合进行内存回收<br>UseParallelOldGC: 使用Parallel Scavenge+ Parallel Old的收集器组合进行内存回收</li></ul><h3 id="GC-相关参数总结"><a href="#GC-相关参数总结" class="headerlink" title="GC 相关参数总结"></a>GC 相关参数总结</h3><ul><li>与串行回收器相关的参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-XX:+UseSerialGC:在新生代和老年代使用串行回收器。</span><br><span class="line">-XX:+SurvivorRatio:设置 eden 区大小和 survivor 区大小的比例。</span><br><span class="line">-XX:+PretenureSizeThreshold:设置大对象直接进入老年代的阈值。当对象的大小超过这个值时，将直接在老年代分配。</span><br><span class="line">-XX:MaxTenuringThreshold:设置对象进入老年代的年龄的最大值。每一次 Minor GC 后，对象年龄就加 1。任何大于这个年龄的对象，一定会进入老年代。</span><br></pre></td></tr></table></figure><ul><li>与并行 GC 相关的参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-XX:+UseParNewGC: 在新生代使用并行收集器。</span><br><span class="line">-XX:+UseParallelOldGC: 老年代使用并行回收收集器。</span><br><span class="line">-XX:ParallelGCThreads：设置用于垃圾回收的线程数。通常情况下可以和 CPU 数量相等。但在 CPU 数量比较多的情况下，设置相对较小的数值也是合理的。</span><br><span class="line">-XX:MaxGCPauseMills：设置最大垃圾收集停顿时间。它的值是一个大于 0 的整数。收集器在工作时，会调整 Java 堆大小或者其他一些参数，尽可能地把停顿时间控制在 MaxGCPauseMills 以内。</span><br><span class="line">-XX:GCTimeRatio:设置吞吐量大小，它的值是一个 0-100 之间的整数。假设 GCTimeRatio 的值为 n，那么系统将花费不超过 1/(1+n) 的时间用于垃圾收集。</span><br><span class="line">-XX:+UseAdaptiveSizePolicy:打开自适应 GC 策略。在这种模式下，新生代的大小，eden 和 survivor 的比例、晋升老年代的对象年龄等参数会被自动调整，以达到在堆大小、吞吐量和停顿时间之间的平衡点。</span><br></pre></td></tr></table></figure><ul><li>与 CMS 回收器相关的参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-XX:+UseConcMarkSweepGC: 新生代使用并行收集器，老年代使用 CMS+串行收集器。</span><br><span class="line">-XX:+ParallelCMSThreads: 设定 CMS 的线程数量。</span><br><span class="line">-XX:+CMSInitiatingOccupancyFraction:设置 CMS 收集器在老年代空间被使用多少后触发，默认为 68%。</span><br><span class="line">-XX:+UseFullGCsBeforeCompaction:设定进行多少次 CMS 垃圾回收后，进行一次内存压缩。</span><br><span class="line">-XX:+CMSClassUnloadingEnabled:允许对类元数据进行回收。</span><br><span class="line">-XX:+CMSParallelRemarkEndable:启用并行重标记。</span><br><span class="line">-XX:CMSInitatingPermOccupancyFraction:当永久区占用率达到这一百分比后，启动 CMS 回收 (前提是-XX:+CMSClassUnloadingEnabled 激活了)。</span><br><span class="line">-XX:UseCMSInitatingOccupancyOnly:表示只在到达阈值的时候，才进行 CMS 回收。</span><br><span class="line">-XX:+CMSIncrementalMode:使用增量模式，比较适合单 CPU。</span><br></pre></td></tr></table></figure><ul><li>与 G1 回收器相关的参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-XX:+UseG1GC：使用 G1 回收器。</span><br><span class="line">-XX:+UnlockExperimentalVMOptions:允许使用实验性参数。</span><br><span class="line">-XX:+MaxGCPauseMills:设置最大垃圾收集停顿时间。</span><br><span class="line">-XX:+GCPauseIntervalMills:设置停顿间隔时间。</span><br></pre></td></tr></table></figure><ul><li>其他参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:+DisableExplicitGC: 禁用显示 GC。</span><br></pre></td></tr></table></figure><h3 id="JVM调优实战"><a href="#JVM调优实战" class="headerlink" title="JVM调优实战"></a>JVM调优实战</h3><p>除了Java堆和永久代之外还有以下区域还会占用较多的内存，这些内存总和受到操作系统进程最大内存的限制</p><ul><li>DirectMemory：可通过 -XX:MaxDirectMemorySize调整大小，内存不足时抛出OutOfMemoryError 或者 OutOfMemoryError: Direct buffer memory.如果把系统大部分的内存都分配给了java堆，并且很长时间没有发生GC操作。那么DirectMemory可能会溢出。因为DirectMemory只是等待老年代满了之后FullGC，然后顺便帮它清理掉内存的废弃对象。否则就只能一直等到抛出内存溢出异常。</li><li>线程堆栈： 可通过-Xss调整大小，内存不足时抛出StackOverflowError</li><li>Socket 缓存区：每个Socket连接都Receive和Send两个缓存区，分别占大约37KB和25KB内存，连续多的话这块内存也很可观。</li><li>JNI 代码：如果代码中使用JNI调用本地库，那本地库使用的内存也不在堆中。</li><li>虚拟机和GC：虚拟机、GC的代码执行也要消耗一定的内存</li></ul><ul><li>解读GC 日志</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[GC (Allocation Failure) 2017-10-15T16:02:33.567+0800: 790.871: [ParNew</span><br><span class="line">Desired survivor size 17432576 bytes, new threshold 6 (max 6)</span><br><span class="line">- age 1: 7619992 bytes, 7619992 total</span><br><span class="line">- age 2: 1081760 bytes, 8701752 total</span><br></pre></td></tr></table></figure><p>可以明显看出上述 GC 日志包含两次 Minor GC。 注意到第二次 Minor GC 的情况， 日志打出 “Desired survivor size 53673984 bytes”， 但是却存活了 “- age 1: 107256200 bytes, 107256200 total” 这么多。 可以看出明显的新生代的 Survivor 空间不足。正因为 Survivor 空间不足， 那么从 Eden 存活下来的和原来在 Survivor 空间中不够老的对象占满 Survivor 后， 就会提升到老年代， 可以看到这一轮 Minor GC 后老年代由原来的 0K 占用变成了 105782K 占用， 这属于一个典型的 JVM 内存问题， 称为 “premature promotion”(过早提升)。”premature promotion” 在短期看来不会有问题， 但是经常性的”premature promotion”， 最总会导致大量短期对象被提升到老年代， 最终导致老年代空间不足， 引发另一个 JVM 内存问题 “promotion failure”（提升失败： 即老年代空间不足以容乃 Minor GC 中提升上来的对象）。 “promotion failure” 发生就会让 JVM 进行一次 CMS 垃圾收集进而腾出空间接受新生代提升上来的对象， CMS 垃圾收集时间比 Minor GC 长， 导致吞吐量下降、 时延上升， 将对用户体验造成影响。</p><h3 id="Java什么情况下会报OutOfMemoryError"><a href="#Java什么情况下会报OutOfMemoryError" class="headerlink" title="Java什么情况下会报OutOfMemoryError"></a>Java什么情况下会报OutOfMemoryError</h3><ul><li>如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StatckOverflowError异常；若果虚拟机栈可以动态扩展(当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈)，当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。</li><li>根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样，在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的(通过-Xmx和-Xms控制)。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。</li><li>根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。</li><li>运行时常量池是方法区的一部分，自然会受到方法区内存的限制，当常量池无法在申请到内存时会抛出OutOfMemoryError异常。</li><li>直接内存(Direct Memory)并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError异常的出现。在JDK1.4中新加入了NIO(New Input/Output)类，引入了一种基于通道(Channel)与缓冲区(Buffer)的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作，这样能在一些场景中显著提高性能，因为避免了在Java堆和Native中来回复制数据。显然，本机直接内存的分配不会受到Java堆大小的限制，但是，既然是内存，则肯定还是会受到本机总内存(包括RAM及SWAP区或分页文件)的大小及处理器寻址空间的限制。</li></ul><h3 id="虚拟机类加载机制"><a href="#虚拟机类加载机制" class="headerlink" title="虚拟机类加载机制"></a>虚拟机类加载机制</h3><ul><li>类加载的生命周期：加载—验证—准备—解析—初始化—使用—卸载</li><li><p>类初始化阶段：<br>是类加载过程的最后一步，到了初始化阶段，才真正开始执行类中定义的Java程序代码（或者说字节码）<br>在准备阶段，变量已经赋值锅系统要求的初始值，，而在初始化阶段，则根据程序员通过程序制定的主观计划去初始化类变量和其他资源，从另一个方面表达：<br>初始化阶段是执行类构造器()方法的过程。<br>方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块(static{}块)中的语句合并产生的，编译器收集的顺序是由语句在原文件中出现的顺序决定的。</p></li><li><p>静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。如：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public class test&#123;</span><br><span class="line">static&#123;</span><br><span class="line">i=0; //给变量赋值可以正常编译通过</span><br><span class="line">System.out.print(i); //这句编译器会提示“非法向前引用”</span><br><span class="line">&#125;</span><br><span class="line">static int i=1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  () 方法与类的构造函数不同，它不需要显示的调用父类构造器，虚拟机会保证在子类的clinit方法执行之前，父类的cinit方法已经执行完毕。因此在虚拟机中第一个被执行的cinit方法的类肯定是java.lang.object。由于父类的clinit方法先执行，也就意味着父类中定义的静态语句块要优先于子类变量赋值操作。比如如下B的值将会是2而不是1</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">static class Parent&#123;</span><br><span class="line">public static int A =1;</span><br><span class="line">static &#123;</span><br><span class="line">A =2</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static class Sub extends Parent&#123;</span><br><span class="line">public static int B=A</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args)&#123;</span><br><span class="line">system.out.println(Sub.B);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><pre><code>- 1）clinit 方法不是必须的，如果一个类中没有静态语句块，也没有对变量的赋值操作，也就没有必要为这个类生成clint方法。- 2）接口中不能使用静态语句块，但仍然有变量初始化赋值操作，因此接口和类一样都会生成clinit方法。但接口与类不同的是，执行接口的clinit方法不需要先执行父接口的clinit方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口实现类在初始化时也一样不会执行接口的clinit方法- 3）虚拟机保证一个类的clinit方法在多线程环境中被正确地加锁、同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的clinit方法，其他线程都需要阻塞等待，直到活动线程执行clinit方法完毕。同一个类加载器下，一个类型只会初始化一次</code></pre><ul><li>类加载器<ul><li>1）用于实现类的加载动作.比较两个类是否相等，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则即使来源于同一个Class文件，被同一个虚拟机加载，只要类加载器不同，那么这两个类就必定不相等。相等指的是 equals() isAssignableFrom()方法，isInstance()方法返回的结果。也包括使用instanceof关键字做对象所属关系判定等情况。</li><li>2）双亲委派模型<br>从Java虚拟机的角度，只存在两种不同的类加载器：一种是启动类加载器(Bootstrap ClassLoader)，这个类加载器由C++实现，另一种就是所有其他的类加载器，由Java实现并且都继承自抽象类java.lang.ClassLoader</li><li>3）从开发人员的角度，类加载器还可以分更细：<ul><li>启动类加载器(Bootstrap ClassLoader)：负责加载Java_HOME\lib目录中的，或者被-Xbooclasspath参数所指定的路径中的，并且是虚拟机识别的类库加载到虚拟机内存中。无法被java程序直接饮用。</li><li>扩展类加载器（Extension ClassLoader） 负责加载JAVA_HOME\lib\ext目录中，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器</li><li>应用程序加载器（Application ClassLoader） 这个类加载器是ClassLoader中的getSystemClassLoader()方法的返回值，所以也被称为系统类加载器。负责加载用户类路径（ClassPath）上所指定的类库，如果应用程序没有自定义锅自己的类加载器，一般情况下这个就是程序中默认的类加载器</li></ul></li></ul></li></ul><h3 id="JVM逃逸分析"><a href="#JVM逃逸分析" class="headerlink" title="JVM逃逸分析"></a><a href="http://www.importnew.com/23150.html" target="_blank" rel="noopener">JVM逃逸分析</a></h3><ul><li>逃逸分析的基本行为就是分析对象动态作用域：当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他地方中，称为方法逃逸</li><li>如果能证明一个对象不会逃逸到方法或线程外，则可能为这个变量进行一些高效的优化</li></ul><h4 id="优化点"><a href="#优化点" class="headerlink" title="优化点"></a>优化点</h4><ul><li>栈上分配<ul><li>我们都知道Java中的对象都是在堆上分配的，而垃圾回收机制会回收堆中不再使用的对象，但是筛选可回收对象，回收对象还有整理内存都需要消耗时间。如果能够通过逃逸分析确定某些对象不会逃出方法之外，那就可以让这个对象在栈上分配内存，这样该对象所占用的内存空间就可以随栈帧出栈而销毁，就减轻了垃圾回收的压力。</li><li>在一般应用中，如果不会逃逸的局部对象所占的比例很大，如果能使用栈上分配，那大量的对象就会随着方法的结束而自动销毁了。</li></ul></li><li>同步消除：参考<a href="https://my.oschina.net/hosee/blog/615865" target="_blank" rel="noopener">高并发Java 九锁的优化和注意事项</a></li><li>标量替换<ul><li>Java虚拟机中的原始数据类型（int，long等数值类型以及reference类型等）都不能再进一步分解，它们就可以称为标量。相对的，如果一个数据可以继续分解，那它称为聚合量，Java中最典型的聚合量是对象。如果逃逸分析证明一个对象不会被外部访问，并且这个对象是可分解的，那程序真正执行的时候将可能不创建这个对象，而改为直接创建它的若干个被这个方法使用到的成员变量来代替。拆散后的变量便可以被单独分析与优化，可以各自分别在栈帧或寄存器上分配空间，原本的对象就无需整体分配空间了</li></ul></li></ul><h3 id="JIT-与Java10"><a href="#JIT-与Java10" class="headerlink" title="JIT 与Java10"></a>JIT 与Java10</h3><ul><li>Introduction:<ul><li>对于大部分应用开发者来说，Java编译器指的是JDK自带的javac指令。这一指令可将Java源程序编译成.class文件，其中包含的代码格式我们称之为Java bytecode（Java字节码）。这种代码格式无法直接运行，但可以被不同平台JVM中的interpreter解释执行。<em>由于interpreter效率低下，JVM中的JIT compiler（即时编译器）会在运行时有选择性地将运行次数较多的方法编译成二进制代码，直接运行在底层硬件上。</em>Oracle的HotSpot VM便附带两个用C++实现的JIT compiler：C1及C2。</li><li>与interpreter，GC等JVM的其他子系统相比，JIT compiler并不依赖于诸如直接内存访问的底层语言特性。它可以看成一个输入Java bytecode输出二进制码的黑盒，其实现方式取决于开发者对开发效率，可维护性等的要求。Graal是一个以Java为主要编程语言，面向Java bytecode的编译器。与用C++实现的C1及C2相比，它的模块化更加明显，也更加容易维护。Graal既可以作为动态编译器，在运行时编译热点方法；亦可以作为静态编译器，实现AOT编译。在Java 10中，Graal作为试验性JIT compiler一同发布（JEP 317）。这篇文章将介绍Graal在动态编译上的应用。有关静态编译，可查阅JEP 295或Substrate VM。</li></ul></li><li>Tiered Compilation<ul><li>HotSpot中的tiered compilation<ul><li>JIT compiler — C1及C2（或称为Client及Server）<ul><li>前者没有应用激进的优化技术，因为这些优化往往伴随着耗时较长的代码分析。因此，C1的编译速度较快，而C2所编译的方法运行速度较快。在Java 7前，用户需根据自己的应用场景选择合适的JIT compiler。举例来说，针对偏好高启动性能的GUI用户端程序则使用C1，针对偏好高峰值性能的服务器端程序则使用C2。</li><li>Java 7引入了tiered compilation的概念，综合了C1的高启动性能及C2的高峰值性能。这两个JIT compiler以及interpreter将HotSpot的执行方式划分为五个级别：<ul><li>level 0：interpreter解释执行</li><li>level 1：C1编译，无profiling</li><li>level 2：C1编译，仅方法及循环back-edge执行次数的profiling</li><li>level 3：C1编译，除level 2中的profiling外还包括branch（针对分支跳转字节码）及receiver type（针对成员方法调用或类检测，如checkcast，instnaceof，aastore字节码）的profiling</li><li>level 4：C2编译。其中，1级和4级为接受状态 — 除非已编译的方法被invalidated（通常在deoptimization中触发），否则HotSpot不会再发出该方法的编译请求。</li></ul></li></ul></li></ul></li></ul></li></ul><h3 id="JVM优秀博客"><a href="#JVM优秀博客" class="headerlink" title="JVM优秀博客"></a>JVM优秀博客</h3><ul><li><a href="http://www.importnew.com/30461.html" target="_blank" rel="noopener">Java字节码结构剖析一：常量池</a></li><li><a href="http://www.importnew.com/30505.html" target="_blank" rel="noopener">Java字节码结构剖析二：字段表</a></li><li><a href="http://www.importnew.com/30521.html" target="_blank" rel="noopener">Java字节码结构剖析三：方法表</a></li><li><a href="http://www.importnew.com/26595.html" target="_blank" rel="noopener">从字节码和 JVM 的角度解析 Java 核心类 String 的不可变特性</a></li><li><a href="https://my.oschina.net/hosee?tab=popular" target="_blank" rel="noopener">高并发java博客系列</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kylin query原理剖析</title>
      <link href="/2017/10/31/kylin-query%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/"/>
      <url>/2017/10/31/kylin-query%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近我们组负责数据建模的同学抱怨kylin的relization选择策略：同一个project下一条查询语句本来期望命中某一个cube的，结果系统却选择了其他cube。之前也有大概翻阅过kylin这块的实现源码，知道如果同一个project下如果有多个满足条件的的实现，会按照成本排序并选择成本最低的那个实现。对于成本这块的度量标准，没有做过多研究，于是带着问题，对这块源码进行了一次梳理。</p><h2 id="源码剖析"><a href="#源码剖析" class="headerlink" title="源码剖析"></a>源码剖析</h2><p>为使博文简洁相关实现只贴部分核心代码，以下所指的Realization对应于构建好的Cube。</p><h4 id="查询入口"><a href="#查询入口" class="headerlink" title="查询入口"></a>查询入口</h4><ul><li>QueryService.doQueryWithCache()</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> //kylin.query.cache-enabled是否开启，如果开启将会从cache里面去读结果</span><br><span class="line"> if (queryCacheEnabled) &#123;</span><br><span class="line">sqlResponse = searchQueryInCache(sqlRequest);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> try &#123;</span><br><span class="line">if (null == sqlResponse) &#123;</span><br><span class="line">if (isSelect) &#123;</span><br><span class="line">//查询入口</span><br><span class="line">sqlResponse = query(sqlRequest);</span><br><span class="line">&#125; else if (kylinConfig.isPushDownEnabled() &amp;&amp; kylinConfig.isPushDownUpdateEnabled()) &#123;</span><br><span class="line">//如果开启了pushDown的话允许非查询的sql，如update</span><br><span class="line">sqlResponse = update(sqlRequest);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">logger.debug(&quot;Directly return exception as the sql is unsupported, and query pushdown is disabled&quot;);</span><br><span class="line">                       throw new BadRequestException(msg.getNOT_SUPPORTED_SQL());</span><br><span class="line">&#125;</span><br><span class="line"> ...</span><br><span class="line"> catch()&#123;</span><br><span class="line"> ...</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>这里，我们忽略从缓存中查找（searchQueryInCache），以及非select查询的情况，单单从一次正常的查询进行分析，进入query方法。</p><ul><li>QueryService.query()</li></ul><p>query方法相对来说比较简单，记录了query开始和结束的信息，相当于做了一个切面的工作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public SQLResponse query(SQLRequest sqlRequest) throws Exception &#123;</span><br><span class="line">    SQLResponse ret = null;</span><br><span class="line">    try &#123;</span><br><span class="line">        final String user = SecurityContextHolder.getContext().getAuthentication().getName();</span><br><span class="line">        badQueryDetector.queryStart(Thread.currentThread(), sqlRequest, user);</span><br><span class="line"></span><br><span class="line">        ret = queryWithSqlMassage(sqlRequest);</span><br><span class="line">        return ret;</span><br><span class="line"></span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        String badReason = (ret != null &amp;&amp; ret.isPushDown()) ? BadQueryEntry.ADJ_PUSHDOWN : null;</span><br><span class="line">        badQueryDetector.queryEnd(Thread.currentThread(), badReason);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中badQueryDetector是一个单起的线程，用来统计和监测bad query的。当有bad query时notify相关的观察者，做一些操作，如打印日志，记录bad query等。kylin 中很多事件的通知都是通过生产者消费者模式订阅发布的。继续进入queryWithSqlMessage()</p><ul><li>QueryService.queryWithSqlMessage()</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Connection conn = null;</span><br><span class="line">try &#123;</span><br><span class="line"> conn = QueryConnection.getConnection(sqlRequest.getProject());</span><br><span class="line"> ...</span><br><span class="line"> return execute(correctedSql, sqlRequest, conn);</span><br><span class="line"> ...</span><br><span class="line">   &#125; finally &#123;</span><br><span class="line">DBUtils.closeQuietly(conn);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>这个方法里首先获取了数据库连接，kylin的查询的中间层是基于Calcite的，接下来会看一下QueryConnection背后的逻辑。不过话说回来kylin这种整个大块的try catch异常捕获的机制某种意义上来说是种不负责任的表现。</p><ul><li>QueryConnection.getConnection():</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public static Connection getConnection(String project) throws SQLException &#123;</span><br><span class="line">    if (!isRegister) &#123;</span><br><span class="line">        DriverManager.registerDriver(new Driver());</span><br><span class="line">        isRegister = true;</span><br><span class="line">    &#125;</span><br><span class="line">    File olapTmp = OLAPSchemaFactory.createTempOLAPJson(project, KylinConfig.getInstanceFromEnv());</span><br><span class="line">    Properties info = new Properties();</span><br><span class="line">    info.put(&quot;model&quot;, olapTmp.getAbsolutePath());</span><br><span class="line">    return DriverManager.getConnection(&quot;jdbc:calcite:&quot;, info);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>方法比较简单，主要是通过OLAPSchemaFactory.createTempOLAPJson()生成了连接的元数据文件，用来创建连接</p><ul><li>OLAPSchemaFactory</li></ul><p>OLAPSchemaFactory 实现了calcite的 SchemaFactory接口，实现了create方法，用来创建连接时生成Schema</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public Schema create(SchemaPlus parentSchema, String schemaName, Map&lt;String, Object&gt; operand) &#123;</span><br><span class="line">    String project = (String) operand.get(SCHEMA_PROJECT);</span><br><span class="line">    Schema newSchema = new OLAPSchema(project, schemaName, false);</span><br><span class="line">    return newSchema;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在OLAPSchema的init方法中调用了KylinConfigBase.getStorageUrl方法，此方法返回了我们在配置文件中配置的kylin数据的存储信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public StorageURL getStorageUrl() &#123;</span><br><span class="line">        String url = getOptional(&quot;kylin.storage.url&quot;, &quot;default@hbase&quot;);</span><br><span class="line">        </span><br><span class="line">        // for backward compatibility</span><br><span class="line">        // 对2.0早期版本的配置做了兼容</span><br><span class="line">        if (&quot;hbase&quot;.equals(url))</span><br><span class="line">            url = &quot;default@hbase&quot;;</span><br><span class="line"></span><br><span class="line">        return StorageURL.valueOf(url);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里也可以看出kylin默认的存储系统是HBase</p><ul><li>QueryService.execute()</li></ul><p>从之前的QueryService.queryWithSqlMessage()方法继续往下深入到 execute()方法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ResultSet resultSet = null;</span><br><span class="line">if (isPrepareStatementWithParams(sqlRequest)) &#123;</span><br><span class="line">stat = conn.prepareStatement(correctedSql); // to be closed in the finally</span><br><span class="line">PreparedStatement prepared = (PreparedStatement) stat;</span><br><span class="line">processStatementAttr(prepared, sqlRequest);</span><br><span class="line">for (int i = 0; i &lt; ((PrepareSqlRequest) sqlRequest).getParams().length; i++) &#123;</span><br><span class="line">setParam(prepared, i + 1, ((PrepareSqlRequest) sqlRequest).getParams()[i]);</span><br><span class="line">&#125;</span><br><span class="line">resultSet = prepared.executeQuery();</span><br><span class="line">&#125; else &#123;</span><br><span class="line">stat = conn.createStatement();</span><br><span class="line">processStatementAttr(stat, sqlRequest);</span><br><span class="line">resultSet = stat.executeQuery(correctedSql);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>OLAPTable</li></ul><p>最后查出的结果是在resultSet里，追踪到这一步发现再往下追踪都是Calcite底层的逻辑了，kylin肯定是对Calcite 做了一定的扩展，并且将结果按照kylin预定义的规则做了各种聚合操作。Calcite文档中表示，可以实现三种类型的Table:</p><ul><li>a simple implementation of Table, using the ScannableTable interface, that enumerates all rows directly;</li><li>a more advanced implementation that implements FilterableTable, and can filter out rows according to simple predicates;</li><li>advanced implementation of Table, using TranslatableTable, that translates to relational operators using planner rules.</li></ul><p>发现在core-query模块中OLAPTable 实现了TranslatableTable。而OLAPTable 中实现的asQueryable方法有三种Enumerator的实现，这里默认选的是OLAP的实现。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public &lt;T&gt; Queryable&lt;T&gt; asQueryable(QueryProvider queryProvider, SchemaPlus schema, String tableName) &#123;</span><br><span class="line">       return new AbstractTableQueryable&lt;T&gt;(queryProvider, schema, this, tableName) &#123;</span><br><span class="line">            @SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">            public Enumerator&lt;T&gt; enumerator() &#123;</span><br><span class="line">                final OLAPQuery query = new OLAPQuery(EnumeratorTypeEnum.OLAP, 0);</span><br><span class="line">                return (Enumerator&lt;T&gt;) query.enumerator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">OLAPQuery.enumerator</span><br><span class="line">public Enumerator&lt;Object[]&gt; enumerator() &#123;</span><br><span class="line">        OLAPContext olapContext = OLAPContext.getThreadLocalContextById(contextId);</span><br><span class="line">        switch (type) &#123;</span><br><span class="line">        case OLAP:</span><br><span class="line">            return BackdoorToggles.getPrepareOnly() ? new EmptyEnumerator() : new OLAPEnumerator(olapContext, optiqContext);</span><br><span class="line">        case LOOKUP_TABLE:</span><br><span class="line">            return BackdoorToggles.getPrepareOnly() ? new EmptyEnumerator() : new LookupTableEnumerator(olapContext);</span><br><span class="line">        case HIVE:</span><br><span class="line">            return BackdoorToggles.getPrepareOnly() ? new EmptyEnumerator() : new HiveEnumerator(olapContext);</span><br><span class="line">        default:</span><br><span class="line">            throw new IllegalArgumentException(&quot;Wrong type &quot; + type + &quot;!&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>OLAPEnumerator.queryStorage()</li></ul><p>由OLAPTable.asQueryable进入，到了OLAPEnumerator.queryStorage()，终于能看到真实的查库操作了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">private ITupleIterator queryStorage() &#123;</span><br><span class="line">    logger.debug(&quot;query storage...&quot;);</span><br><span class="line"></span><br><span class="line">    // bind dynamic variables</span><br><span class="line">    bindVariable(olapContext.filter);</span><br><span class="line"></span><br><span class="line">    olapContext.resetSQLDigest();</span><br><span class="line">    SQLDigest sqlDigest = olapContext.getSQLDigest();</span><br><span class="line"></span><br><span class="line">    // query storage engine</span><br><span class="line">    IStorageQuery storageEngine = StorageFactory.createQuery(olapContext.realization);</span><br><span class="line">    ITupleIterator iterator = storageEngine.search(olapContext.storageContext, sqlDigest, olapContext.returnTupleInfo);</span><br><span class="line">    if (logger.isDebugEnabled()) &#123;</span><br><span class="line">        logger.debug(&quot;return TupleIterator...&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return iterator;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里StorageEngine 由StorageFactory创建，且有三种不同的实现，默认还是HBase</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">   private static ThreadLocal&lt;ImplementationSwitch&lt;IStorage&gt;&gt; storages = new ThreadLocal&lt;&gt;();</span><br><span class="line"></span><br><span class="line">   public static IStorage storage(IStorageAware aware) &#123;</span><br><span class="line">       ImplementationSwitch&lt;IStorage&gt; current = storages.get();</span><br><span class="line">       if (storages.get() == null) &#123;</span><br><span class="line">           current = new ImplementationSwitch&lt;&gt;(KylinConfig.getInstanceFromEnv().getStorageEngines(), IStorage.class);</span><br><span class="line">           storages.set(current);</span><br><span class="line">       &#125;</span><br><span class="line">       return current.get(aware.getStorageType());</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">  //KylinConfig.getInstanceFromEnv().getStorageEngines()</span><br><span class="line">public Map&lt;Integer, String&gt; getStorageEngines() &#123;</span><br><span class="line">       Map&lt;Integer, String&gt; r = Maps.newLinkedHashMap();</span><br><span class="line">       // ref constants in IStorageAware</span><br><span class="line">       r.put(0, &quot;org.apache.kylin.storage.hbase.HBaseStorage&quot;);</span><br><span class="line">       r.put(1, &quot;org.apache.kylin.storage.hybrid.HybridStorage&quot;);</span><br><span class="line">       r.put(2, &quot;org.apache.kylin.storage.hbase.HBaseStorage&quot;);</span><br><span class="line">       r.putAll(convertKeyToInteger(getPropertiesByPrefix(&quot;kylin.storage.provider.&quot;)));</span><br><span class="line">       return r;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><ul><li>OLAPTableScan.register()</li></ul><p>由于OLAPTable实现了TranslatableTable，它会通过一系列的relation operators将结果聚合,relation operators的注册逻辑在OLAPTableScan中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">   public void register(RelOptPlanner planner) &#123;</span><br><span class="line">       // force clear the query context before traversal relational operators</span><br><span class="line">       OLAPContext.clearThreadLocalContexts();</span><br><span class="line"></span><br><span class="line">       // register OLAP rules</span><br><span class="line">       planner.addRule(OLAPToEnumerableConverterRule.INSTANCE);</span><br><span class="line">       planner.addRule(OLAPFilterRule.INSTANCE);</span><br><span class="line">       planner.addRule(OLAPProjectRule.INSTANCE);</span><br><span class="line">       planner.addRule(OLAPAggregateRule.INSTANCE);</span><br><span class="line">       planner.addRule(OLAPJoinRule.INSTANCE);</span><br><span class="line">       planner.addRule(OLAPLimitRule.INSTANCE);</span><br><span class="line">       planner.addRule(OLAPSortRule.INSTANCE);</span><br><span class="line">       planner.addRule(OLAPUnionRule.INSTANCE);</span><br><span class="line">       planner.addRule(OLAPWindowRule.INSTANCE);</span><br><span class="line">       ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>这里着重看OLAPToEnumerableConverterRule 里返回的 OLAPToEnumerableConverter的实现，它是解释我在前言里提到的问题的关键。</p><ul><li>OLAPToEnumerableConverter.implement()</li></ul><p>这里面有对所有满足query条件的realization选择的实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public Result implement(EnumerableRelImplementor enumImplementor, Prefer pref) &#123;</span><br><span class="line">   </span><br><span class="line">  ...</span><br><span class="line">     // identify model &amp; realization</span><br><span class="line">     List&lt;OLAPContext&gt; contexts = listContextsHavingScan();</span><br><span class="line"></span><br><span class="line">     // intercept query</span><br><span class="line">     List&lt;QueryInterceptor&gt; intercepts = QueryInterceptorUtil.getQueryInterceptors();</span><br><span class="line">     for (QueryInterceptor intercept : intercepts) &#123;</span><br><span class="line">         intercept.intercept(contexts);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">//RealizationChooser 中有对Realization选择的具体实现</span><br><span class="line">     RealizationChooser.selectRealization(contexts);</span><br><span class="line">     ...</span><br><span class="line"></span><br><span class="line">return impl.visitChild(this, 0, inputAsEnum, pref);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><ul><li>RealizationChooser.attemptSelectRealization()</li></ul><p>attemptSelectRealization方法里面主要干了两件事： 1）拉取属于该project与factTableName下的所有Realization，经过一系列的条件过滤掉不符合query的Realization，并将符合条件的Realization按照RealizationCost排序。2）对第一步收集的Realization map，调用QueryRouter.selectRealization()，一旦QueryRouter.selectRealization()有返回值立即中断循环返回最终选择的Realization</p><ul><li>RealizationChooser.makeOrderedModelMap() 部分的实现逻辑如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">  //按条件过滤realization</span><br><span class="line">   for (IRealization real : realizations) &#123;</span><br><span class="line">       //过滤disabled cube</span><br><span class="line">       if (real.isReady() == false) &#123;</span><br><span class="line">           context.realizationCheck.addIncapableCube(real,</span><br><span class="line">                   RealizationCheck.IncapableReason.create(RealizationCheck.IncapableType.CUBE_NOT_READY));</span><br><span class="line">           continue;</span><br><span class="line">       &#125;</span><br><span class="line">       //过滤不包含querycontext里面全部的columns</span><br><span class="line">       if (containsAll(real.getAllColumnDescs(), first.allColumns) == false) &#123;</span><br><span class="line">           context.realizationCheck.addIncapableCube(real, RealizationCheck.IncapableReason</span><br><span class="line">                   .notContainAllColumn(notContain(real.getAllColumnDescs(), first.allColumns)));</span><br><span class="line">           continue;</span><br><span class="line">       &#125;</span><br><span class="line">       ／／过滤存在黑名单里面的cube</span><br><span class="line">       if (RemoveBlackoutRealizationsRule.accept(real) == false) &#123;</span><br><span class="line">           context.realizationCheck.addIncapableCube(real, RealizationCheck.IncapableReason</span><br><span class="line">                   .create(RealizationCheck.IncapableType.CUBE_BLACK_OUT_REALIZATION));</span><br><span class="line">           continue;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">//过滤完，按RealizationCost排序</span><br><span class="line">       RealizationCost cost = new RealizationCost(real);</span><br><span class="line">       DataModelDesc m = real.getModel();</span><br><span class="line">       Set&lt;IRealization&gt; set = models.get(m);</span><br><span class="line">       if (set == null) &#123;</span><br><span class="line">           set = Sets.newHashSet();</span><br><span class="line">           set.add(real);</span><br><span class="line">           models.put(m, set);</span><br><span class="line">           costs.put(m, cost);</span><br><span class="line">       &#125; else &#123;</span><br><span class="line">           set.add(real);</span><br><span class="line">           RealizationCost curCost = costs.get(m);</span><br><span class="line">           if (cost.compareTo(curCost) &lt; 0)</span><br><span class="line">               costs.put(m, cost);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>重点就在RealizationCost的实现里了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public RealizationCost(IRealization real) &#123;</span><br><span class="line">          // ref Candidate.PRIORITIES</span><br><span class="line">          this.priority = Candidate.PRIORITIES.get(real.getType());</span><br><span class="line"></span><br><span class="line">          // ref CubeInstance.getCost()</span><br><span class="line">          int c = real.getAllDimensions().size() * CubeInstance.COST_WEIGHT_DIMENSION</span><br><span class="line">                  + real.getMeasures().size() * CubeInstance.COST_WEIGHT_MEASURE;</span><br><span class="line">          for (JoinTableDesc join : real.getModel().getJoinTables()) &#123;</span><br><span class="line">              if (join.getJoin().isInnerJoin())</span><br><span class="line">                  c += CubeInstance.COST_WEIGHT_INNER_JOIN;</span><br><span class="line">          &#125;</span><br><span class="line">          this.cost = c;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p>到此，对于kylin的Realization的成本计算规则清楚了。就是对dimension，measure,jointable三个维度的数量进行加权求和，得到的就是每个Realization对应的成本。相对的，每个维度对应的权重是有所斟酌的，dimension对应的是10，measure为1（考虑到是预计算的结果），jointable为100。从这也能看出建模时应该考虑的优化方向：避免过多的dimension，以及jointable的操作，结果尽量从预计算中出。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这次经过对kylin query源码的分析，基本上对kylin的核心代码都过了一遍，学习了不少优秀的代码解耦方式，也对底层原理加深了理解。关于RealizationCost的实现，目前kylin实现比较简单，在遍历所有满足条件的实现时找到Realization便返回处理的有些过于仓促。对于其是map的结构或许kylin在之后的扩展方面也是有所考虑。目前我们还不打算扩展Realizaiton的选择策略，了解了源码就可以在建模层面将查询结果不如意的情况给规避了。</p>]]></content>
      
      
        <tags>
            
            <tag> kylin - Java - 源码 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>superset customization</title>
      <link href="/2017/10/13/superset-customization/"/>
      <url>/2017/10/13/superset-customization/</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li><p>由于数据组目前重度依赖kylin，然而kylin并没有官方开源的数据可视化工具。所幸kylin提供了丰富的查询API供我们直接传入SQL进行查询，与此同时发现superset有非官方对接kylin的开源插件，虽然两年没有维护了，对代码进行了部分重构也就成功将superset和kylin对接起来了。</p></li><li><p>通常一个开源框架在使用过程中，总是会有各种各样的针对具体场景的定制化需求，superset自然不例外。于是下面简单记录一下superset定制化的过程中干的一些事情，算是做一个小的总结。</p></li></ul><h3 id="python2升级python3"><a href="#python2升级python3" class="headerlink" title="python2升级python3"></a>python2升级python3</h3><p>虽然python2.7是superset官方推荐的版本，但是由于python2.7默认的Encoding的问题导致使用过程中一旦出现中文就出错。为了避免这种情况，我选择将python2.7升级到python3.5。当然升级还算顺利，碰到的最大的坑就是python2中的MysqlDB模块在python3中已经不维护了，于是修改superset源码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#init.py</span><br><span class="line">#pip install pymysql</span><br><span class="line"></span><br><span class="line">import pymysql</span><br><span class="line">pymysql.install_as_MySQLdb()</span><br></pre></td></tr></table></figure><h3 id="superset-ldap配置"><a href="#superset-ldap配置" class="headerlink" title="superset ldap配置"></a>superset ldap配置</h3><ul><li><p>基本上各大开源组件都支持ldap的配置，superset也不例外。ldap的好处就是给企业用户提供统一的账户授权，方便权限管理。</p></li><li><p>由于google上基本上查不到相关的资料或者都是配置出错了没法解决的提问，这一块踩了不少的坑。然后发现superset其实是直接用的flask-appbuilder里提供的security组件支持ldap的，通过熟悉ldap的原理以及查看flask-appbuilder的官方文档和security组件的源代码最终解决了ldap的配置问题。</p></li><li><p>先在config.py中引入flask-appbuilder的相关依赖</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#引入引用：</span><br><span class="line">from flask_appbuilder.security.manager import AUTH_OID,</span><br><span class="line">AUTH_REMOTE_USER,</span><br><span class="line">AUTH_DB, AUTH_LDAP,</span><br><span class="line">AUTH_OAUTH,</span><br><span class="line">AUTH_OAUTH</span><br></pre></td></tr></table></figure></li><li><p>再在config.py AUTH一块的区域中加入如下配置</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//最终配置（这里用到二次验证，即bind给定账户，然后根据权限去搜索输入的用户，拉取用户的dn，最后用得到的密码和拉取到的dn去bind_user即可）</span><br><span class="line">#2表示使用ldap</span><br><span class="line">AUTH_TYPE = 2 </span><br><span class="line">#自己公司的ldap服务器</span><br><span class="line">AUTH_LDAP_SERVER = &quot;ldap://ldap.example.com&quot; </span><br><span class="line">#ldap有两种认证方式，这里用到二次验证，需要配置一个admin账号</span><br><span class="line">AUTH_LDAP_BIND_USER = &quot;cn=admin,dc=ldap,dc=mobvoi,dc=com&quot;</span><br><span class="line">#admin账号的密码</span><br><span class="line">AUTH_LDAP_BIND_PASSWORD = &quot;xxxx&quot;</span><br><span class="line">AUTH_LDAP_USE_TLS = False</span><br><span class="line">#在哪个节点查找用户</span><br><span class="line">AUTH_LDAP_SEARCH = &quot;ou=users,dc=ldap,dc=mobvoi,dc=com&quot;</span><br><span class="line">#查找用户的字段</span><br><span class="line">AUTH_LDAP_UID_FIELD = &quot;uid&quot;</span><br><span class="line">#是否允许superset数据库中没有记录的ldap用户自动注册</span><br><span class="line">AUTH_USER_REGISTRATION = True</span><br><span class="line">#注册新用户时默认分配的权限</span><br><span class="line">AUTH_USER_REGISTRATION_ROLE = &quot;Dashboard Readonly&quot;</span><br></pre></td></tr></table></figure></li></ul><h3 id="superset-docker化"><a href="#superset-docker化" class="headerlink" title="superset docker化"></a>superset docker化</h3><p>为了减轻运维服务器升级时服务的迁移成本，决定将统一修改的源码托管起来，每一次修改重新发布docker，这样能大大降低运维成本。</p><ul><li><p>拉取ubuntu镜像，挂载本地磁盘，并进入后台运行模式</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it -v /home/:/home/ leemiracle/unbuntu /bin/bash</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>在ubuntu中安装python3.5</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://www.python.org/ftp/python/3.5.4/Python-3.5.4rc1.tgz</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>拖下superset、pylin源代码，安装pyklin 与ldap</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clonet git@github.com:lichaojacobs/superset.git</span><br><span class="line">git clone git@github.com:lichaojacobs/pykylin.git</span><br><span class="line">cd pykylin</span><br><span class="line">pip install -r ./requirements.txt</span><br><span class="line">  python setup.py install</span><br><span class="line">  pip install pyldap</span><br></pre></td></tr></table></figure></li><li><p>修改flask-appbuilder代码（重写了权限部分逻辑）</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">flask-appbuilder/security/manager.py  auth_user_ldap (method)</span><br><span class="line">try:</span><br><span class="line">    user_db = self.auth_user_db(username,password)</span><br><span class="line">    if user_db!=None:</span><br><span class="line">       return user_db</span><br><span class="line"> except Exception:</span><br><span class="line">    log.info(&quot;using ldap and first try db_auth failed&quot;)</span><br></pre></td></tr></table></figure></li><li><p>修改superset源码（增加status接口，主要是对服务状态进行监测）</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from flask import jsonify</span><br><span class="line"> class MyIndexView(IndexView):</span><br><span class="line">   @expose(&apos;/&apos;)</span><br><span class="line">   def index(self):</span><br><span class="line">       return redirect(&apos;/superset/welcome&apos;)</span><br><span class="line">   @expose(&apos;/status&apos;)</span><br><span class="line">   def status(self):</span><br><span class="line">       return jsonify(status=&apos;ok&apos;)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>修改superset config.py，加入缓存配置，ldap配置等</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">加上缓存：</span><br><span class="line"> CACHE_DEFAULT_TIMEOUT = 7200</span><br><span class="line"> CACHE_CONFIG = &#123;</span><br><span class="line">   &apos;CACHE_TYPE&apos;:&apos;redis&apos;,</span><br><span class="line">   &apos;CACHE_DEFAULT_TIMEOUT&apos;:7200,</span><br><span class="line">   &apos;CACHE_KEY_PREFIX&apos;:&apos;superset_&apos;,</span><br><span class="line">   &apos;CACHE_REDIS_HOST&apos;:&apos;xxxxxx.aliyuncs.com&apos;,</span><br><span class="line">   &apos;CACHE_REDIS_PORT&apos;:6379,</span><br><span class="line">   &apos;CACHE_REDIS_DB&apos;:&apos;&apos;,</span><br><span class="line">   &apos;CACHE_REDIS_PASSWORD&apos;:&apos;xxxxx&apos;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>加上GA脚本用于统计页面PV UV</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line"> (function(i,s,o,g,r,a,m)&#123;i[&apos;GoogleAnalyticsObject&apos;]=r;i[r]=i[r]||function()&#123;</span><br><span class="line"> (i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),</span><br><span class="line"> m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)</span><br><span class="line"> &#125;)(window,document,&apos;script&apos;,&apos;https://www.google-analytics.com/analytics.js&apos;,&apos;ga&apos;);</span><br><span class="line"></span><br><span class="line"> &#123;% if g.user.get_full_name %&#125;</span><br><span class="line"> ga(&apos;set&apos;, &apos;userId&apos;, &apos;&#123;&#123;g.user.get_full_name()&#125;&#125;&apos;);</span><br><span class="line"> &#123;% endif %&#125;</span><br><span class="line"> ga(&apos;create&apos;, &apos;UA-64695573-19&apos;, &apos;auto&apos;);</span><br><span class="line"> ga(&apos;send&apos;, &apos;pageview&apos;);</span><br><span class="line"></span><br><span class="line"> &lt;/script&gt;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>自编译superset</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd $SUPERSET_HOME/superset/assets</span><br><span class="line">./js_build.sh</span><br><span class="line">#由于实际编译过程中出现了各种测试异常，我将npm run test,npm run cover去掉了</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>安装superset</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apt-get install build-essential libssl-dev libffi-dev python-dev python-pip libsasl2-dev libldap2-dev</span><br><span class="line">  apt-get install python3.5-dev</span><br><span class="line">pip install --upgrade setuptools pip</span><br><span class="line">#直接运行上一步编译好的文件</span><br><span class="line">python superset/setup.py install</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>打包镜像，重新commit修改，并打上tag</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker commit -m &quot;superset docker init&quot; -a &quot;author&quot; &lt;container id&gt; &lt;docker repository host&gt;/superset:&lt;version&gt;</span><br></pre></td></tr></table></figure></li><li><p>运行docker</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -m 1G --net=host &lt;docker repository host&gt;/superset:&lt;version&gt; superset runserver -p 8088 -t 500</span><br></pre></td></tr></table></figure></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>虽然目前步子迈得有点小，但好在还在进步。在自己努力下，从之前痛苦的数据架构升级到现在，数据组的基础架构平台也算是稳定的在提供服务了。kylin也在啃了很久的源代码，做了一些优化之后从之前每天要挂三四次，数据主从同步问题每次执行任务都要重现到现在也能平稳运行。路还很长，想往底层钻的深一点，继续加油吧～</p>]]></content>
      
      
        <tags>
            
            <tag> superset </tag>
            
            <tag> 二次开发 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>lombok builder 泛型擦除</title>
      <link href="/2017/09/23/lombok-builder-%E6%B3%9B%E5%9E%8B%E6%93%A6%E9%99%A4%E9%97%AE%E9%A2%98/"/>
      <url>/2017/09/23/lombok-builder-%E6%B3%9B%E5%9E%8B%E6%93%A6%E9%99%A4%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li><p>众所周知，Java长期以来比较遭业界嫌弃的是太笨重，代码冗余过大。然而依托于Java庞大健全的开源社区，这些缺点正在逐渐改善。Java 8 引进的lambda以及函数式编程的思想让我们的代码越来越简洁。lombok等各大开源神器让我们的冗余代码越来越少。</p></li><li><p>使用lombok已经很长时间了，一直很好用，然而最近发现使用lombok builder构造泛型的时候会出现泛型擦除的情况，导致得到的对象是Object类型</p></li></ul><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><ul><li>定义一个待使用builder构造的泛型类</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@NoArgsConstructor</span><br><span class="line">@AllArgsConstructor</span><br><span class="line">@Builder</span><br><span class="line">public class PageableResponse&lt;T&gt; &#123;</span><br><span class="line"></span><br><span class="line">  List&lt;T&gt; results;</span><br><span class="line">  @SerializedName(&quot;total_pages&quot;)</span><br><span class="line">  long totalPages;</span><br><span class="line">  @SerializedName(&quot;total_elements&quot;)</span><br><span class="line">  long totalElements;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>正常情况下，我是可以这样使用的:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public PageableResponse&lt;String&gt; getPageableResponse() &#123;</span><br><span class="line">  return PageableResponse</span><br><span class="line">      .builder()</span><br><span class="line">      .results(Lists.newArrayList())</span><br><span class="line">      .totalElements(0)</span><br><span class="line">      .totalPages(0)</span><br><span class="line">      .build();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然而出现类型转换错误，PageableResponse.builder()生成的是PageableResponse.PageableResponseBuilder<code>&lt;Object&gt;</code> 想来也是，我这里调用builder方法时候并没有指定任何类型。</p><h3 id="泛型基础知识"><a href="#泛型基础知识" class="headerlink" title="泛型基础知识"></a>泛型基础知识</h3><p>那为什么会得到这样的结果？这里就要回顾一下泛型的知识了，这里我们的场景比较复杂一点，属于泛型类里面的静态泛型方法。这里提一下知识点：</p><h5 id="java泛型转换的几个事实："><a href="#java泛型转换的几个事实：" class="headerlink" title="java泛型转换的几个事实："></a>java泛型转换的几个事实：</h5><ul><li>虚拟机中没有泛型，只有普通的类和方法</li><li>无论何时定义一个泛型类型，都自动提供了一个相应的原始类型。原始类型的名字就是删去类型参数后的泛型类型名。擦除类型变量，替换为限定类型（无限定的变量用Object）。当程序调用泛型方法时，如果擦除返回类型，编译器插入强制类型转换</li><li>所有的类型参数都用它们的限定类型替换 如:Pair<code>&lt;T&gt;</code> 擦除类型之后就变成了Pair<code>&lt;Object&gt;</code></li><li>桥方法被合成来保持多态</li><li>为保证类型安全性，必要时插入强制类型转换</li></ul><h5 id="泛型的约束与局限性"><a href="#泛型的约束与局限性" class="headerlink" title="泛型的约束与局限性"></a>泛型的约束与局限性</h5><ul><li><p>不能用基本类型实例化类型参数</p></li><li><p>运行时类型查询只适用于原始类型:</p><ul><li>if(a instanceof Pair<code>&lt;String&gt;</code>)//ERROR</li><li><p>同理getClass方法也总是返回原始类型:</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Pair&lt;String&gt; stringPair=...</span><br><span class="line">Pair&lt;Employ&gt; employPair=...</span><br><span class="line">if(stringPair.getClass()==employPair.getClass()) //True</span><br><span class="line">//两次调用都将返回Pair.class</span><br></pre></td></tr></table></figure></li></ul></li><li><p>不能创建参数化类型数组</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Pair&lt;String&gt;[] table = new Pair&lt;String&gt;[10]//ERROR</span><br><span class="line">//类型擦除之后，table的类型是Pair[]。可以把它转换为Object[]</span><br><span class="line">Object[] arr = table</span><br><span class="line">//数组会记住它的元素类型，如果试图存储其他类型的元素，会抛出ArrayStoreException异常</span><br><span class="line">//只是不允许创建这些数组，但是声明类型为Pair&lt;String&gt;[]的变量仍是合法的，只是不能用new Pair&lt;String&gt;[10]初始化这个变量</span><br><span class="line">//可以声明通配类型的数组，然后进行类型转换，导致的结果将是不安全的</span><br><span class="line">Pair&lt;String&gt;[] table = (Pair&lt;String&gt;[]) new Pair&lt;?&gt;[10];</span><br></pre></td></tr></table></figure></li><li><p>不能实例化类型变量: 不能new T(….)</p><ul><li>类型擦除会将T改变成Object，而且，本意肯定不希望调用new Object()</li><li><p>可以通过反射调用Class.newInstance方法来构造泛型对象，细节有点复杂</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//不能以下方式调用</span><br><span class="line">first = T.class.newInstance();//ERROR</span><br><span class="line">//表达式T.class是不合法的，必须如下方式才可以支配Class对象：</span><br><span class="line"></span><br><span class="line">public static&lt;T&gt; Pair&lt;T&gt; makePair(Class&lt;T&gt; cl)&#123;</span><br><span class="line">   try &#123; return new Pair&lt;&gt;(cl.newInstance(),cl.newInstance()) &#125;</span><br><span class="line">   catch (Exception ex) &#123; return null; &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Pair&lt;String&gt; p = Pair.makePair(String.class);</span><br><span class="line">//注意，Class类本身就是泛型，String.class是一个Class&lt;String&gt;的实例（唯一实例）。因此makePair方法能够判断出pair的类型。</span><br></pre></td></tr></table></figure></li></ul></li><li><p>不能以如下的方式构造泛型数组</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//类型擦除会让这个方法永远构造Object[2]数组，而要求extends Comparable，这显然会抛出转换异常</span><br><span class="line">// 此时可以把 extends Comparable去掉，这样能保证强制转换的正确性</span><br><span class="line">public static &lt;T extends Comparable&gt; T[] minmax(T... a) &#123;</span><br><span class="line"></span><br><span class="line">   Object[] mm = new Object[2];</span><br><span class="line">   mm[0] = a[0];</span><br><span class="line">   mm[1] = a[1];</span><br><span class="line"></span><br><span class="line">   return (T[]) mm;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> 或者使用反射，调用Array.newInstance:</span><br><span class="line"></span><br><span class="line"> public static &lt;T extends Comparable&gt; T[] minmax(T... a) &#123;</span><br><span class="line"></span><br><span class="line">   T[] mm = (T[]) Array.newInstance(a.getClass().getComponentType(), 2);</span><br><span class="line">   ...</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ul><h5 id="泛型类里面的泛型方法和静态泛型方法是有区别的："><a href="#泛型类里面的泛型方法和静态泛型方法是有区别的：" class="headerlink" title="泛型类里面的泛型方法和静态泛型方法是有区别的："></a>泛型类里面的泛型方法和静态泛型方法是有区别的：</h5><ul><li>泛型类定义的泛型 在整个类中有效 如果被方法使用，当泛型类确定类型之后，泛型方法也就确定类型了</li><li>而对于静态泛型方法而言，其泛型的类型是不依赖于泛型类的类型，也就是这两者的类型完全不相干（依据上一条的事实一）</li></ul><h5 id="泛型类型的继承规则"><a href="#泛型类型的继承规则" class="headerlink" title="泛型类型的继承规则"></a>泛型类型的继承规则</h5><ul><li>考虑一个类和一个子类，如Employee和Manager。Pair<code>&lt;Manager&gt;</code> 却不是Pair<code>&lt;Employee&gt;</code>的子类。事实上，它们的关系如下图所示</li></ul><p><img src="http://jacobs.wanhb.cn/images/generics1.png" alt=""></p><ul><li>泛型类可以扩展或实现其他泛型类。这一点与普通的类没什么区别。如：ArrayList<code>&lt;T&gt;</code> 类实现List<code>&lt;T&gt;</code>接口，意味着一个ArrayList<code>&lt;Manager&gt;</code> 可以被转换为一个List<code>&lt;Manager&gt;</code>。但是一个ArrayList<code>&lt;Manager&gt;</code> 不是一个ArrayList<code>&lt;Employee&gt;</code> 或者List<code>&lt;Employee&gt;</code>，它们的关系如下图:</li></ul><p><img src="http://jacobs.wanhb.cn/images/generics2.png" alt=""></p><h3 id="案例解释"><a href="#案例解释" class="headerlink" title="案例解释"></a>案例解释</h3><ul><li>泛型知识作了一番复习之后，我们再回到上面的案例。之所以无法正常的使用builder，是因为静态泛型方法不继承泛型类的类型。下面是案例类生成的代码：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public class PageableResponse&lt;T&gt; &#123;</span><br><span class="line">  List&lt;T&gt; results;</span><br><span class="line">  @JSONField(</span><br><span class="line">    name = &quot;total_pages&quot;</span><br><span class="line">  )</span><br><span class="line">  long totalPages;</span><br><span class="line">  @JSONField(</span><br><span class="line">    name = &quot;total_elements&quot;</span><br><span class="line">  )</span><br><span class="line">  long totalElements;</span><br><span class="line"></span><br><span class="line">  //这个T不依赖于PageableResponse&lt;T&gt;的T</span><br><span class="line">  public static &lt;T&gt; PageableResponse.PageableResponseBuilder&lt;T&gt; builder() &#123;</span><br><span class="line">    return new PageableResponse.PageableResponseBuilder();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li>而正常的没有泛型的生成代码如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public class Address &#123;</span><br><span class="line">  private String city;</span><br><span class="line"></span><br><span class="line">  //不带泛型</span><br><span class="line">  public static Address.AddressBuilder builder() &#123;</span><br><span class="line">    return new Address.AddressBuilder();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public Address() &#123;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  @ConstructorProperties(&#123;&quot;city&quot;&#125;)</span><br><span class="line">  public Address(String city) &#123;</span><br><span class="line">    this.city = city;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static class AddressBuilder &#123;</span><br><span class="line">    private String city;</span><br><span class="line"></span><br><span class="line">    AddressBuilder() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Address.AddressBuilder city(String city) &#123;</span><br><span class="line">      this.city = city;</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Address build() &#123;</span><br><span class="line">      return new Address(this.city);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String toString() &#123;</span><br><span class="line">      return &quot;Address.AddressBuilder(city=&quot; + this.city + &quot;)&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>所以如果必须要使用泛型且如果这种情况不是太多的话，可以自己实现lombok的builder生成的代码，以案例定义的类为例，改造完的代码如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">@NoArgsConstructor</span><br><span class="line">@AllArgsConstructor</span><br><span class="line">public class PageableResponse&lt;T&gt; &#123;</span><br><span class="line"></span><br><span class="line">  List&lt;T&gt; results;</span><br><span class="line">  @SerializedName(&quot;total_pages&quot;)</span><br><span class="line">  long totalPages;</span><br><span class="line">  @SerializedName(&quot;total_elements&quot;)</span><br><span class="line">  long totalElements;</span><br><span class="line"></span><br><span class="line">  //这里使用非静态方法的写法可以达到继承泛型类型的目的</span><br><span class="line">  public PageableResponseBuilder&lt;T&gt; builder() &#123;</span><br><span class="line">    return new PageableResponseBuilder&lt;T&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public class PageableResponseBuilder&lt;T&gt; &#123;</span><br><span class="line"></span><br><span class="line">    List&lt;T&gt; results;</span><br><span class="line">    long totalPages;</span><br><span class="line">    long totalElements;</span><br><span class="line"></span><br><span class="line">    public PageableResponseBuilder&lt;T&gt; totalPages(long totalPages) &#123;</span><br><span class="line">      this.totalPages = totalPages;</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public PageableResponseBuilder&lt;T&gt; totalElements(long totalElements) &#123;</span><br><span class="line">      this.totalElements = totalElements;</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public PageableResponseBuilder&lt;T&gt; results(List&lt;T&gt; results) &#123;</span><br><span class="line">      this.results = results;</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public PageableResponse&lt;T&gt; build() &#123;</span><br><span class="line">      return new PageableResponse&lt;&gt;(results, totalPages, totalElements);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> 成长 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kylin二次开发——测试环境搭建</title>
      <link href="/2017/08/07/Kylin%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91%E2%80%94%E2%80%94%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2017/08/07/Kylin%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91%E2%80%94%E2%80%94%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<h3 id="调研背景"><a href="#调研背景" class="headerlink" title="调研背景"></a>调研背景</h3><p>虽然公司目前在生产环境上正式用上了kylin，但是由于其本身年龄不长，社区并不完善，难难免会暴露出各种各样的源码级别的问题(包括上一篇介绍的kylin的同步机制的问题)。这时候使用者想等着官方推出新的release未免太过于被动。于是，我们想着对kylin进行二次开发以满足我们对定制化需求。事实上，目前我们使用的所有开源框架在一定程度上都进行了多多少少的二次开发：</p><ul><li>superset接入kylin，完成了自编译集成进了docker，并修改了 flask-appbuilder的源码逻辑，兼容ldap与原本的账号系统。</li><li>airflow 正在考虑从输出信息中判断task是否执行成功，而不是单纯的靠进程是否异常退出判断（主要考虑到支持kylin的任务调度）</li></ul><p>其实在kylin的官网对于开发环境搭建大致的步骤都做了介绍下面描述整个过程</p><h3 id="搭建过程"><a href="#搭建过程" class="headerlink" title="搭建过程"></a>搭建过程</h3><ul><li><p>想应用经过自己二次开发的kylin当然必须得全覆盖的跑一遍所有的单元测试。这就意味着必须得有Hadoop+hive+hbase等一整套测试环境。这里使用kylin官方推荐的Hortonworks Sandbox。为了方便，直接使用Sandbox on docker。</p><ul><li><a href="https://hortonworks.com/tutorial/sandbox-deployment-and-install-guide/section/3/" target="_blank" rel="noopener">按照官网的教程（docker占用的内存至少8G以上，否则运行不了）</a></li><li>在执行start_sandbox-hdp.sh的时候需要往映射的端口里面加入hive metastore的thrift端口9083，否则本地跑单元测试的时候连不上metastore</li><li><p>ssh上运行的container，修改admin密码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh -p 2222 root@localhost</span><br><span class="line">或者http://127.0.0.1:4200/ 进入shell浏览器界面</span><br><span class="line">//执行</span><br><span class="line">  ambari-admin-password-reset</span><br></pre></td></tr></table></figure></li></ul></li><li><p>用修改的admin密码登录<a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a>，确保dashboard中hive+mapreduce+hdfs+hbase正常启动</p></li><li><p>修改kylin.properties的几个值:</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//KYLIN_HOME/examples/test_case_data/sandbox/kylin.properties</span><br><span class="line"></span><br><span class="line">kylin.job.use-remote-cli=true</span><br><span class="line">kylin.job.remote-cli-hostname=sandbox</span><br><span class="line">kylin.job.remote-cli-username=root</span><br><span class="line">kylin.job.remote-cli-password=xxxx</span><br><span class="line">//这个默认是22端口，由于我本地不生效，就直接设置为2222端口</span><br><span class="line">kylin.job.remote-cli-port=2222</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>如果单个单元进行测试，不想每次从头开始，方便集中debug某个moudle的错误，可以注释掉pom.xml中的check-style插件</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- &lt;plugin&gt;</span><br><span class="line">                  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                  &lt;artifactId&gt;maven-checkstyle-plugin&lt;/artifactId&gt;</span><br><span class="line">                  &lt;version&gt;2.17&lt;/version&gt;</span><br><span class="line">                  &lt;dependencies&gt;</span><br><span class="line">                      &lt;dependency&gt;</span><br><span class="line">                          &lt;groupId&gt;com.puppycrawl.tools&lt;/groupId&gt;</span><br><span class="line">                          &lt;artifactId&gt;checkstyle&lt;/artifactId&gt;</span><br><span class="line">                          &lt;version&gt;6.19&lt;/version&gt;</span><br><span class="line">                      &lt;/dependency&gt;</span><br><span class="line">                  &lt;/dependencies&gt;</span><br><span class="line">                  &lt;executions&gt;</span><br><span class="line">                      &lt;execution&gt;</span><br><span class="line">                          &lt;id&gt;check-style&lt;/id&gt;</span><br><span class="line">                          &lt;phase&gt;validate&lt;/phase&gt;</span><br><span class="line">                          &lt;configuration&gt;</span><br><span class="line">                              &lt;configLocation&gt;dev-support/checkstyle.xml&lt;/configLocation&gt;</span><br><span class="line">                              &lt;suppressionsLocation&gt;dev-support/checkstyle-suppressions.xml&lt;/suppressionsLocation&gt;</span><br><span class="line">                              &lt;includeTestSourceDirectory&gt;true&lt;/includeTestSourceDirectory&gt;</span><br><span class="line">                              &lt;consoleOutput&gt;true&lt;/consoleOutput&gt;</span><br><span class="line">                              &lt;failsOnError&gt;true&lt;/failsOnError&gt;</span><br><span class="line">                          &lt;/configuration&gt;</span><br><span class="line">                          &lt;goals&gt;</span><br><span class="line">                              &lt;goal&gt;check&lt;/goal&gt;</span><br><span class="line">                          &lt;/goals&gt;</span><br><span class="line">                      &lt;/execution&gt;</span><br><span class="line">                  &lt;/executions&gt;</span><br><span class="line">              &lt;/plugin&gt; --&gt;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>执行测试命令</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//base</span><br><span class="line">mvn test -fae -Dhdp.version=$&#123;HDP_VERSION:-&quot;2.6.1.0-129&quot;&#125; -P sandbox -X</span><br><span class="line">//全覆盖</span><br><span class="line">mvn verify -Dhdp.version=$&#123;HDP_VERSION:-&quot;2.6.1.0-129&quot;&#125; -fae 2&gt;&amp;1 | tee mvnverify.log</span><br></pre></td></tr></table></figure></li></ul><h3 id="踩坑记录"><a href="#踩坑记录" class="headerlink" title="踩坑记录"></a>踩坑记录</h3><ul><li><p>该暴露的端口得暴露出来，否则本地测试的时候对指定的端口无法进行tcp通信</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">9083:9083 //hive metastore</span><br><span class="line">8050:8050 // kylin 获取job output信息端口</span><br><span class="line">50010:50010 // dfs.datanode.address，这里踩坑很久,不配置的话会有：createBlockOutputStream when copying data into HDFS错误</span><br></pre></td></tr></table></figure></li><li><p>启动start_sandbox-hdp脚本的时候，可能会出现postgresql服务启动不了，这很可能是因为其申请的共享内存超过了系统的，只需要进入到容器里面做相关操作</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssh -p root@localhost</span><br><span class="line">sudo sysctl -w kernel.shmmax=17179869184 //假设你有16G内存，按实际扩充</span><br><span class="line">sudo service postgresql start</span><br><span class="line">//postgresql启动之后在容器外重新执行</span><br><span class="line">./start_sandbox-hdp.sh 就可以了</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> kylin </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kylin master-slave同步原理及问题排查</title>
      <link href="/2017/07/06/kylin-master-slave%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86%E5%8F%8A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
      <url>/2017/07/06/kylin-master-slave%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86%E5%8F%8A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/</url>
      <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>最近俩个月，团队整个数据基础架构慢慢转移到kylin上面来。而kylin也不负众望，对于一些复杂的聚合查询响应速度远超于hive。随着数据量的上来，kylin的单体部署逐渐无法支撑大量的并行读写任务。于是，自然而然的考虑到kylin的读写分离。一写多读，正好也符合kylin官方文档上的cluster架构。然而在实际的使用中也出现了一些问题:</p><ul><li>主节点更新了schema而从节点未sync</li><li>从节点中部分sync成功，而不是全部</li></ul><p>而很明显的是kylin中所有的数据，包括所有元数据都是落地在HBase中的，那唯一导致节点间数据不一致的可能就只有各个节点都有本地缓存的情况了。为了理解原理方便debug，我对kylin master-slave的同步原理做了一些源代码层面的剖析。</p><h3 id="原理剖析"><a href="#原理剖析" class="headerlink" title="原理剖析"></a>原理剖析</h3><h4 id="主从配置方式"><a href="#主从配置方式" class="headerlink" title="主从配置方式"></a>主从配置方式</h4><p>关于配置的格式，不得不吐槽官方文档的滑水。并没有给出详细的节点配置格式，查阅相关源码才发现正确的配置格式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//kylin.properties下面的配置，根据源码，配置的格式为：user:pwd@host:port</span><br><span class="line">kylin.server.cluster-servers=user:password@host:port,user:password@host:port,user:password@host:port</span><br></pre></td></tr></table></figure><h4 id="流程解析"><a href="#流程解析" class="headerlink" title="流程解析"></a>流程解析</h4><p><img src="http://jacobs.wanhb.cn/images/master-slave-kylin.png" alt="流程解析"></p><h4 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h4><ul><li><p>先来看看整个同步机制的核心BroadCaster类的实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">//Broadcaster的构造函数</span><br><span class="line">private Broadcaster(final KylinConfig config) &#123;</span><br><span class="line">      this.config = config;</span><br><span class="line">      //获取kylin.properties中&quot;kylin.server.cluster-servers&quot;配置的值</span><br><span class="line">      //也就是集群中所有节点的配置了</span><br><span class="line">      final String[] nodes = config.getRestServers();</span><br><span class="line">      if (nodes == null || nodes.length &lt; 1) &#123;</span><br><span class="line">          logger.warn(&quot;There is no available rest server; check the &apos;kylin.server.cluster-servers&apos; config&quot;);</span><br><span class="line">          broadcastEvents = null; // disable the broadcaster</span><br><span class="line">          return;</span><br><span class="line">      &#125;</span><br><span class="line">      logger.debug(nodes.length + &quot; nodes in the cluster: &quot; + Arrays.toString(nodes));</span><br><span class="line"></span><br><span class="line">      //开一个单线程，不间断的循环从broadcastEvents队列里面获取注册的事件。</span><br><span class="line">      Executors.newSingleThreadExecutor(new DaemonThreadFactory()).execute(new Runnable() &#123;</span><br><span class="line">          @Override</span><br><span class="line">          public void run() &#123;</span><br><span class="line">              final List&lt;RestClient&gt; restClients = Lists.newArrayList();</span><br><span class="line">              for (String node : config.getRestServers()) &#123;</span><br><span class="line">                  //根据配置的节点信息注册RestClient</span><br><span class="line">                  restClients.add(new RestClient(node));</span><br><span class="line">              &#125;</span><br><span class="line">              final ExecutorService wipingCachePool = Executors.newFixedThreadPool(restClients.size(), new DaemonThreadFactory());</span><br><span class="line">              while (true) &#123;</span><br><span class="line">                  try &#123;</span><br><span class="line">                      final BroadcastEvent broadcastEvent = broadcastEvents.takeFirst();</span><br><span class="line">                      logger.info(&quot;Announcing new broadcast event: &quot; + broadcastEvent);</span><br><span class="line">                      for (final RestClient restClient : restClients) &#123;</span><br><span class="line">                          wipingCachePool.execute(new Runnable() &#123;</span><br><span class="line">                              @Override</span><br><span class="line">                              public void run() &#123;</span><br><span class="line">                                  try &#123;</span><br><span class="line">                                      restClient.wipeCache(broadcastEvent.getEntity(), broadcastEvent.getEvent(), broadcastEvent.getCacheKey());</span><br><span class="line">                                  &#125; catch (IOException e) &#123;</span><br><span class="line">                                      logger.warn(&quot;Thread failed during wipe cache at &quot; + broadcastEvent, e);</span><br><span class="line">                                  &#125;</span><br><span class="line">                              &#125;</span><br><span class="line">                          &#125;);</span><br><span class="line">                      &#125;</span><br><span class="line">                  &#125; catch (Exception e) &#123;</span><br><span class="line">                      logger.error(&quot;error running wiping&quot;, e);</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>通过Broadcaster的构造函数其实就能清楚整个同步过程的大概逻辑了。无非就是启动一个线程去轮询阻塞队列里面的元素，有的话就消费下来广播到其他从节点从而达到清理缓存的目的。</p></li><li><p>再来看看广播的实际逻辑实现,基本封装在RestClient中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">//此处是根据配置的节点信息正则匹配：&quot;user:pwd@host:port&quot;</span><br><span class="line">public RestClient(String uri) &#123;</span><br><span class="line">   Matcher m = fullRestPattern.matcher(uri);</span><br><span class="line">   if (!m.matches())</span><br><span class="line">       throw new IllegalArgumentException(&quot;URI: &quot; + uri + &quot; -- does not match pattern &quot; + fullRestPattern);</span><br><span class="line"></span><br><span class="line">   String user = m.group(1);</span><br><span class="line">   String pwd = m.group(2);</span><br><span class="line">   String host = m.group(3);</span><br><span class="line">   String portStr = m.group(4);</span><br><span class="line">   int port = Integer.parseInt(portStr == null ? &quot;7070&quot; : portStr);</span><br><span class="line"></span><br><span class="line">   init(host, port, user, pwd);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>根据配置的节点信息实例化RestClient，然后在init方法中，拼接wipe cache的url</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">private void init(String host, int port, String userName, String password) &#123;</span><br><span class="line">  this.host = host;</span><br><span class="line">  this.port = port;</span><br><span class="line">  this.userName = userName;</span><br><span class="line">  this.password = password;</span><br><span class="line">  //拼接rest接口</span><br><span class="line">  this.baseUrl = &quot;http://&quot; + host + &quot;:&quot; + port + &quot;/kylin/api&quot;;</span><br><span class="line"></span><br><span class="line">  client = new DefaultHttpClient();</span><br><span class="line"></span><br><span class="line">  if (userName != null &amp;&amp; password != null) &#123;</span><br><span class="line">      CredentialsProvider provider = new BasicCredentialsProvider();</span><br><span class="line">      UsernamePasswordCredentials credentials = new UsernamePasswordCredentials(userName, password);</span><br><span class="line">      provider.setCredentials(AuthScope.ANY, credentials);</span><br><span class="line">      client.setCredentialsProvider(provider);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现kylin所有的交互接口基本上底层都是调用的自己的rest接口，它自己所谓的jdbc的查询方式其实也只是在rest接口上封装了一层，底层还是http请求。可谓是挂羊头卖狗肉了。看看RestClient中怎么去通知其他节点wipe cache的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void wipeCache(String entity, String event, String cacheKey) throws IOException &#123;</span><br><span class="line">   String url = baseUrl + &quot;/cache/&quot; + entity + &quot;/&quot; + cacheKey + &quot;/&quot; + event;</span><br><span class="line">   HttpPut request = new HttpPut(url);</span><br><span class="line"></span><br><span class="line">   try &#123;</span><br><span class="line">       HttpResponse response = client.execute(request);</span><br><span class="line">       String msg = EntityUtils.toString(response.getEntity());</span><br><span class="line"></span><br><span class="line">       if (response.getStatusLine().getStatusCode() != 200)</span><br><span class="line">           throw new IOException(&quot;Invalid response &quot; + response.getStatusLine().getStatusCode() + &quot; with cache wipe url &quot; + url + &quot;\n&quot; + msg);</span><br><span class="line">   &#125; catch (Exception ex) &#123;</span><br><span class="line">       throw new IOException(ex);</span><br><span class="line">   &#125; finally &#123;</span><br><span class="line">       request.releaseConnection();</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>已经很明了了，就是调的rest接口：/kylin/api/cache/{entity}/{cacaheKey}/{event}</p></li><li><p>当slave节点接收到wipeCache的指令时的处理逻辑如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public void notifyMetadataChange(String entity, Event event, String cacheKey) throws IOException &#123;</span><br><span class="line">     Broadcaster broadcaster = Broadcaster.getInstance(getConfig());</span><br><span class="line"></span><br><span class="line">     //这里会判断当前节点是否注册为listener了，如果注册了，此逻辑会被ignored</span><br><span class="line">     broadcaster.registerListener(cacheSyncListener, &quot;cube&quot;);</span><br><span class="line"></span><br><span class="line">     broadcaster.notifyListener(entity, event, cacheKey);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> //注册listener的逻辑</span><br><span class="line"> public void registerListener(Listener listener, String... entities) &#123;</span><br><span class="line">  synchronized (CACHE) &#123;</span><br><span class="line">      // ignore re-registration</span><br><span class="line">      List&lt;Listener&gt; all = listenerMap.get(SYNC_ALL);</span><br><span class="line">      if (all != null &amp;&amp; all.contains(listener)) &#123;</span><br><span class="line">          return;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      for (String entity : entities) &#123;</span><br><span class="line">          if (!StringUtils.isBlank(entity))</span><br><span class="line">              addListener(entity, listener);</span><br><span class="line">      &#125;</span><br><span class="line">      //注册几种事件类型</span><br><span class="line">      addListener(SYNC_ALL, listener);</span><br><span class="line">      addListener(SYNC_PRJ_SCHEMA, listener);</span><br><span class="line">      addListener(SYNC_PRJ_DATA, listener);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>notifyListener主要就是对所有事件处理逻辑的划分，根据事件类型选择处理逻辑，一般sheme的更新走的是默认逻辑</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">public void notifyListener(String entity, Event event, String cacheKey) throws IOException &#123;</span><br><span class="line">     synchronized (CACHE) &#123;</span><br><span class="line">         List&lt;Listener&gt; list = listenerMap.get(entity);</span><br><span class="line">         if (list == null)</span><br><span class="line">             return;</span><br><span class="line"></span><br><span class="line">         logger.debug(&quot;Broadcasting metadata change: entity=&quot; + entity + &quot;, event=&quot; + event + &quot;, cacheKey=&quot; + cacheKey + &quot;, listeners=&quot; + list);</span><br><span class="line"></span><br><span class="line">         // prevents concurrent modification exception</span><br><span class="line">         list = Lists.newArrayList(list);</span><br><span class="line">         switch (entity) &#123;</span><br><span class="line">         case SYNC_ALL:</span><br><span class="line">             for (Listener l : list) &#123;</span><br><span class="line">                 l.onClearAll(this);</span><br><span class="line">             &#125;</span><br><span class="line">             clearCache(); // clear broadcaster too in the end</span><br><span class="line">             break;</span><br><span class="line">         case SYNC_PRJ_SCHEMA:</span><br><span class="line">             ProjectManager.getInstance(config).clearL2Cache();</span><br><span class="line">             for (Listener l : list) &#123;</span><br><span class="line">                 l.onProjectSchemaChange(this, cacheKey);</span><br><span class="line">             &#125;</span><br><span class="line">             break;</span><br><span class="line">         case SYNC_PRJ_DATA:</span><br><span class="line">             ProjectManager.getInstance(config).clearL2Cache(); // cube&apos;s first becoming ready leads to schema change too</span><br><span class="line">             for (Listener l : list) &#123;</span><br><span class="line">                 l.onProjectDataChange(this, cacheKey);</span><br><span class="line">             &#125;</span><br><span class="line">             break;</span><br><span class="line">         //大部分的走向</span><br><span class="line">         default:</span><br><span class="line">             for (Listener l : list) &#123;</span><br><span class="line">                 l.onEntityChange(this, entity, event, cacheKey);</span><br><span class="line">             &#125;</span><br><span class="line">             break;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         logger.debug(&quot;Done broadcasting metadata change: entity=&quot; + entity + &quot;, event=&quot; + event + &quot;, cacheKey=&quot; + cacheKey);</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>看到default分支会执行onEntityChange这个方法，看一下这个方法干的是什么</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">private Broadcaster.Listener cacheSyncListener = new Broadcaster.Listener() &#123;</span><br><span class="line">   @Override</span><br><span class="line">   public void onClearAll(Broadcaster broadcaster) throws IOException &#123;</span><br><span class="line">       removeAllOLAPDataSources();</span><br><span class="line">       cleanAllDataCache();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   @Override</span><br><span class="line">   public void onProjectSchemaChange(Broadcaster broadcaster, String project) throws IOException &#123;</span><br><span class="line">       removeOLAPDataSource(project);</span><br><span class="line">       cleanDataCache(project);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   @Override</span><br><span class="line">   public void onProjectDataChange(Broadcaster broadcaster, String project) throws IOException &#123;</span><br><span class="line">       removeOLAPDataSource(project); // data availability (cube enabled/disabled) affects exposed schema to SQL</span><br><span class="line">       cleanDataCache(project);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   @Override</span><br><span class="line">   public void onEntityChange(Broadcaster broadcaster, String entity, Event event, String cacheKey) throws IOException &#123;</span><br><span class="line">       if (&quot;cube&quot;.equals(entity) &amp;&amp; event == Event.UPDATE) &#123;</span><br><span class="line">           final String cubeName = cacheKey;</span><br><span class="line">           new Thread() &#123; // do not block the event broadcast thread</span><br><span class="line">               public void run() &#123;</span><br><span class="line">                   try &#123;</span><br><span class="line">                       Thread.sleep(1000);</span><br><span class="line">                       cubeService.updateOnNewSegmentReady(cubeName);</span><br><span class="line">                   &#125; catch (Throwable ex) &#123;</span><br><span class="line">                       logger.error(&quot;Error in updateOnNewSegmentReady()&quot;, ex);</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;.start();</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;;</span><br></pre></td></tr></table></figure><p>看到对于cache的同步是单独实现了一个listener的，Event为update的时候，会单独启动一个线程去执行刷新缓存操作</p></li></ul><h3 id="加入简单的重试逻辑"><a href="#加入简单的重试逻辑" class="headerlink" title="加入简单的重试逻辑"></a>加入简单的重试逻辑</h3><p>由于目前对于同步失败的猜想是目标服务短暂不可用（响应超时或者处于失败重启阶段），于是我只是单纯的将失败的任务重新塞入broadcastEvents队列尾部供再一次调用。当然这种操作过于草率和暴力，却也是验证猜想最简单快速的方式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">for (final RestClient restClient : restClients) &#123;</span><br><span class="line">           wipingCachePool.execute(new Runnable() &#123;</span><br><span class="line">             @Override</span><br><span class="line">             public void run() &#123;</span><br><span class="line">               try &#123;</span><br><span class="line">                 restClient.wipeCache(broadcastEvent.getEntity(), broadcastEvent.getEvent(),</span><br><span class="line">                     broadcastEvent.getCacheKey());</span><br><span class="line">               &#125; catch (IOException e) &#123;</span><br><span class="line">                 logger</span><br><span class="line">                     .warn(&quot;Thread failed during wipe cache at &#123;&#125;, error msg: &#123;&#125;&quot;, broadcastEvent,</span><br><span class="line">                         e.getMessage());</span><br><span class="line">                 try &#123;</span><br><span class="line">                   //这里重新塞入队列尾部，等待重新执行</span><br><span class="line">                   broadcastEvents.putLast(broadcastEvent);</span><br><span class="line">                   logger.info(&quot;put failed broadcastEvent to queue. broacastEvent: &#123;&#125;&quot;,</span><br><span class="line">                       broadcastEvent);</span><br><span class="line">                 &#125; catch (InterruptedException ex) &#123;</span><br><span class="line">                   logger.warn(&quot;error reentry failed broadcastEvent to queue, broacastEvent:&#123;&#125;, error: &#123;&#125; &quot;,</span><br><span class="line">                       broadcastEvent, ex);</span><br><span class="line">                 &#125;</span><br><span class="line">               &#125;</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;);</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure><p>编译部署之后，日志中出现了如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Thread failed during wipe cache at java.lang.IllegalStateException: Invalid use of BasicClientConnManager: connection still allocated.</span><br></pre></td></tr></table></figure><p>比较意外，不过也终于发现了问题的所在。Kylin在启动的时候会按照配置的nodes实例化一次RestClient，之后就直接从缓存中拿了，而kylin用的DefaultHttpClient每次只允许一次请求，请求完必须释放链接，否则无法复用HttpClient。所以需要修改wipeCache方法的逻辑如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public void wipeCache(String entity, String event, String cacheKey) throws IOException &#123;</span><br><span class="line">    String url = baseUrl + &quot;/cache/&quot; + entity + &quot;/&quot; + cacheKey + &quot;/&quot; + event;</span><br><span class="line">    HttpPut request = new HttpPut(url);</span><br><span class="line"></span><br><span class="line">    HttpResponse response =null;</span><br><span class="line">    try &#123;</span><br><span class="line">        response = client.execute(request);</span><br><span class="line">        String msg = EntityUtils.toString(response.getEntity());</span><br><span class="line"></span><br><span class="line">        if (response.getStatusLine().getStatusCode() != 200)</span><br><span class="line">            throw new IOException(&quot;Invalid response &quot; + response.getStatusLine().getStatusCode() + &quot; with cache wipe url &quot; + url + &quot;\n&quot; + msg);</span><br><span class="line">    &#125; catch (Exception ex) &#123;</span><br><span class="line">        throw new IOException(ex);</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        //确保释放连接</span><br><span class="line">        if(response!=null) &#123;</span><br><span class="line">          EntityUtils.consume(response.getEntity());</span><br><span class="line">        &#125;</span><br><span class="line">        request.releaseConnection();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> infrastructure </tag>
            
            <tag> 成长 </tag>
            
            <tag> 源码 </tag>
            
            <tag> Kylin </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>HBase架构脑图</title>
      <link href="/2017/06/25/HBase%E6%9E%B6%E6%9E%84%E8%84%91%E5%9B%BE/"/>
      <url>/2017/06/25/HBase%E6%9E%B6%E6%9E%84%E8%84%91%E5%9B%BE/</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近，由于我司数据基础新架构正在陆续往以Kylin为中心的方向上走，而Kylin的底层存储又是重度依赖HBase，为了保证数据服务的稳定性，无奈最近开始研究HBase的源码以及工作原理。翻阅各种书籍博客之余，大致总结了一张初期的HBase架构脑图，之后会随着认知的深入去不断更新。结合源码方面的分析也会陆续进行…比较蛋疼的是，HBase的源码太过庞大，不如Kylin的结构清晰，慢慢看吧…</p><h3 id="脑图"><a href="#脑图" class="headerlink" title="脑图"></a>脑图</h3><p><img src="https://pic1.zhimg.com/80/v2-a0a55482bb8c93edfedc2eb59c678424_hd.png" alt="HBase 架构图"></p>]]></content>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> Hbase </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kylin学习笔记</title>
      <link href="/2017/05/02/Kylin%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2017/05/02/Kylin%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="OLAP-on-Line-AnalysisProcessing-的实现方式"><a href="#OLAP-on-Line-AnalysisProcessing-的实现方式" class="headerlink" title="OLAP(on-Line AnalysisProcessing)的实现方式"></a>OLAP(on-Line AnalysisProcessing)的实现方式</h3><ul><li>ROLAP:<br>基于关系数据库的OLAP实现（Relational OLAP）。ROLAP将多维数据库的多维结构划分为两类表:一类是事实表,用来存储数据和维关键字;另一类是维表,即对每个维至少使用一个表来存放维的层次、成员类别等维的描述信息。维表和事实表通过主关键字和外关键字联系在一起,形成了”星型模式”。对于层次复杂的维,为避免冗余数据占用过大的存储空间,可以使用多个表来描述,这种星型模式的扩展称为”雪花模式”。特点是将细节数据保留在关系型数据库的事实表中，聚合后的数据也保存在关系型的数据库中。这种方式查询效率最低，不推荐使用。</li><li><p>MOLAP:<br>多维数据组织的OLAP实现（Multidimensional OLAP。以多维数据组织方式为核心,也就是说,MOLAP使用多维数组存储数据。多维数据在存储中将形成”立方块（Cube）”的结构,在MOLAP中对”立方块”的”旋转”、”切块”、”切片”是产生多维数据报表的主要技术。特点是将细节数据和聚合后的数据均保存在cube中，所以以空间换效率，查询时效率高，但生成cube时需要大量的时间和空间。</p></li><li><p>HOLAP: 基于混合数据组织的OLAP实现（Hybrid OLAP）。如低层是关系型的，高层是多维矩阵型的。这种方式具有更好的灵活性。特点是将细节数据保留在关系型数据库的事实表中，但是聚合后的数据保存在cube中,聚合时需要比ROLAP更多的时间,查询效率比ROLAP高，但低于MOLAP。</p></li><li><p>kylin的cube数据是作为key-value结构存储在hbase中的，key是每一个维度成员的组合值，不同的cuboid下面的key的结构是不一样的，例如cuboid={brand，product，year}下面的一个key可能是brand=’Nike’，product=’shoe’，year=2015，那么这个key就可以写成Nike:shoe:2015，但是如果使用这种方式的话会出现很多重复，所以一般情况下我们会把一个维度下的所有成员取出来，然后保存在一个数组里面，使用数组的下标组合成为一个key，这样可以大大节省key的存储空间，kylin也使用了相同的方法，只不过使用了字典树（Trie树），每一个维度的字典树作为cube的元数据以二进制的方式存储在hbase中，内存中也会一直保持一份。</p></li></ul><h3 id="cube-构建"><a href="#cube-构建" class="headerlink" title="cube 构建"></a>cube 构建</h3><ul><li>Dimension：Mandatory、hierarchy、derived</li><li>增量cube: kylin的核心在于预计算缓存数据，因此无法达到真正的实时查询效果。一个cube中包含了多个segment，每一个segment对应着一个物理cube，在实际存储上对应着一个hbase的一个表。每次查询的时候会查询所有的segment聚合之后的值进行返回，但是当segment数量较多时，查询效率会降低，这时会对segment进行合并。被合并的几个segment所对应的hbase表并没有被删除。</li><li>cube词典树：cube数据是作为key-value结构存储在HBase中的。key是每一个维度成员的组合值</li></ul><h3 id="Streaming-cubing"><a href="#Streaming-cubing" class="headerlink" title="Streaming cubing"></a>Streaming cubing</h3><ul><li>支持实时数据的cub。与传统的cub一样，共享storage engine(HBase)以及query engine。kylin Streaming cubing相比其他实时分析系统来说，不需要特别大的内存，也不需要实现真正的实时分析。因为在OLAP中，存在几分钟的数据延迟是完全可以接受的。于是实现手法上采用了micro batch approach。</li><li>micro batch approach:将监听到的数据按照时间窗口的方式划分，并且为每个窗口封装了一个微量批处理，批处理后的结果直接存到HBase。</li><li>Streaming cubing data 最终会慢慢转换成普通的cubes,因为所有的数据是直接保存到HBase中的，并且保存为一个新的segment，当segment数量到达一定程度时，job engine会将segment 合并起来形成一个大的cube。</li></ul><h3 id="实战问题总结"><a href="#实战问题总结" class="headerlink" title="实战问题总结"></a>实战问题总结</h3><p>由于集群环境是CDH集群，所以选择了kylin CDH 1.6的版本，支持从Kafka读取消息建立Streaming cubes直接写入HDFS中</p><ul><li>选择一个集群namenode节点，将解压包放入/opt/cloudrea/parcels/目录中。如果是部署单节点，暂时不用更改配置文件。所有的配置加载都在bin/kylin.sh中。</li><li>直接kylin.sh start/stop 运行脚本，服务就会在7070端口起一个web界面。这个界面是可以进行可视化操作的。</li></ul><h3 id="Hive-数据源"><a href="#Hive-数据源" class="headerlink" title="Hive 数据源"></a>Hive 数据源</h3><ul><li>直接测试hive数据源是没有问题的，这一功能比较完善，也是主打功能。</li></ul><h3 id="kafka数据源"><a href="#kafka数据源" class="headerlink" title="kafka数据源"></a>kafka数据源</h3><p>从kylin 1.6 版本开始正式支持Kafka做数据源，将Streaming Cubes实时写入 HBase中。这一块在测试的时候也出现了问题：</p><ul><li><p>Kafka版本问题</p><ul><li>由于实验环境的CDH集群Kafka版本是0.9的，而kylin 仅支持0.10以上的版本，所以需要对CDH kafka集群进行升级。</li></ul></li><li><p>mapreduce运行环境无jar包</p><ul><li>kylin中提交cube build之后，map reduce任务直接抛错。错误提示是，找不到Kafka的Consumer类。根本原因是kylin默认集群上的map reduce classpath是会加载kafka-clients.jar包的，所以在提交任务的时候没有将kafka-clients.jar包打进去。这时可以有三种做法：</li><li>直接修改kylin的源码，将kafka-clients.jar包给包括进去（待尝试）。</li><li>可以通过修改集群的HADOOP_ClASSPATH的路径，将jar包给包括进去。</li><li>hadoop classpath 查看classpath目录信息 将对应jar包直接拷入map reduce classpath中，这方法简单，但是缺点就是需要逐个得对node进行操作。</li></ul></li><li><p>Property is not embedded format</p><ul><li><p>现在意识到，使用开源框架不会看其源码是不行的…就在我折腾俩天终于将mapreduce任务跑起来之后，新的错误出现了:”ava.lang.RuntimeException: java.io.IOException: Property ‘xxx’ is not embedded format”。莫名奇妙的错误。迫使我直接去github上看kylin kafka模块的源码。在TimedJsonStreamParser.java中发现代码逻辑中默认json数据中，如果key存在下划线就会将该key按照下划线split… 然后看key对应的value是不是map类型，如果不是直接抛出标题的错误。</p></li><li><p>明确了问题之后，如何复写默认下划线split的配置成为问题。由于官网的文档十分鸡肋，很多坑都没有涉及到，所以继续看源码。发现StreamingParser.java这个类中会去写一些默认的配置。</p></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public static final String PROPERTY_TS_COLUMN_NAME = &quot;tsColName&quot;;</span><br><span class="line">public static final String PROPERTY_TS_PARSER = &quot;tsParser&quot;;</span><br><span class="line">public static final String PROPERTY_TS_PATTERN = &quot;tsPattern&quot;;</span><br><span class="line">public static final String EMBEDDED_PROPERTY_SEPARATOR = &quot;separator&quot;;</span><br><span class="line"></span><br><span class="line">static &#123;</span><br><span class="line">        derivedTimeColumns.put(&quot;minute_start&quot;, 1);</span><br><span class="line">        derivedTimeColumns.put(&quot;hour_start&quot;, 2);</span><br><span class="line">        derivedTimeColumns.put(&quot;day_start&quot;, 3);</span><br><span class="line">        derivedTimeColumns.put(&quot;week_start&quot;, 4);</span><br><span class="line">        derivedTimeColumns.put(&quot;month_start&quot;, 5);</span><br><span class="line">        derivedTimeColumns.put(&quot;quarter_start&quot;, 6);</span><br><span class="line">        derivedTimeColumns.put(&quot;year_start&quot;, 7);</span><br><span class="line">        defaultProperties.put(PROPERTY_TS_COLUMN_NAME, &quot;timestamp&quot;);</span><br><span class="line">        defaultProperties.put(PROPERTY_TS_PARSER, &quot;org.apache.kylin.source.kafka.DefaultTimeParser&quot;);</span><br><span class="line">        defaultProperties.put(PROPERTY_TS_PATTERN, DateFormat.DEFAULT_DATETIME_PATTERN_WITHOUT_MILLISECONDS);</span><br><span class="line">        defaultProperties.put(EMBEDDED_PROPERTY_SEPARATOR, &quot;_&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>自然而然会联想到，这个默认的配置肯定是可以在用户设置的时候通过key（separator）去覆盖的…于是发现在构建Streaming table的时候，可以通过Parse Properties去覆盖配置。<br>于是直接写成如下的形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">tsColName=timestamp;separator=no</span><br><span class="line"></span><br><span class="line">//源码中拿到这个配置之后会做覆盖处理，然后执行 getValueByKey：</span><br><span class="line"></span><br><span class="line">protected String getValueByKey(String key, Map&lt;String, Object&gt; rootMap) throws IOException &#123;</span><br><span class="line">        if (rootMap.containsKey(key)) &#123;</span><br><span class="line">            return objToString(rootMap.get(key));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String[] names = nameMap.get(key);</span><br><span class="line">        if (names == null &amp;&amp; key.contains(separator)) &#123;</span><br><span class="line">            names = key.toLowerCase().split(separator);</span><br><span class="line">            nameMap.put(key, names);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (names != null &amp;&amp; names.length &gt; 0) &#123;</span><br><span class="line">            tempMap.clear();</span><br><span class="line">            tempMap.putAll(rootMap);</span><br><span class="line">            //这块如果复写了separator属性的话split后的names数组长度为1会跳过这一步循环，防止解析出错</span><br><span class="line">            for (int i = 0; i &lt; names.length - 1; i++) &#123;</span><br><span class="line">                Object o = tempMap.get(names[i]);</span><br><span class="line">                if (o instanceof Map) &#123;</span><br><span class="line">                    tempMap.clear();</span><br><span class="line">                    tempMap.putAll((Map&lt;String, Object&gt;) o);</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    throw new IOException(&quot;Property &apos;&quot; + names[i] + &quot;&apos; is not embedded format&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            Object finalObject = tempMap.get(names[names.length - 1]);</span><br><span class="line">            return objToString(finalObject);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return StringUtils.EMPTY;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> kylin </tag>
            
            <tag> infrastructure </tag>
            
            <tag> data </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>HBase学习与经验总结</title>
      <link href="/2017/03/15/HBase%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
      <url>/2017/03/15/HBase%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>因为业务的关系，也零零碎碎的接触到了HBase，并对其产生了兴趣。这段时间又抽空看了一下《HBase权威指南》，于是，秉承着学习记录的习惯，把一些知识和经验写成了博客。</p><h3 id="一些基础知识"><a href="#一些基础知识" class="headerlink" title="一些基础知识"></a>一些基础知识</h3><ul><li>表：HBase将数据组织到自己的HTable表中，这个表是根据列族(colomn family)在物理上保存数据的，每个列族都有自己的文件夹和storefiles，不像关系型数据库那样将一个表保存成一个文件，表明也是文件系统路径的一部分。</li><li>行键：每行都有唯一的行键，行键没有数据类型，它内部被认为是一个字节数组。</li><li>列族：HBase表中的行是按一个叫colomn family的列族分组的，也是在磁盘上也是按列族存储数据的，由于这个原因，故当定义一个hbase表时，除了定义表名外，还必须定义列族。传统数据库没有列族的概念。</li><li>列修饰符：列簇定义真实的列，被称之为列修饰符，你可以认为列修饰符就是列本身。</li></ul><h3 id="存储机制"><a href="#存储机制" class="headerlink" title="存储机制"></a>存储机制</h3><ul><li>数据结构：LSM树。使用日志文件和内存存储来将随机写转换成顺序写，因此也能保证稳定的数据插入速率。由于读和写独立，因此在这俩种操作之间没有冲突。由于存储数据的布局较优，查询一个键需要的磁盘寻到次数在一个可预测的范围内。并且读取与该键连续的任意数量的记录都不会引发任何额外的磁盘寻道。</li><li>启动HBase时。HMaster负责将所有的region分配到HRegion Server 上，其中也包括特别的-Root-和.META.表。HRegion Server负责打开region,并创建对应的HRegion实例。这些列族是用户之前创建表时定义的。每个store实例包含一个或多个storefile实例，它们实际数据存储文件HFile的轻量级封装。每个Store还有其对应的一个MemStore, 一个HregionServer分享了一个HLog实例。</li><li>基本的请求流程: 客户端先联系ZK子集群查找行键。此过程是通过ZK获取含有-ROOT-的region服务器名来完成的。通过含有-Root-的region服务器可以查询到含有.META表中对应的region服务器名，其中包括请求的行键信息。这俩处主要内容都被缓存起来了。并且都只查询一次。最终通过查询.META.服务器来获取客户端的行键数据所在的region的服务器名。</li><li>写数据的过程。第一步决定数据是否需要写到由HLog类实现的预写日志中。WAL是标准的Hadoop SequenceFile 并且存储了HlogKey实例。这些键包括序列号和实际数据。所以在服务器崩溃时可以回滚还没有持久化的数据。一旦数据被写入WAL中，数据就会被放到MemStore中，同时还会检查memstore是否已经骂了，如果满了，就会被请求刷写到磁盘中去。刷写请求由另外一个HRegionServer的线程处理，它会把数据写成一个新的HFile。同时也会保存最后写入的序号。系统就知道哪些数据现在被持久化了。</li></ul><h3 id="region拆分与合并"><a href="#region拆分与合并" class="headerlink" title="region拆分与合并"></a>region拆分与合并</h3><h4 id="拆分"><a href="#拆分" class="headerlink" title="拆分"></a>拆分</h4><ul><li>当一个region里的存储文件增长到大于配置的hbase.hregion.max.filesize大小或者在列族层面配置的大小时，region会被一分为二。这个过程通常非常迅速，因为系统只是为新region创建了俩个对应的文件，每个region是原始region的一半。</li><li>region服务器在父region中创建splits目录来完成这个过程。接下来关闭该region。此后这个region不再接受任何请求。然后region服务器通过在splits目录中设立必须的文件结构来准备新的子region，包括新region目录和参考文件。如果这个过程成功完成，它将把两个新region目录移到表目录中。.META.表中父region的状态会被更新，以表示其现在拆分的节点和子节点是什么。可以避免父region被意外重新打开。</li></ul><h4 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h4><h3 id="rowKey的设计"><a href="#rowKey的设计" class="headerlink" title="rowKey的设计"></a>rowKey的设计</h3><h4 id="关于rowKey的一些认识"><a href="#关于rowKey的一些认识" class="headerlink" title="关于rowKey的一些认识"></a>关于rowKey的一些认识</h4><ul><li>HBase表里只有键(KeyValue对象的Key部分，包括行键、列限定符和时间戳)可以建立索引。访问一个特定行的唯一办法是通过行键。</li><li>设计HBase表时，行键是唯一重要的事情，因此应该按照预期的访问方式来建立行键。此结论基于俩个事实依据：<ul><li>region基于行键为一个区间的行提供服务，并且负责区间内每一行</li><li>HFile在硬盘上存储有序的行。当region刷写留在内存里的行时生成了HFile。这些行已经排过序，也会有序地刷写到硬盘上。</li></ul></li></ul><h4 id="设计规范"><a href="#设计规范" class="headerlink" title="设计规范"></a>设计规范</h4><ul><li>先定好查询方案，可以使用包含部分键的扫描机制设计出非常有效的左对齐索引（字典序从左到右排序）当一个字段被加到键中时就多了一个可以检索的维度。</li><li>用户需要保证行键中每个字段的值都被补齐到这个字段所设的长度，这样字典序才会按照预期排列，（按照二进制内容比较，并升序排列），用户需要为每个字段设定一个固定的长度来保证每个字段比较时只会与同字段内容从左到右比较，否则可能出现溢出的情况。</li><li><p>可能的起始键的含义：</p><ul><li><userid> 扫描一个给定用户ID下的所有消息</userid></li><li><userid>-<date> 扫描一个给定用户ID下特定日期的消息</date></userid></li><li><userid>-<date>-<messageid> 扫描一个用户ID和日期下的一个消息。</messageid></date></userid></li></ul></li><li><p>防止系统产生热读写的设计：</p><ul><li>salting方式：使用salting前缀来保证数据分散到所有的region服务器。缺点就是当用户要扫描一个连续的范围时，可能需要跨region请求，这样的话可以通过多线程读取。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">byte prefix=(byte)(Long.hashCode(time)%&lt;number of region servers&gt;);</span><br></pre></td></tr></table></figure><ul><li>随机化：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">byte rowkey=MD5(timestamp);</span><br></pre></td></tr></table></figure><p>利用散列函数能将行键分散到所有的region服务器上，对于时间连续的数据，这样做不好，因为散列之后无法通过时间范围扫描数据。由于用户可以用散列的方式重新生成行键，随机化的方式很适合每次读取一行数据的应用，如果用户的数据不需要连续扫描而只需要随机读取，用户可以使用这种策略。</p></li></ul>]]></content>
      
      
        <tags>
            
            <tag> HBase </tag>
            
            <tag> BigData </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>InnoDB读书笔记</title>
      <link href="/2017/02/15/InnoDB%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2017/02/15/InnoDB%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><ol><li><p>数据库的四种隔离级别</p><ul><li>Read Uncommitted(读未提交): 如果一个事务已经开始写数据，则另外一个事务则不允许同时进行写操作，但允许其他事务读此行数据。于是事务B可能读取到了事务A未提交的数据。</li><li>Read -Committed(读提交): 读取数据的事务允许其他事务继续访问该行事务，但是未提交的写事务存在时禁止一切其他事务，该隔离级别避免了脏读，但是却可能出现不可重复读。事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。</li><li>Repeated read （可重复读） 读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。mysql的InnoDB的Repeated read 级别就可以解决幻读的问题(源自Next-Key Locking算法)，而oracle只能将隔离级别设置在Serializable才能解决幻读的问题。</li><li>Serializable（串行化）： 序列化是最高的事务隔离级别，同时代价也花费最高，性能很低，一般很少使用，在该级别下，事务顺序执行，不仅可以避免脏读、不可重复读，还避免了幻像读。 提供严格的事务隔离。它要求事务序列化执行，事务只能一个接着一个地执行，但不能并发执行。</li></ul></li><li><p>InnoDB关键特性</p><ul><li>插入缓冲（Insert Buffer） 不可能每张表上只有一个聚集索引，在进行插入操作时候，数据页存放还是按照主键a进行顺序存放的，但是对于非聚集索引叶子节点的插入不再是顺序的了，这时会需要离散地访问非聚集索引页，需要注意，辅助索引的插入顺序依然是顺序的，或者说比较顺序的，比如用户购买表中的时间字段，通常情况下，用户购买时间是一个辅助索引，用来根据时间条件进行查询，但是在插入时却是根据时间递增而插入的，因此插入较为顺序。Insert Buffer 对于非聚集索引的插入和更新操作，不是每一次直接插入到索引页面，而是判断插入的是否在非索引页是否在缓冲池，若在，则直接插入。若不在先放入一个Insert Buffer中，好似欺骗。 因此  Insert Buffer需要满足的俩个条件： 索引是辅助索引，并且索引不是唯一的。</li><li>俩次写（Double Write）</li><li>自适应哈希索引（Adaptive Hash Index）</li><li>异步IO</li><li>刷新邻接页</li></ul></li></ol><h3 id="索引与算法"><a href="#索引与算法" class="headerlink" title="索引与算法"></a>索引与算法</h3><ol><li>InnoDB存储引擎支持的几种常见索引<ul><li>B+ 树索引：其下索引又分为：聚集索引，辅助索引（内部均为B+树）</li><li>全文索引</li><li>哈希索引 （哈希索引是自适应的，根据表的使用情况自动生成） 注意：B+ 树索引并不能找到一个给定键值的具体行。B+树索引能找到的只是被查找数据行所在的页，然后数据库通过把页读入到内存，再在内训中进行查找，最后得到想要查找的数据。</li></ul></li></ol><h4 id="B-树索引"><a href="#B-树索引" class="headerlink" title="B+树索引"></a>B+树索引</h4><ol><li>定义和性质<ul><li>由二叉树和平衡二叉树演化而来。是为磁盘或其他直接存取辅助设备设计的一种平衡查找树，在B+树中，所有记录的节点都是按照键值的大小顺序存放在同一层的叶子节点上。由各叶子节点指针进行连接。</li><li>B+树的插入必须保证插入之后叶子节点的记录依然排序。</li><li>B+树在数据库中的有一个特点就是高扇出性，因此在数据库中B+树的高度一般都在2～4层，也就是说查找一个键值记录最多只需要2到4次IO。</li></ul></li><li><p>B+树定义</p><ul><li>分为聚集索引和辅助索引。俩者的不同之处在于叶子节点存放的是否是一整行的信息。MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址，而InnoDB的辅助索引data域存储相应记录主键的值而不是地址。而聚集索引能够在叶子节点上直接找到数据。此外，由于定义了数据的逻辑顺序，聚集索引能够特别快地针对范围值查询，查询优化器能够快速发现某一段范围的数据页需要扫描。</li><li>聚集索引。由于实际的数据页只能按照一棵B+树进行排序，因此每张表只能拥有一个聚集索引。在多数情况下，查询优化器倾向于使用聚集索引。因为:<br>1）聚集索引能够直接在B+树索引上找到数据。<br>2）定义了数据的逻辑顺序，聚集索引能够特别快地访问针对范围值的查询。能够快速的发现某一段范围的数据页需要扫描<br><strong>误区</strong>：很多书和博客都介绍，聚集索引按照顺序物理地存储数据，其实这样会导致维护成本非常高，所以聚集索引的存储并不是物理上连续的。而是逻辑连续。<strong>原因</strong>：<br>1）页通过双向链表链接，页按照主键的顺序排序；<br>2）页中的记录也是通过双向链表进行维护的，物理存储上可以同样不按照主键存储。</li></ul></li><li><p>B+树索引的分裂</p></li><li>B+树索引的管理<ul><li><strong>Cardinality值：</strong><br>1）并不是在所有的查询条件中出现的列都需要添加索引，一般的经验是，在访问表中很少一部分时使用B+树索引才有意义，对于性别、地区、类型字段，它们的可取值范围很小，称为低选择性，于是建索引是完全没有必要的。<br>2）对于高选择性的确定，可以通过show index结果列中的列Cardinality来观察，表示的是索引中不重复记录数量的预估值。注意：仅仅是个预估值，而不是一个准确值。</li></ul></li><li><p>B+索引树的使用</p><ul><li><p>分析<br>1）用Show index table 查看索引的情况<br>2）用explain+sql语句 来分析这条sql的情况，包括使用的possible_keys（可能选择的索引）与key(实际选择的索引)</p></li><li><p>联合索引<br>是指对表上的多个列进行索引，创建方法与单个索引的创建方法一样，不同之处在于有多个索引列,如对于联合索引(a,b) ,以下是关于联合索引的一些情况</p><ul><li><p>select * from table where a=x and b=x;</p><pre><code>显然是可以用(a,b)索引的，因为索引的顺序就是按照（a,b）来排序的</code></pre></li><li><p>select * from table where a=xxx; 这个也是可以使用(a,b)索引的，只不过只能使用一部分</p></li><li>select * from table where b=xxx; 这个是不能使用索引的，因为压根就没按照b做排序。</li></ul></li><li><p>联合索引的好处是，在第一个索引的基础上，已经给第二个键值做了排序的处理，从而可以减少一次排序操作：比如我们要查某个用户购买商品的情况，并且按照时间排序。</p></li><li><p>覆盖索引</p><ul><li>Innodb存储引擎支持覆盖索引，即从辅助索引中就可以得到查询的记录，而不需要查询聚集索引中的记录。使用覆盖索引的一个好处是辅助索引不包含整行记录的所有信息，因此，大小要远小于聚集索引，可以大大减少IO操作。</li><li><p>对于Innodb的辅助索引而言，由于其包含了主键信息，因此其叶子节点存放的数据为(primary key1,primary key2,…)例如，下列语句都可以通过使用一次辅助索引来完成查询</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select primary key1,key2,… from table</span><br><span class="line">where</span><br><span class="line">key1=xxx;</span><br><span class="line">对于覆盖索引的另一个好处是对某些统计问题而言，</span><br><span class="line">不会选择通过聚集索引来进行统计。因为辅助索引的量远小于聚集索引，</span><br><span class="line">可以大大减少IO操作。</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ol><h4 id="优化器选择不使用索引的情况"><a href="#优化器选择不使用索引的情况" class="headerlink" title="优化器选择不使用索引的情况"></a>优化器选择不使用索引的情况</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select * from order </span><br><span class="line">where orderId&gt;10000 and orderId&lt;102000; </span><br><span class="line">（从orderId的基数看来非常大，这是前提）</span><br></pre></td></tr></table></figure><ul><li>这句sql按道理来说可以使用辅助索引的，可选的 KEYS有primary orderId, 等索引，然而最后选择了primary聚集索引，也就是全表扫描。</li><li>原因在于用户要选取的数据是整行信息，而orderId索引不能覆盖到我们要查询的信息，因此在对orderId索引查询到指定的数据之后，还需要一次书签访问来查询整行信息，虽然orderId索引中的数据是顺序存放的，但是再一次进行书签来查找数据则是无序的，因此变为了磁盘上的离散读操作。</li><li>如果要求访问的数据量很小。则优化器还会选择辅助索引，但是当访问的数据占整个表中数据的蛮大一部分的时候（通常是百分之二十），优化器则会选择通过聚集索引来查找数据，因此之前已经提到过，顺序读要远远快于离散读。</li><li><p>因此，对于不能进行索引覆盖的情况，优化器选择辅助索引的条件是占有的数据量很小。当然这是由当前传统的机械硬盘的特性决定的。</p></li><li><p>InnoDB存储引擎中的哈希算法<br>自适应哈希索引：数据库自身创建并使用的，经过哈希函数映射到一个哈希表中，因此对于字典类型的查找非常快速。如 select * from table where index_cole=“xxx” 但是对于范围查找久无能为力了。</p></li></ul><h3 id="InnoDB锁机制"><a href="#InnoDB锁机制" class="headerlink" title="InnoDB锁机制"></a>InnoDB锁机制</h3><h4 id="lock-与-latch区别"><a href="#lock-与-latch区别" class="headerlink" title="lock 与 latch区别"></a>lock 与 latch区别</h4><ul><li>latch一般称为轻量级锁，因为其要求锁定的时间必须非常短。若持续的时间长，则应用的性能会非常差。在InnoDB存储引擎中，latch又可以氛围mutex（互斥量）与 rwlock 读写锁。目的是用来保证并发线程的操作临界资源的正确性，并且通常没有死锁检测的机制，仅仅通过应用程序加锁的顺序来保证无死锁的情况发生。</li><li>lock的对象是事务，用来锁定的是数据库中的对象。如表，页，行。并且一般lock的对象仅在事务commit或rollback后进行释放。且有死锁机制</li><li>latch查看 show engine innodb mutex。lock查看 show engine innodb status</li></ul><h4 id="InnoDB存储引擎中的锁"><a href="#InnoDB存储引擎中的锁" class="headerlink" title="InnoDB存储引擎中的锁"></a>InnoDB存储引擎中的锁</h4><ul><li><strong>InnoDB实现了俩种标准的行级锁</strong>：<ul><li>共享锁：允许事务读一行数据 </li><li>排他锁：允许事务删除或者更新一行数据</li></ul></li></ul><p>如果一个事务T1已经获得了行r的共享锁，那么另外的事务T2可以立即获得行的共享锁，称这种情况为锁兼容。但若有其他事务T3想要获得这一行的排他锁，则必须等待事务T1，2释放行上的共享锁称这种情况为锁不兼容。<br>可以总结出：<strong>X锁与任何锁都不兼容。而S锁仅仅和S锁兼容。需注意的是，S和X锁都是行级锁，兼容是指对同一记录锁的兼容性情况</strong>。</p><ul><li><p><strong>意向锁的定义：</strong>InnoDB支持多粒度锁定，这种锁定允许事务在行级上的锁和表级上的锁同时存在。为了支持在不同粒度上的枷锁操作。InnoDB存储引擎支持一种额外的枷锁方式，称为<strong>意向锁</strong>。它是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度上进行加锁。</p></li><li><p>如需要在页上的记录r进行行上X锁，那么分别需要对数据库A、表、页上意向锁IX，最后对记录r上X锁。若其中任何一个部分导致等待，那么该操作需要等待粗粒度的锁完成。举例子说明：在对记录r加X锁之前。已经有事务对表1进行了S表锁，那么表1上一存在S锁，之后事务需要对记录R表1上加上IX，由于不兼容，所以该事务需要等待表锁操作的完成。</p></li><li>InnoDB存储引擎支持意向锁设计比较简练。其意向锁即为表级别的锁，设计的目的主要是为了在事务中揭示下一行将被请求的锁类型。其支持俩种意向锁：<ul><li>意向共享锁<strong>（IS Lock）</strong>，事务想要获得一张表中某几行的共享锁。 </li><li>意向排他锁（<strong>IX Lock</strong>），事务想要获得一张表中某几行的排他锁。</li></ul></li></ul><p>由于InnoDB支持的是<strong>行级别的锁</strong>，因此意向锁其实不会阻塞<strong>除全表扫</strong>以外的任何请求，故而意向锁与行级别锁的兼容性<br><img src="http://jacobs.wanhb.cn/images/lock.png" alt="兼容性"></p><p>意向锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加<strong>排他锁（X)</strong>；对于普通SELECT语句，<strong>InnoDB不会加任何锁</strong>；事务可以通过以下语句显示给记录集加共享锁或排他锁。</p><h4 id="InnoDB行锁实现方式"><a href="#InnoDB行锁实现方式" class="headerlink" title="InnoDB行锁实现方式"></a>InnoDB行锁实现方式</h4><p>InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">只有通过索引条件检索数据，InnoDB才使用行级锁，否则，将使用表级锁</span><br></pre></td></tr></table></figure></p><h4 id="一致性非锁定读"><a href="#一致性非锁定读" class="headerlink" title="一致性非锁定读"></a>一致性非锁定读</h4><p>InnoDB中通过多版本控制的方式来读取当前执行时间数据库中的行的数据。如果读取的行正在执行DELETE或者UPDATE操作，这时读取操作不会因此等待下去，相反，InnoDB存储引擎会去读取行的一个快照数据<strong>。是InnoDB默认的读取方式</strong></p><ul><li>称之为非锁定读是因为不需要等待访问的行上X锁的释放。快照数据是指该行的之前版本的数据；也称作多版本并发控制（MVCC）</li><li>在不同的事务隔离级别Read Committed和Repeatable read下，<strong>InnoDB使用非锁定的一致性读</strong>。但是对于快照的定义却不相同。<strong>在RC下，对于快照数据，总是读取被锁定行最新的一份快照数据，而在RR下，总是读取事务开始时的行数据版本</strong></li><li>快照隔离只是适用于<strong>只读事务</strong>，但是对于<strong>读-写事务</strong>，由于默认是<strong>一致性非锁定读</strong>它却无法解决棘手的<strong>写倾斜问题</strong>（具体定义看《数据密集型应用系统设计》），要想解决写倾斜问题（幻读的一种），还得在读取的时候显式加锁，即<strong>一致性锁定读方式</strong></li></ul><h4 id="一致性锁定读"><a href="#一致性锁定读" class="headerlink" title="一致性锁定读"></a>一致性锁定读</h4><p>在默认的配置下，即事务的隔离级别为Repeatable Read模式下，<strong>InnoDB存储引擎的select操作使用一致性非锁定读</strong>，锁实现采用<strong>next-key-lock算法</strong>解决幻读但是在某些情况下，用户需要显式地加锁来保证数据的一致性。InnoDB存储引擎对于SELECT语句支持俩种一致性的锁定读操作：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select....FOR UPDATE //对行记录加一个X锁，其他事务不能对已锁定的行加上任何锁。</span><br><span class="line">select....LOCK IN SHARE MODE//对读取的行记录加一个S锁，其他事务可以向被锁定的行加S锁，但是如果加上X锁则会被阻塞。</span><br></pre></td></tr></table></figure></p><p>必须在一个事务中，如果事务提交，锁也就释放了。</p><h4 id="自增长与锁"><a href="#自增长与锁" class="headerlink" title="自增长与锁"></a>自增长与锁</h4><h4 id="外键和锁"><a href="#外键和锁" class="headerlink" title="外键和锁"></a>外键和锁</h4><ol><li>外键<ul><li>对于外键列，如果没有显示地给这个列加索引，<strong>则InnoDB存储引擎自动对其加一个索引，因为这样可以避免默认使用表级锁的情况（在博文InnoDB行锁实现方式一小节提到）</strong></li><li>对于外键的插入或更新，首先需要查询父表中的记录，对于父表的SELECT操作，不是采用一致性非锁定读的方式，因为这样会发生数据不一致问题。使用的是Select … lock in share mode方式，即主动对父表加一个S锁。</li></ul></li></ol><h4 id="锁的算法"><a href="#锁的算法" class="headerlink" title="锁的算法"></a>锁的算法</h4><ol><li><p>InnoDB 3种行锁的算法</p><ul><li>RecordLock: 单个行记录上的锁。总会去锁住索引记录，如果InnoDB在建立的时候没有设置任何一个索引，那么这时，InnoDB存储引擎会使用隐士的主键来锁定<ul><li>Gap Lock: 间隙锁，锁定一个范围，但不包含记录本身。目的是为了解决Phantom Problem，即阻止多个事务将记录插入到同一范围内。</li></ul></li><li>Next-Key Lock: Gap Lock+ record Lock,锁定一个范围，并且锁定记录本身。结合了GapLock和RecordLock。在Next-Key Lock算法下，InnoDB对于行的查询都是采用这种锁定算法。例如一个索引有10，11，13，20这四个值，那么该索引可能被Next-Key Locking的区间为： (-8,10)、[10,11)…。</li></ul></li><li><p>Next-Key Lock</p><ul><li>采用的锁定技术为Next-Key Locking,锁定的不是单个值，而是一个范围，是谓词索引的一种改进。若事务T1已经通过next-key locking锁定了范围：(10,11]、(11,13] 当插入新的记录12时，则锁定的范围会变成:(10,11]、(11,12],(12,13]。 然而当查询的索引含有<strong>唯一属性</strong>时，InnoDB会对Next-Key Lock进行优化降级为Record Lock，即仅仅锁住索引本身，而不是范围。</li></ul></li><li><p>解决Phantom Problem</p><ul><li><strong>Phantom Problem定义：</strong> 是指在同一事务下，连续执行俩次同样的SQL语句可能导致不同的结果，第二次执行的SQL语句可能会返回之前不存在的行。</li><li><strong>在默认的事务隔离级别下，即REPEATABLE READ下</strong>，InnoDB存储引擎采用Next-Key Locking机制来避免幻象问题。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">举例说明：表t由1，2，5三个值组成，事务T1执行：select * from t where a&gt;2 for update;</span><br><span class="line">这时，T1并没有提交操作，此时结果返回5，与此同时，另一个事务T2插入了4这个值，在执行一遍sql</span><br><span class="line">会返回4，5，即俩次的结果不一致。InnoDB的next-key locking算法避免Phantom Problem,是对（2，+8）这个范围加了X锁，因此对于任何这个范围插入的操作都是不被允许的。</span><br></pre></td></tr></table></figure></li></ol><h4 id="锁问题"><a href="#锁问题" class="headerlink" title="锁问题"></a>锁问题</h4><ol><li><p>脏读</p><ul><li>脏数据和脏页的关系：脏页指的是在缓冲池中已经被修改的页，但是还没有刷新到磁盘中，即数据库实例内存中的页和磁盘的页的数据是不一致的。当然在刷新到磁盘之前，日志都已经被写入到了重做日志文件中，而脏数据是指<strong>事务对缓冲池中的行记录的修改，并且还没有被提交。</strong> 一个事务可以读到另一个事务中为提交的数据，这显然违反了数据库的个隔离性。</li></ul></li><li>不可重复读<ul><li>重点在于修改。是指在<strong>一个事务内</strong>多次读取同一数据集合，在这个事务还没有结束时，另外一个事务也访问该同一数据集合，并做了一些DML操作，因此在第一个事务中的俩次读数据之间，由于第二个事务的修改，导致俩次读取到的结果集合可能出现不一致的情况。</li><li>不可重复读和脏读的区别是，脏读是读到未提交的数据，而不可重复读读到的却是已经提及的数据，但是其违反了数据库事务一致性的要求。</li><li>一般来说不可重复读还是可以接受的，不少数据库厂商（Oracle、MicroSoft SQL SERVER）将其数据库事务的默认隔离级别设置为Read COMMITTED</li><li>InnoDB存储引擎中，通过使用<strong>Next-Key Lock</strong>算法来避免不可重复读的问题。Mysql官方文档中将不可重复读的问题定义为Phantom Problem，即幻象问题</li></ul></li><li><p>幻读</p><ul><li>重点在于新增或删除。当事务不是独立执行时发生的一种现象，第一个事务对一个表中的数据进行了修改，涉及到全部的数据行。同时第二个事务也修改了这个表中的数据，这种修改是向表中插入一行新数据。那么之后第一个事务重新读取数据的时候就会出现幻读的情况</li><li><strong>Next-Key Lock</strong>算法解决幻读问题</li></ul></li><li><p>丢失更新</p><ul><li><p>丢失更新是另一个锁导致的问题，简单来说就是一个事务的更新操作会被另外一个事务的更新操作所覆盖，从而导致数据的不一致。</p><ul><li>1）事务T1将行记录r更新为v1,但是事务T1并未提交</li><li>2）与此同时事务T2将行记录更新为V2，事务T2未提交</li><li>3）事务T1提交</li><li>4）事务T2提交 </li></ul><p>但是在任何隔离级别下，都不会导致数据库理论意义上的丢失更新问题。但是生产应用中还有一种逻辑意义的丢失更新，而导致该问题的并不是因为数据库本身的问题。</p><ul><li><p>实际上，在所有多用户计算机系统环境下都有可能产生这个问题：</p></li><li><p>1) 事务T1查询一行数据，放入本地内存，并显示给一个终端用户User1</p></li><li>2) 事务T2也查询该行数据，放入本地内存，并显示给一个终端用户User2</li><li>3) user1修改该行记录，更新数据库并提交</li><li>4) User2也修改该行记录，更新数据库并提交</li></ul><p>要避免丢失更新的发生，就需要让事务在这种情况下的<strong>操作变成串行化，而不是并行操作</strong>。于是在步骤一的过程中，用户读取数据加上一个<strong>排他锁</strong>，这样用户2读取的时候也必须加上排他锁，否则就等待锁的释放。</p></li></ul></li><li><p>阻塞</p><ul><li>因为不同锁之间的兼容性关系，在有些时刻一个事务中的锁需要等待另一个事务中的锁释放它所占用的资源，这就是阻塞。阻塞不是一件坏事，是为了确保事务可以并发且正常地运行。</li><li><p>在InnoDB中，参数 <strong>innodb_lock_wait_timeout</strong>用来控制等待的时间<strong>（默认是50秒）</strong>，innodb_rollback_on_timeout用来设定是否在等待超时时对进行中的事务进行回滚操作，默认不回滚。可以通过代码调整： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set @@innodb_lock_wait_time=60</span><br><span class="line">innodb_rollback_on_timeout是静态的，不可在启动时进行修改。</span><br><span class="line">默认情况下，innoDB存储引擎不会回滚超时引发的错误异常，其实InnoDB存储引擎在大部分情况下，都不会对异常进行回滚。</span><br></pre></td></tr></table></figure></li></ul></li></ol><h4 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h4><ol><li>死锁的概念</li><li>死锁的概率</li><li>死锁的示例</li><li>锁升级</li></ol>]]></content>
      
      
        <tags>
            
            <tag> InnoDB </tag>
            
            <tag> mysql </tag>
            
            <tag> 成长 </tag>
            
            <tag> 读书 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Spring源码解析--事务处理</title>
      <link href="/2017/02/12/Spring%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/"/>
      <url>/2017/02/12/Spring%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/</url>
      <content type="html"><![CDATA[<h3 id="事务处理相关类的层次结构"><a href="#事务处理相关类的层次结构" class="headerlink" title="事务处理相关类的层次结构"></a>事务处理相关类的层次结构</h3><p><img src="http://jacobs.wanhb.cn/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-02-11%20%E4%B8%8B%E5%8D%887.54.25.png" alt="事务处理相关类的层次结构"></p><p>在 Spring事务处理中，可以通过设计一个TransactionProxyFactoryBean来使用AOP功能，通过它可以生成Proxy代理对象。在代理对象中，通过TranscationInterceptor来完成对代理对象方法的拦截。实现声明式事务处理时，是AOP和IOC集成的部分，而对于具体的事物处理实现，是通过设计PlatformTransactionManager，AbstractPlatforTransactionmanager以及一系列具体事务处理器来实现的。PlatformTransactionManager又实现了TransactionInterceptor，这样就能将一系列处理给串联起来。</p><h3 id="Spring声明式事务处理"><a href="#Spring声明式事务处理" class="headerlink" title="Spring声明式事务处理"></a>Spring声明式事务处理</h3><h4 id="设计原理与过程"><a href="#设计原理与过程" class="headerlink" title="设计原理与过程"></a>设计原理与过程</h4><p>在实现声明式的事务处理时，常用的方式是结合IOC容器和Spring已有的TransactionProxyFactoryBean对事务管理进行配置，实现可分为以下几个步骤：</p><ul><li>读取和处理在IOC容器中配置的事务处理属性，并转化为Spring事务处理需要的内部数据结构。</li><li>Spring事务处理模块实现统一的事务处理过程。</li><li>底层的事务处理实现。Spring委托给具体的事务处理器来完成。</li></ul><p><img src="http://jacobs.wanhb.cn/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-02-11%20%E4%B8%8B%E5%8D%888.23.09.png" alt="建立事务处理对象时序图"></p><p>从TransactionProxyFactoryBean入手，通过代码来了解Spring是如何通过AOP功能来完成事务管理配置的，从图中可以看到Spring为声明式事务处理的实现所做的一些准备工作：包括为AOP配置基础设施，这些基础设施包括设置拦截器TransactionInterceptor、通过DefaultPointcutAdvisor或TransactionAttributeSourceAdvisor。同时，在TransactionProxyFactoryBean的实现中，还可以看到注入进来的PlatformTransactionManager和事务处理属性TransactionAttribute等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">public class TransactionProxyFactoryBean extends AbstractSingletonProxyFactoryBean implements BeanFactoryAware &#123;</span><br><span class="line">  private final TransactionInterceptor transactionInterceptor = new TransactionInterceptor();／／这个拦截器通过AOP发挥作用，通过这个拦截器的实现，Spring封装了事务处理实现</span><br><span class="line">  private Pointcut pointcut;</span><br><span class="line"></span><br><span class="line">  public TransactionProxyFactoryBean() &#123;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void setTransactionManager(PlatformTransactionManager transactionManager) &#123;</span><br><span class="line">    this.transactionInterceptor.setTransactionManager(transactionManager);</span><br><span class="line">  &#125;</span><br><span class="line">  //通过依赖注入的事务属性以Properties的形式出现，把BeanDefinition中读到的事务管理的属性信息注入到TransactionInterceptor中</span><br><span class="line">  public void setTransactionAttributes(Properties transactionAttributes) &#123;</span><br><span class="line">    this.transactionInterceptor.setTransactionAttributes(transactionAttributes);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void setTransactionAttributeSource(TransactionAttributeSource transactionAttributeSource) &#123;</span><br><span class="line">    this.transactionInterceptor.setTransactionAttributeSource(transactionAttributeSource);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void setPointcut(Pointcut pointcut) &#123;</span><br><span class="line">    this.pointcut = pointcut;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void setBeanFactory(BeanFactory beanFactory) &#123;</span><br><span class="line">    this.transactionInterceptor.setBeanFactory(beanFactory);</span><br><span class="line">  &#125;</span><br><span class="line">//这里创建Spring  AOP对事务处理的Advisor</span><br><span class="line">  protected Object createMainInterceptor() &#123;</span><br><span class="line">    this.transactionInterceptor.afterPropertiesSet();//事务处理完成AOP配置的地方</span><br><span class="line">    return this.pointcut != null?new DefaultPointcutAdvisor(this.pointcut, this.transactionInterceptor):new TransactionAttributeSourceAdvisor(this.transactionInterceptor);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  protected void postProcessProxyFactory(ProxyFactory proxyFactory) &#123;</span><br><span class="line">    proxyFactory.addInterface(TransactionalProxy.class);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完成了AOP配置，Spring的TransactionInterceptor配置是IOC容器完成Bean的依赖注入时，通过initializeBean方法被调用。</p><p>   在建立TransactionProxyFactoryBean的事务处理拦截器的时候， afterPropertiesSet方法首先对 ProxyFactoryBean的目标Bean设置进行检查，如果这个目标Bean的设置是正确的，就会创建ProxyFactory对象，从而实现AOP的使用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void afterPropertiesSet() &#123;</span><br><span class="line">  if(this.getTransactionManager() == null &amp;&amp; this.beanFactory == null) &#123;</span><br><span class="line">    throw new IllegalStateException(&quot;Set the \&apos;transactionManager\&apos; property or make sure to run within a BeanFactory containing a PlatformTransactionManager bean!&quot;);</span><br><span class="line">  &#125; else if(this.getTransactionAttributeSource() == null) &#123;</span><br><span class="line">    throw new IllegalStateException(&quot;Either \&apos;transactionAttributeSource\&apos; or \&apos;transactionAttributes\&apos; is required: If there are no transactional methods, then don\&apos;t use a transaction aspect.&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="事务处理配置的读入"><a href="#事务处理配置的读入" class="headerlink" title="事务处理配置的读入"></a>事务处理配置的读入</h4><p>在AOP配置完成的基础上，以TransactionAttributeSourceAdvisor的实现为入口，了解具体的事务属性配置是如何读入的，实现如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">private TransactionInterceptor transactionInterceptor;／／同样需要AOP中用到的Interceptro和Pointcut，通过内部类，调用TransactionInterceptor来得到事务的配置属性，在对Proxy的方法进行匹配调用时，会使用到这些配置属性。</span><br><span class="line">private final TransactionAttributeSourcePointcut pointcut = new TransactionAttributeSourcePointcut() &#123;</span><br><span class="line">  protected TransactionAttributeSource getTransactionAttributeSource() &#123;</span><br><span class="line">    return TransactionAttributeSourceAdvisor.this.transactionInterceptor != null?TransactionAttributeSourceAdvisor.this.transactionInterceptor.getTransactionAttributeSource():null;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>在声明式事务处理中，通过对目标对象的方法调用进行拦截实现，这个拦截通过AOP发挥作用。在AOP中，对于拦截的启动，首先需要对方法调用是否需要拦截进行判断，依据时那些在TransactionProxyFactoryBean中为目标对象设置的事务属性。这个匹配判断在TransactionAttributeSourcePointcut中完成。实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123;</span><br><span class="line">  if(TransactionalProxy.class.isAssignableFrom(targetClass)) &#123;</span><br><span class="line">    return false;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    TransactionAttributeSource tas = this.getTransactionAttributeSource();</span><br><span class="line">    return tas == null || tas.getTransactionAttribute(method, targetClass) != null;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在方法中，首先把事务方法的属性配置读取到TransactionAttributeSource对象中，有了这些事务处理的配置以后，根据当前方法调用的method对象和目标对象，对是否需要启动事务处理拦截器进行判断。</p><p>在Pointcut的matches判断过程中，会用到transactionAttributeSource对象，这个transactionAttributeSource对象是在对TransactionInterceptor进行依赖注入时就配置好的，它的设置是在TransactionInterceptor的基类TransactionAspectSupport中完成的。配置的是一个NameMatchTransactionAttributeSouce对象。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public void setTransactionAttributes(Properties transactionAttributes) &#123;</span><br><span class="line">  NameMatchTransactionAttributeSource tas = new NameMatchTransactionAttributeSource();</span><br><span class="line">  tas.setProperties(transactionAttributes);</span><br><span class="line">  this.transactionAttributeSource = tas;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可知，NameMatchTransactionAttributeSouce作为TransacionAttributeSource的具体实现，是实际完成事务处理属性读入和匹配的地方。对于NameMatchTransactionAttributeSouce是怎样实现事务处理属性的读入和匹配的，可看如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">public void setProperties(Properties transactionAttributes) &#123;//设置配置的事务方法</span><br><span class="line">  TransactionAttributeEditor tae = new TransactionAttributeEditor();</span><br><span class="line">  Enumeration propNames = transactionAttributes.propertyNames();</span><br><span class="line"></span><br><span class="line">  while(propNames.hasMoreElements()) &#123;</span><br><span class="line">    String methodName = (String)propNames.nextElement();</span><br><span class="line">    String value = transactionAttributes.getProperty(methodName);</span><br><span class="line">    tae.setAsText(value);</span><br><span class="line">    TransactionAttribute attr = (TransactionAttribute)tae.getValue();</span><br><span class="line">    this.addTransactionalMethod(methodName, attr);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">private Map&lt;String, TransactionAttribute&gt; nameMap = new HashMap();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public void addTransactionalMethod(String methodName, TransactionAttribute attr) &#123;</span><br><span class="line">  if(logger.isDebugEnabled()) &#123;</span><br><span class="line">    logger.debug(&quot;Adding transactional method [&quot; + methodName + &quot;] with attribute [&quot; + attr + &quot;]&quot;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  this.nameMap.put(methodName, attr);</span><br><span class="line">&#125;</span><br><span class="line">／／对调用的方法进行判断，判断它是否是事务方法，如果是，那么取出相应的事务配置属性</span><br><span class="line">public TransactionAttribute getTransactionAttribute(Method method, Class&lt;?&gt; targetClass) &#123;</span><br><span class="line">  if(!ClassUtils.isUserLevelMethod(method)) &#123;</span><br><span class="line">    return null;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    String methodName = method.getName();／／判断当前目标调用的方法与配置的事务方法是否直接匹配</span><br><span class="line">    TransactionAttribute attr = (TransactionAttribute)this.nameMap.get(methodName);</span><br><span class="line">    if(attr == null) &#123;//如果不能直接匹配，就通过调用PatternMatchUtils的simpleMatch方法来进行匹配判断。</span><br><span class="line">      String bestNameMatch = null;</span><br><span class="line">      Iterator var6 = this.nameMap.keySet().iterator();</span><br><span class="line"></span><br><span class="line">      while(true) &#123;</span><br><span class="line">        String mappedName;</span><br><span class="line">        do &#123;</span><br><span class="line">          do &#123;</span><br><span class="line">            if(!var6.hasNext()) &#123;</span><br><span class="line">              return attr;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            mappedName = (String)var6.next();</span><br><span class="line">          &#125; while(!this.isMatch(methodName, mappedName));</span><br><span class="line">        &#125; while(bestNameMatch != null &amp;&amp; bestNameMatch.length() &gt; mappedName.length());</span><br><span class="line"></span><br><span class="line">        attr = (TransactionAttribute)this.nameMap.get(mappedName);</span><br><span class="line">        bestNameMatch = mappedName;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return attr;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">／／事务方法的匹配判断，详细的匹配过程在PatternMatchUtils中实现</span><br><span class="line">protected boolean isMatch(String methodName, String mappedName) &#123;</span><br><span class="line">  return PatternMatchUtils.simpleMatch(mappedName, methodName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="事务处理拦截器的设计与实现："><a href="#事务处理拦截器的设计与实现：" class="headerlink" title="事务处理拦截器的设计与实现："></a>事务处理拦截器的设计与实现：</h4><p>经过TransactionProxyFactoryBean的AOP包装，此时如果对目标对象进行方法调用，起作用的对象实际傻姑娘是一个Proxy代理对象。对目标对象方法的调用，不会直接作用在TransactionProxyFactoryBean设置的目标对象上。而是会被设置的事务处理器拦截。而在TransactionProxyFactoryBean的AOP实现中，获取Proxy对象的过程并不复杂，TransactionProxyFactoryBean作为一个FactoryBean，对Bean对象的引用通过getObejct方法来得到的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public Object getObject() &#123; ／／TransactionProxyFactoryBean 的父类 AbstractSingletonProxyFactoryBean中</span><br><span class="line">／／返回的是一个Proxy，是ProxyFactory生成的AOP代理，已经封装了对事务处理的拦截器设置</span><br><span class="line">  if(this.proxy == null) &#123;</span><br><span class="line">    throw new FactoryBeanNotInitializedException();</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    return this.proxy;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于AOP代理对象的作用方法入口，我们一般都知道invoke方法，这个invke方法在事务处理拦截器TransactionInterceptor中，实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">public Object invoke(final MethodInvocation invocation) throws Throwable &#123;</span><br><span class="line">  Class targetClass = invocation.getThis() != null?AopUtils.getTargetClass(invocation.getThis()):null;</span><br><span class="line">  return this.invokeWithinTransaction(invocation.getMethod(), targetClass, new InvocationCallback() &#123;</span><br><span class="line">    public Object proceedWithInvocation() throws Throwable &#123;</span><br><span class="line">      return invocation.proceed();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line">protected Object invokeWithinTransaction(Method method, Class&lt;?&gt; targetClass, final TransactionAspectSupport.InvocationCallback invocation) throws Throwable &#123;</span><br><span class="line">  final TransactionAttribute txAttr = this.getTransactionAttributeSource().getTransactionAttribute(method, targetClass);／／这里读取事务的属性配置，通过TransactionAttributeSource对象取得</span><br><span class="line">  final PlatformTransactionManager tm = this.determineTransactionManager(txAttr);／／根据TransactionProxyFactoryBean的配置信息获得具体的事务处理器</span><br><span class="line">  final String joinpointIdentification = this.methodIdentification(method, targetClass, txAttr);</span><br><span class="line">  if(txAttr != null &amp;&amp; tm instanceof CallbackPreferringPlatformTransactionManager) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      Object ex1 = ((CallbackPreferringPlatformTransactionManager)tm).execute(txAttr, new TransactionCallback() &#123;</span><br><span class="line">        public Object doInTransaction(TransactionStatus status) &#123;</span><br><span class="line">          TransactionAspectSupport.TransactionInfo txInfo = TransactionAspectSupport.this.prepareTransactionInfo(tm, txAttr, joinpointIdentification, status);／／创建事务，同时把事务过程中得到的信息放到TransactionInfo中去，TransactionInfo是保存当前事务状态的对象。</span><br><span class="line"></span><br><span class="line">          TransactionAspectSupport.ThrowableHolder var4;</span><br><span class="line">          try &#123;</span><br><span class="line">            Object ex = invocation.proceedWithInvocation();</span><br><span class="line">            return ex;</span><br><span class="line">          &#125; catch (Throwable var8) &#123;</span><br><span class="line">            if(txAttr.rollbackOn(var8)) &#123;</span><br><span class="line">              if(var8 instanceof RuntimeException) &#123;</span><br><span class="line">                throw (RuntimeException)var8;</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">              throw new TransactionAspectSupport.ThrowableHolderException(var8);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            var4 = new TransactionAspectSupport.ThrowableHolder(var8);</span><br><span class="line">          &#125; finally &#123;</span><br><span class="line">            TransactionAspectSupport.this.cleanupTransactionInfo(txInfo);</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          return var4;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      if(ex1 instanceof TransactionAspectSupport.ThrowableHolder) &#123;</span><br><span class="line">        throw ((TransactionAspectSupport.ThrowableHolder)ex1).getThrowable();</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        return ex1;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; catch (TransactionAspectSupport.ThrowableHolderException var14) &#123;</span><br><span class="line">      throw var14.getCause();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    TransactionAspectSupport.TransactionInfo ex = this.createTransactionIfNecessary(tm, txAttr, joinpointIdentification);</span><br><span class="line">    Object retVal = null;</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">      retVal = invocation.proceedWithInvocation();／／这里的调用使处理沿着拦截器链进行，使最后目标对象的方法得到调用</span><br><span class="line">    &#125; catch (Throwable var15) &#123;</span><br><span class="line">      this.completeTransactionAfterThrowing(ex, var15);／／如果事务处理方法中调用出现了异常，事务处理如何进行需要根据具体情况考虑是否会滚或者提交</span><br><span class="line">      throw var15;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      this.cleanupTransactionInfo(ex);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    this.commitTransactionAfterReturning(ex);//这里通过事务处理器来对事务进行提交</span><br><span class="line">    return retVal;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于Spring而言，事务管理实际上是通过一个TransactionInfo对象来完成的，在该对象中，封装了事务对象和事务处理的状态信息，这是事务处理的抽象。在这一步完成以后，会对拦截器链进行处理，因为有可能在该事务对象中还配置了除事务处理AOP之外的其他拦截器，在结束对拦截器链处理之后，会对 TransactionInfo中的信息进行更新，以反映最近的事务处理情况，在这个时候，也就完成了事务提交的准备，通过调用事务处理器PlatformTransactionManager的commitTransactionAfterReturning方法来完成事务的提交。这个提交的处理过程已经封装在事务处理器中了，而与具体数据源相关的处理过程，最终委托给相关的事务处理器完成，如：DataSourceTransactionManager、HibernateTransactionManager等。</p><p><img src="http://jacobs.wanhb.cn/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-02-11%20%E4%B8%8B%E5%8D%8810.51.15.png" alt="事务提交时序图"></p><p>这个invoke方法的实现中，可以看到整个事务处理在AOP拦截器中实现的全过程。同时，它也是Spring采用AOP封装事务处理和实现声明式事务处理的核心部分。</p><h3 id="Spring事务处理的设计与实现"><a href="#Spring事务处理的设计与实现" class="headerlink" title="Spring事务处理的设计与实现"></a>Spring事务处理的设计与实现</h3><h4 id="Spring事务传播属性"><a href="#Spring事务传播属性" class="headerlink" title="Spring事务传播属性"></a>Spring事务传播属性</h4><blockquote><p>PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。<br>PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。<br>PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。<br>PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。<br>PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。<br>PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。  PROPAGATION_NESTED –<br>如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。<br>前六个策略类似于EJB CMT，第七个（PROPAGATION_NESTED）是Spring所提供的一个特殊变量。</p></blockquote><h4 id="事务的创建"><a href="#事务的创建" class="headerlink" title="事务的创建"></a>事务的创建</h4><p>声明式事务中，TransactionInterceptor拦截器的invoke方法作为事务处理实现的起点，invoke方法中createTransactionIfNeccessary方法作为事务创建的入口。以下是createTransactionIfNeccessary方法的时序图</p><p><img src="http://jacobs.wanhb.cn/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-02-11%20%E4%B8%8B%E5%8D%8811.09.41.png" alt="createTransactionIfNeccessary方法的时序图"></p><p>在createTransactionIfNeccessary中首先会向AbstractTransactionManager执行getTransaction，这个获取Transaction事务对象的过程，在AbstractTransactionManager中需要对事务不同的情况作出处理，然后创建一个TransactionStatus，并把这个TransactionStatus设置到对应的TransactionInfo中去，同时将TransactionInfo和当前的线程绑定，从而完成事务的创建过程。TransactionStatus和TransactionInfo这俩个对象持有的数据是事务处理器对事务进行处理的主要依据。对这俩个对象的使用贯穿整个事务处理的全过程。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">protected TransactionAspectSupport.TransactionInfo createTransactionIfNecessary(PlatformTransactionManager tm, final TransactionAttribute txAttr, final String joinpointIdentification) &#123;</span><br><span class="line">  if(txAttr != null &amp;&amp; ((TransactionAttribute)txAttr).getName() == null) &#123;</span><br><span class="line">    txAttr = new DelegatingTransactionAttribute((TransactionAttribute)txAttr) &#123;</span><br><span class="line">      public String getName() &#123;</span><br><span class="line">        return joinpointIdentification;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  TransactionStatus status = null;</span><br><span class="line">  if(txAttr != null) &#123;</span><br><span class="line">    if(tm != null) &#123;</span><br><span class="line">      status = tm.getTransaction((TransactionDefinition)txAttr);／／这里使用了定义好的事务方法的配置信息。事务创建由事务处理器来完成，同时返回TransactionStatus来记录当前的事务状态，包括已经创建的事务。</span><br><span class="line">    &#125; else if(this.logger.isDebugEnabled()) &#123;</span><br><span class="line">      this.logger.debug(&quot;Skipping transactional joinpoint [&quot; + joinpointIdentification + &quot;] because no transaction manager has been configured&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return this.prepareTransactionInfo(tm, (TransactionAttribute)txAttr, joinpointIdentification, status);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected TransactionAspectSupport.TransactionInfo prepareTransactionInfo(PlatformTransactionManager tm, TransactionAttribute txAttr, String joinpointIdentification, TransactionStatus status) &#123;</span><br><span class="line">  TransactionAspectSupport.TransactionInfo txInfo = new TransactionAspectSupport.TransactionInfo(tm, txAttr, joinpointIdentification);</span><br><span class="line">  if(txAttr != null) &#123;</span><br><span class="line">    if(this.logger.isTraceEnabled()) &#123;</span><br><span class="line">      this.logger.trace(&quot;Getting transaction for [&quot; + txInfo.getJoinpointIdentification() + &quot;]&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    txInfo.newTransactionStatus(status);</span><br><span class="line">  &#125; else if(this.logger.isTraceEnabled()) &#123;</span><br><span class="line">    this.logger.trace(&quot;Don\&apos;t need to create transaction for [&quot; + joinpointIdentification + &quot;]: This method isn\&apos;t transactional.&quot;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  txInfo.bindToThread();</span><br><span class="line">  return txInfo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>getTansaction实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public final TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException &#123;</span><br><span class="line">       Object transaction = doGetTransaction();</span><br><span class="line"></span><br><span class="line">       // Cache debug flag to avoid repeated checks.</span><br><span class="line">       boolean debugEnabled = logger.isDebugEnabled();</span><br><span class="line"></span><br><span class="line">       if (definition == null) &#123;</span><br><span class="line">              // Use defaults if no transaction definition given.</span><br><span class="line">              definition = new DefaultTransactionDefinition();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       if (isExistingTransaction(transaction)) &#123;</span><br><span class="line">              // Existing transaction found -&gt; check propagation behavior to find out how to behave.</span><br><span class="line">              return handleExistingTransaction(definition, transaction, debugEnabled);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       // Check definition settings for new transaction.</span><br><span class="line">       if (definition.getTimeout() &lt; TransactionDefinition.TIMEOUT_DEFAULT) &#123;</span><br><span class="line">              throw new InvalidTimeoutException(&quot;Invalid transaction timeout&quot;, definition.getTimeout());</span><br><span class="line">       &#125;</span><br><span class="line">／／没有事务存在，需要根据事务传播属性设置来创建事务，这里会看到事务传播属性的设置：mandatory、required required_new nested等</span><br><span class="line">       // No existing transaction found -&gt; check propagation behavior to find out how to proceed.</span><br><span class="line">       if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_MANDATORY) &#123;</span><br><span class="line">              throw new IllegalTransactionStateException(</span><br><span class="line">                            &quot;No existing transaction found for transaction marked with propagation &apos;mandatory&apos;&quot;);</span><br><span class="line">       &#125;</span><br><span class="line">       else if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED ||</span><br><span class="line">                     definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW ||</span><br><span class="line">                     definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123;</span><br><span class="line">              SuspendedResourcesHolder suspendedResources = suspend(null);</span><br><span class="line">              if (debugEnabled) &#123;</span><br><span class="line">                     logger.debug(&quot;Creating new transaction with name [&quot; + definition.getName() + &quot;]: &quot; + definition);</span><br><span class="line">              &#125;</span><br><span class="line">              try &#123;</span><br><span class="line">                     boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER);</span><br><span class="line">                     DefaultTransactionStatus status = newTransactionStatus(</span><br><span class="line">                                   definition, transaction, true, newSynchronization, debugEnabled, suspendedResources);</span><br><span class="line">                     doBegin(transaction, definition);</span><br><span class="line">                     prepareSynchronization(status, definition);</span><br><span class="line">                     return status;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (RuntimeException ex) &#123;</span><br><span class="line">                     resume(null, suspendedResources);</span><br><span class="line">                     throw ex;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (Error err) &#123;</span><br><span class="line">                     resume(null, suspendedResources);</span><br><span class="line">                     throw err;</span><br><span class="line">              &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       else &#123;</span><br><span class="line">              // Create &quot;empty&quot; transaction: no actual transaction, but potentially synchronization.</span><br><span class="line">              if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT &amp;&amp; logger.isWarnEnabled()) &#123;</span><br><span class="line">                     logger.warn(&quot;Custom isolation level specified but no actual transaction initiated; &quot; +</span><br><span class="line">                                   &quot;isolation level will effectively be ignored: &quot; + definition);</span><br><span class="line">              &#125;</span><br><span class="line">              boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS);</span><br><span class="line">              return prepareTransactionStatus(definition, null, true, newSynchronization, debugEnabled, null);</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>handleExsitingTransaction方法是理解Spring事务传播属性的关键：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">* Create a TransactionStatus for an existing transaction.</span><br><span class="line">*/</span><br><span class="line">private TransactionStatus handleExistingTransaction(</span><br><span class="line">              TransactionDefinition definition, Object transaction, boolean debugEnabled)</span><br><span class="line">              throws TransactionException &#123;</span><br><span class="line"></span><br><span class="line">       if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) &#123;</span><br><span class="line">              throw new IllegalTransactionStateException(</span><br><span class="line">                            &quot;Existing transaction found for transaction marked with propagation &apos;never&apos;&quot;);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) &#123;</span><br><span class="line">              if (debugEnabled) &#123;</span><br><span class="line">                     logger.debug(&quot;Suspending current transaction&quot;);</span><br><span class="line">              &#125;</span><br><span class="line">              Object suspendedResources = suspend(transaction);</span><br><span class="line">              boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS);</span><br><span class="line">              return prepareTransactionStatus(</span><br><span class="line">                            definition, null, false, newSynchronization, debugEnabled, suspendedResources);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) &#123;</span><br><span class="line">              if (debugEnabled) &#123;</span><br><span class="line">                     logger.debug(&quot;Suspending current transaction, creating new transaction with name [&quot; +</span><br><span class="line">                                   definition.getName() + &quot;]&quot;);</span><br><span class="line">              &#125;</span><br><span class="line">              SuspendedResourcesHolder suspendedResources = suspend(transaction);</span><br><span class="line">              try &#123;</span><br><span class="line">                     boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER);</span><br><span class="line">                     DefaultTransactionStatus status = newTransactionStatus(</span><br><span class="line">                                   definition, transaction, true, newSynchronization, debugEnabled, suspendedResources);</span><br><span class="line">                     doBegin(transaction, definition);</span><br><span class="line">                     prepareSynchronization(status, definition);</span><br><span class="line">                     return status;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (RuntimeException beginEx) &#123;</span><br><span class="line">                     resumeAfterBeginException(transaction, suspendedResources, beginEx);</span><br><span class="line">                     throw beginEx;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (Error beginErr) &#123;</span><br><span class="line">                     resumeAfterBeginException(transaction, suspendedResources, beginErr);</span><br><span class="line">                     throw beginErr;</span><br><span class="line">              &#125;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123;</span><br><span class="line">              if (!isNestedTransactionAllowed()) &#123;</span><br><span class="line">                     throw new NestedTransactionNotSupportedException(</span><br><span class="line">                                   &quot;Transaction manager does not allow nested transactions by default - &quot; +</span><br><span class="line">                                   &quot;specify &apos;nestedTransactionAllowed&apos; property with value &apos;true&apos;&quot;);</span><br><span class="line">              &#125;</span><br><span class="line">              if (debugEnabled) &#123;</span><br><span class="line">                     logger.debug(&quot;Creating nested transaction with name [&quot; + definition.getName() + &quot;]&quot;);</span><br><span class="line">              &#125;</span><br><span class="line">              if (useSavepointForNestedTransaction()) &#123;／／在Spring管理的事务中，创建事务保存点</span><br><span class="line">                     // Create savepoint within existing Spring-managed transaction,</span><br><span class="line">                     // through the SavepointManager API implemented by TransactionStatus.</span><br><span class="line">                     // Usually uses JDBC 3.0 savepoints. Never activates Spring synchronization.</span><br><span class="line">                     DefaultTransactionStatus status =</span><br><span class="line">                                   prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null);</span><br><span class="line">                     status.createAndHoldSavepoint();</span><br><span class="line">                     return status;</span><br><span class="line">              &#125;</span><br><span class="line">              else &#123;</span><br><span class="line">                     // Nested transaction through nested begin and commit/rollback calls.</span><br><span class="line">                     // Usually only for JTA: Spring synchronization might get activated here</span><br><span class="line">                     // in case of a pre-existing JTA transaction.</span><br><span class="line">                     boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER);</span><br><span class="line">                     DefaultTransactionStatus status = newTransactionStatus(</span><br><span class="line">                                   definition, transaction, true, newSynchronization, debugEnabled, null);</span><br><span class="line">                     doBegin(transaction, definition);</span><br><span class="line">                     prepareSynchronization(status, definition);</span><br><span class="line">                     return status;</span><br><span class="line">              &#125;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       // Assumably PROPAGATION_SUPPORTS or PROPAGATION_REQUIRED.</span><br><span class="line">       if (debugEnabled) &#123;</span><br><span class="line">              logger.debug(&quot;Participating in existing transaction&quot;);</span><br><span class="line">       &#125;</span><br><span class="line">       if (isValidateExistingTransaction()) &#123;</span><br><span class="line">              if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT) &#123;</span><br><span class="line">                     Integer currentIsolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel();</span><br><span class="line">                     if (currentIsolationLevel == null || currentIsolationLevel != definition.getIsolationLevel()) &#123;</span><br><span class="line">                            Constants isoConstants = DefaultTransactionDefinition.constants;</span><br><span class="line">                            throw new IllegalTransactionStateException(&quot;Participating transaction with definition [&quot; +</span><br><span class="line">                                          definition + &quot;] specifies isolation level which is incompatible with existing transaction: &quot; +</span><br><span class="line">                                          (currentIsolationLevel != null ?</span><br><span class="line">                                                        isoConstants.toCode(currentIsolationLevel, DefaultTransactionDefinition.PREFIX_ISOLATION) :</span><br><span class="line">                                                        &quot;(unknown)&quot;));</span><br><span class="line">                     &#125;</span><br><span class="line">              &#125;</span><br><span class="line">              if (!definition.isReadOnly()) &#123;</span><br><span class="line">                     if (TransactionSynchronizationManager.isCurrentTransactionReadOnly()) &#123;</span><br><span class="line">                            throw new IllegalTransactionStateException(&quot;Participating transaction with definition [&quot; +</span><br><span class="line">                                          definition + &quot;] is not marked as read-only but existing transaction is&quot;);</span><br><span class="line">                     &#125;</span><br><span class="line">              &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER);</span><br><span class="line">       return prepareTransactionStatus(definition, transaction, false, newSynchronization, debugEnabled, null);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="事务挂起"><a href="#事务挂起" class="headerlink" title="事务挂起"></a>事务挂起</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">protected final SuspendedResourcesHolder suspend(Object transaction) throws TransactionException &#123;／／返回的SuspendedResourcesHolder会作为参数传给TransactionStatus</span><br><span class="line">       if (TransactionSynchronizationManager.isSynchronizationActive()) &#123;</span><br><span class="line">              List&lt;TransactionSynchronization&gt; suspendedSynchronizations = doSuspendSynchronization();</span><br><span class="line">              try &#123;</span><br><span class="line">                     Object suspendedResources = null;／／把挂起事务的处理交给具体事务处理器去完成，如果具体的事务处理器不支持事务挂起，则默认抛出TransactionSuspensionNotSupportedException</span><br><span class="line">                     if (transaction != null) &#123;</span><br><span class="line">                            suspendedResources = doSuspend(transaction);</span><br><span class="line">                     &#125;//这里在线程中保存与事务处理有关的信息，并重置线程中相关的ThreadLocal变量</span><br><span class="line">                     String name = TransactionSynchronizationManager.getCurrentTransactionName();</span><br><span class="line">                     TransactionSynchronizationManager.setCurrentTransactionName(null);</span><br><span class="line">                     boolean readOnly = TransactionSynchronizationManager.isCurrentTransactionReadOnly();</span><br><span class="line">                     TransactionSynchronizationManager.setCurrentTransactionReadOnly(false);</span><br><span class="line">                     Integer isolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel();</span><br><span class="line">                     TransactionSynchronizationManager.setCurrentTransactionIsolationLevel(null);</span><br><span class="line">                     boolean wasActive = TransactionSynchronizationManager.isActualTransactionActive();</span><br><span class="line">                     TransactionSynchronizationManager.setActualTransactionActive(false);</span><br><span class="line">                     return new SuspendedResourcesHolder(</span><br><span class="line">                                   suspendedResources, suspendedSynchronizations, name, readOnly, isolationLevel, wasActive);</span><br><span class="line">              &#125;</span><br><span class="line">              catch (RuntimeException ex) &#123;</span><br><span class="line">                     // doSuspend failed - original transaction is still active… 如果处理失败，则恢复原始的事务</span><br><span class="line">                     doResumeSynchronization(suspendedSynchronizations);</span><br><span class="line">                     throw ex;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (Error err) &#123;</span><br><span class="line">                     // doSuspend failed - original transaction is still active...</span><br><span class="line">                     doResumeSynchronization(suspendedSynchronizations);</span><br><span class="line">                     throw err;</span><br><span class="line">              &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       else if (transaction != null) &#123;</span><br><span class="line">              // Transaction active but no synchronization active.</span><br><span class="line">              Object suspendedResources = doSuspend(transaction);</span><br><span class="line">              return new SuspendedResourcesHolder(suspendedResources);</span><br><span class="line">       &#125;</span><br><span class="line">       else &#123;</span><br><span class="line">              // Neither transaction nor synchronization active.</span><br><span class="line">              return null;</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Reactivate transaction synchronization for the current thread</span><br><span class="line"> * and resume all given synchronizations.</span><br><span class="line"> * @param suspendedSynchronizations List of TransactionSynchronization objects</span><br><span class="line"> */doSuspend 失败则恢复事务</span><br><span class="line">private void doResumeSynchronization(List&lt;TransactionSynchronization&gt; suspendedSynchronizations) &#123;</span><br><span class="line">       TransactionSynchronizationManager.initSynchronization();／／维护着ThreadLocal变量</span><br><span class="line">       for (TransactionSynchronization synchronization : suspendedSynchronizations) &#123;</span><br><span class="line">              synchronization.resume();</span><br><span class="line">              TransactionSynchronizationManager.registerSynchronization(synchronization);</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="事务的提交"><a href="#事务的提交" class="headerlink" title="事务的提交"></a>事务的提交</h4><p>在声明式事务处理中，事务的提交在TransactionInteceptor的invoke方法中实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">commitTransactionAfterReturning(txInfo)</span><br></pre></td></tr></table></figure></p><p>txInfo是TransactionInfo对象，是创建事务时生成的。同时，Spring的事务管理框架的生成的TransactionStatus对象就包含在TransactionInfo对象中。commitTransactionAfterReturning具体实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">protected void commitTransactionAfterReturning(TransactionAspectSupport.TransactionInfo txInfo) &#123;</span><br><span class="line">  if(txInfo != null &amp;&amp; txInfo.hasTransaction()) &#123;</span><br><span class="line">    if(this.logger.isTraceEnabled()) &#123;</span><br><span class="line">      this.logger.trace(&quot;Completing transaction for [&quot; + txInfo.getJoinpointIdentification() + &quot;]&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    txInfo.getTransactionManager().commit(txInfo.getTransactionStatus());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>调用具体的事务管理器实现。而在事务管理器中的实现在AbstractPlatformTransactionManager中存在一个模版：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">* This implementation of commit handles participating in existing</span><br><span class="line">* transactions and programmatic rollback requests.</span><br><span class="line">* Delegates to &#123;@code isRollbackOnly&#125;, &#123;@code doCommit&#125;</span><br><span class="line">* and &#123;@code rollback&#125;.</span><br><span class="line">* @see org.springframework.transaction.TransactionStatus#isRollbackOnly()</span><br><span class="line">* @see #doCommit</span><br><span class="line">* @see #rollback</span><br><span class="line">*/</span><br><span class="line">@Override</span><br><span class="line">public final void commit(TransactionStatus status) throws TransactionException &#123;</span><br><span class="line">       if (status.isCompleted()) &#123;</span><br><span class="line">              throw new IllegalTransactionStateException(</span><br><span class="line">                            &quot;Transaction is already completed - do not call commit or rollback more than once per transaction&quot;);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status;</span><br><span class="line">       if (defStatus.isLocalRollbackOnly()) &#123;／／如果事务处理过程中发生了异常，调用回滚。</span><br><span class="line">              if (defStatus.isDebug()) &#123;</span><br><span class="line">                     logger.debug(&quot;Transactional code has requested rollback&quot;);</span><br><span class="line">              &#125;</span><br><span class="line">              processRollback(defStatus);</span><br><span class="line">              return;</span><br><span class="line">       &#125;</span><br><span class="line">       if (!shouldCommitOnGlobalRollbackOnly() &amp;&amp; defStatus.isGlobalRollbackOnly()) &#123;</span><br><span class="line">              if (defStatus.isDebug()) &#123;</span><br><span class="line">                     logger.debug(&quot;Global transaction is marked as rollback-only but transactional code requested commit&quot;);</span><br><span class="line">              &#125;／／处理回滚</span><br><span class="line">              processRollback(defStatus);</span><br><span class="line">              // Throw UnexpectedRollbackException only at outermost transaction boundary</span><br><span class="line">              // or if explicitly asked to.</span><br><span class="line">              if (status.isNewTransaction() || isFailEarlyOnGlobalRollbackOnly()) &#123;</span><br><span class="line">                     throw new UnexpectedRollbackException(</span><br><span class="line">                                   &quot;Transaction rolled back because it has been marked as rollback-only&quot;);</span><br><span class="line">              &#125;</span><br><span class="line">              return;</span><br><span class="line">       &#125;</span><br><span class="line">       ／／处理提交入口</span><br><span class="line">       processCommit(defStatus);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可以看出rollback和commit都在这个方法中实现。看看 processCommit的实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">private void processCommit(DefaultTransactionStatus status) throws TransactionException &#123;</span><br><span class="line">       try &#123;</span><br><span class="line">              boolean beforeCompletionInvoked = false;</span><br><span class="line">              try &#123;／／事务的提交准备工作由具体的事务处理器来完成</span><br><span class="line">                     prepareForCommit(status);</span><br><span class="line">                     triggerBeforeCommit(status);</span><br><span class="line">                     triggerBeforeCompletion(status);</span><br><span class="line">                     beforeCompletionInvoked = true;</span><br><span class="line">                     boolean globalRollbackOnly = false;</span><br><span class="line">                     if (status.isNewTransaction() || isFailEarlyOnGlobalRollbackOnly()) &#123;</span><br><span class="line">                            globalRollbackOnly = status.isGlobalRollbackOnly();</span><br><span class="line">                     &#125;／／嵌套事务的处理过程。</span><br><span class="line">                     if (status.hasSavepoint()) &#123;</span><br><span class="line">                            if (status.isDebug()) &#123;</span><br><span class="line">                                   logger.debug(&quot;Releasing transaction savepoint&quot;);</span><br><span class="line">                            &#125;</span><br><span class="line">                            status.releaseHeldSavepoint();</span><br><span class="line">                     &#125;</span><br><span class="line">                     else if (status.isNewTransaction()) &#123;／／根据当前线程中保存的事务状态进行处理，如果当前的事务是一个新的事务，调用具体事务处理器的完成提交，如果当前所持有的事务不是一个新事务，则不提交，由已经存在的事务来完成提交</span><br><span class="line">                            if (status.isDebug()) &#123;</span><br><span class="line">                                   logger.debug(&quot;Initiating transaction commit&quot;);</span><br><span class="line">                            &#125;</span><br><span class="line">                            doCommit(status);</span><br><span class="line">                     &#125;</span><br><span class="line">                     // Throw UnexpectedRollbackException if we have a global rollback-only</span><br><span class="line">                     // marker but still didn&apos;t get a corresponding exception from commit.</span><br><span class="line">                     if (globalRollbackOnly) &#123;</span><br><span class="line">                            throw new UnexpectedRollbackException(</span><br><span class="line">                                          &quot;Transaction silently rolled back because it has been marked as rollback-only&quot;);</span><br><span class="line">                     &#125;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (UnexpectedRollbackException ex) &#123;</span><br><span class="line">                     // can only be caused by doCommit</span><br><span class="line">                     triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK);</span><br><span class="line">                     throw ex;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (TransactionException ex) &#123;</span><br><span class="line">                     // can only be caused by doCommit</span><br><span class="line">                     if (isRollbackOnCommitFailure()) &#123;</span><br><span class="line">                            doRollbackOnCommitException(status, ex);</span><br><span class="line">                     &#125;</span><br><span class="line">                     else &#123;</span><br><span class="line">                            triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN);</span><br><span class="line">                     &#125;</span><br><span class="line">                     throw ex;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (RuntimeException ex) &#123;</span><br><span class="line">                     if (!beforeCompletionInvoked) &#123;</span><br><span class="line">                            triggerBeforeCompletion(status);</span><br><span class="line">                     &#125;</span><br><span class="line">                     doRollbackOnCommitException(status, ex);</span><br><span class="line">                     throw ex;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (Error err) &#123;</span><br><span class="line">                     if (!beforeCompletionInvoked) &#123;</span><br><span class="line">                            triggerBeforeCompletion(status);</span><br><span class="line">                     &#125;</span><br><span class="line">                     doRollbackOnCommitException(status, err);</span><br><span class="line">                     throw err;</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">              // Trigger afterCommit callbacks, with an exception thrown there</span><br><span class="line">              // propagated to callers but the transaction still considered as committed.</span><br><span class="line">              try &#123;</span><br><span class="line">                     triggerAfterCommit(status);</span><br><span class="line">              &#125;</span><br><span class="line">              finally &#123;</span><br><span class="line">                     triggerAfterCompletion(status, TransactionSynchronization.STATUS_COMMITTED);</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">       &#125;</span><br><span class="line">       finally &#123;</span><br><span class="line">              cleanupAfterCompletion(status);</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出，对事务的提交处理都是紧紧围绕TransactionStatus保存的事务处理相关状态进行判断。具体的提交处理过程都设计成抽象方法，交由具体的事务处理器来完成。</p><h4 id="事务的回滚"><a href="#事务的回滚" class="headerlink" title="事务的回滚"></a>事务的回滚</h4><p>在事务的提交方法中看到了事务的回滚入口，即processRollback方法，其实现代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">private void processRollback(DefaultTransactionStatus status) &#123;</span><br><span class="line">       try &#123;</span><br><span class="line">              try &#123;</span><br><span class="line">                     triggerBeforeCompletion(status);</span><br><span class="line">                     if (status.hasSavepoint()) &#123;／／嵌套事务的回滚处理</span><br><span class="line">                            if (status.isDebug()) &#123;</span><br><span class="line">                                   logger.debug(&quot;Rolling back transaction to savepoint&quot;);</span><br><span class="line">                            &#125;</span><br><span class="line">                            status.rollbackToHeldSavepoint();</span><br><span class="line">                     &#125;／／当前事务调用方法中新建事务的回滚处理</span><br><span class="line">                     else if (status.isNewTransaction()) &#123;</span><br><span class="line">                            if (status.isDebug()) &#123;</span><br><span class="line">                                   logger.debug(&quot;Initiating transaction rollback&quot;);</span><br><span class="line">                            &#125;</span><br><span class="line">                            doRollback(status);</span><br><span class="line">                     &#125;／／如果在当前事务调用方法中没有新建事务的回滚处理</span><br><span class="line">                     else if (status.hasTransaction()) &#123;</span><br><span class="line">                            if (status.isLocalRollbackOnly() || isGlobalRollbackOnParticipationFailure()) &#123;</span><br><span class="line">                                   if (status.isDebug()) &#123;</span><br><span class="line">                                          logger.debug(&quot;Participating transaction failed - marking existing transaction as rollback-only&quot;);</span><br><span class="line">                                   &#125;</span><br><span class="line">                                   doSetRollbackOnly(status);</span><br><span class="line">                            &#125;／／由线程的前一个事务来处理回滚，这里不执行任何操作。</span><br><span class="line">                            else &#123;</span><br><span class="line">                                   if (status.isDebug()) &#123;</span><br><span class="line">                                          logger.debug(&quot;Participating transaction failed - letting transaction originator decide on rollback&quot;);</span><br><span class="line">                                   &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                     &#125;</span><br><span class="line">                     else &#123;</span><br><span class="line">                            logger.debug(&quot;Should roll back transaction but cannot - no transaction available&quot;);</span><br><span class="line">                     &#125;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (RuntimeException ex) &#123;</span><br><span class="line">                     triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN);</span><br><span class="line">                     throw ex;</span><br><span class="line">              &#125;</span><br><span class="line">              catch (Error err) &#123;</span><br><span class="line">                     triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN);</span><br><span class="line">                     throw err;</span><br><span class="line">              &#125;</span><br><span class="line">              triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK);</span><br><span class="line">       &#125;</span><br><span class="line">       finally &#123;</span><br><span class="line">              cleanupAfterCompletion(status);</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>显然，看了代码我们很快就能理解，Spring 事务传播属性中的 Required_New和NESTED（嵌套事务）的本质区别</p><ol><li>PROPAGATION_REQUIRES_NEW 启动一个新的, 不依赖于环境的 “内部” 事务. 这个事务将被完全 commited 或 rolled back 而不依赖于外部事务, 它拥有自己的隔离范围, 自己的锁, 等等. 当内部事务开始执行时, 外部事务将被挂起, 内务事务结束时, 外部事务将继续执行. </li><li>另一方面, PROPAGATION_NESTED 开始一个 “嵌套的” 事务,  它是已经存在事务的一个真正的子事务. 潜套事务开始执行时,它将取得一个 savepoint. 如果这个嵌套事务失败,我们将回滚到此savepoint潜套事务是外部事务的一部分,只有外部事务结束后它才会被提交。</li><li>由此可见, PROPAGATION_REQUIRES_NEW 和 PROPAGATION_NESTED 的最大区别在于, PROPAGATION_REQUIRES_NEW 完全是一个新的事务, 而 PROPAGATION_NESTED 则是外部事务的子事务, 如果外部事务 commit, 潜套事务也会被 commit, 这个规则同样适用于 roll back. </li></ol><p>也就是说：</p><ol><li>PROPAGATION_REQUIRES_NEW事务不受外部事务的影响，是隔离的。</li><li>PROPAGATION_NESTED，如果内部事务失败且内部，它会回到savepoint之前的状态不会产生脏数据，而外部事务catch住异常后可以选择回滚或者提交；如果外部事务失败，由于嵌套事务是外部事务的一部分，则会导致外部事务与嵌套事务一起回滚。</li></ol>]]></content>
      
      
        <tags>
            
            <tag> spring </tag>
            
            <tag> ioc </tag>
            
            <tag> aop </tag>
            
            <tag> 源码 </tag>
            
            <tag> 事务处理 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Spring源码解读(-)</title>
      <link href="/2017/02/07/Spring%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
      <url>/2017/02/07/Spring%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
      <content type="html"><![CDATA[<h3 id="IOC"><a href="#IOC" class="headerlink" title="IOC"></a>IOC</h3><h4 id="设计理念"><a href="#设计理念" class="headerlink" title="设计理念"></a><strong>设计理念</strong></h4><p>先来看看接口设计预览图：<br><img src="http://jacobs.wanhb.cn/images/ioc1.jpg" alt="IOC接口设计规范"></p><p> bean 实例化总体步骤<br><img src="http://jacobs.wanhb.cn/images/bean%20initialization.jpg" alt="bean initialization"></p><h4 id="资源定位与注册"><a href="#资源定位与注册" class="headerlink" title="资源定位与注册"></a>资源定位与注册</h4><p>容器的初始化过程是IOC实现的入口，过程如下：</p><ol><li>Resource定位。BeanDefitioin的资源定位，由ResourceLoader通过统一的Resource接口完成<br>，这个Resource对各种形式的BeanDefinition使用都提供了统一接口。</li><li>BeanDefition的载入。这个载入过程是用户定义好的Bean表示成IOC容器内部的数据结构，而这个容器的数据结构就是BeanDefition。BeanDefition实际上就是POJO对象在IOC容器中的抽象，通过这个BeanDefition定义的数据结构，使得IOC容器能够方便地对POJO对象也就是Bean进行管理。</li><li>向IOC容器注册这些BeanDefition的过程。调用BeanDefitionRegistry接口的实现来完成。把解析得到的BeanDefition向容器中进行注册。在IOC内部将BeanDefition注入到一个HasMap中去(BeanDefitionHolder),IOC容器就是通过这个HashMap来持有这些BeanDefition数据的。</li><li><strong>值得注意：</strong>容器初始化过程不包括依赖注入的实现，Bean定义的载入和依赖注入是俩个独立的过程。依赖注入一般发生在第一次getBean的时候或者通过设置lazyinit实现预先注入Bean。 </li></ol><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><ul><li><p>AbstractApplicationContext定义了基本的refresh方法，其他的由子类去实现扩展,即AbstractApplicationContext是容器初始化的入口</p></li><li><p>DefaultListableBeanFactory是IOC容器的基础，FileSystemXmlApplicationContext、WebXmlApplicationContext都是建立在DefaultListableBeanFactory之上，实现自定义BeanDefinition的载入方式</p></li></ul><h4 id="依赖注入"><a href="#依赖注入" class="headerlink" title="依赖注入"></a>依赖注入</h4><p><img src="http://jacobs.wanhb.cn/images/ioc3.jpg" alt="注入过程"></p><ul><li>初始化过程完成的主要工作是在IOC容器中建立BeanDefinition数据映射。此过程中并没有实现IOC容器对Bean依赖关系进行注入。</li><li>对于依赖注入，其触发条件是用户第一次向IOC容器索要Bean时触发的。当然也可以通过控制lazy-init属性来让容器完成对bean的预实例化。</li><li><strong>依赖注入的起点：</strong> IOC容器接口BeanFactory中定义了一个getBean接口，这个接口的实现就是触发依赖注入的地方。可以在DefaultListableBeanFactory的类AbstractBeanFactory入手看看getBean的实现。</li><li>SimpleInstsntiationStrategy类，这个Strategy是Spring用来生成Bean对象的默认类，提供了俩种实例化Java对象的方法，一种是通过BeanUtils，它使用了JVM的反射功能，一种是CGLIB来生成，代码如下:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public Object instantiate(RootBeanDefinition bd, String beanName, BeanFactory owner) &#123;</span><br><span class="line">// Don&apos;t override the class with CGLIB if no overrides.</span><br><span class="line">if (bd.getMethodOverrides().isEmpty()) &#123;</span><br><span class="line">Constructor&lt;?&gt; constructorToUse;</span><br><span class="line">synchronized (bd.constructorArgumentLock) &#123;</span><br><span class="line">constructorToUse = (Constructor&lt;?&gt;) bd.resolvedConstructorOrFactoryMethod;</span><br><span class="line">if (constructorToUse == null) &#123;</span><br><span class="line">final Class&lt;?&gt; clazz = bd.getBeanClass();</span><br><span class="line">if (clazz.isInterface()) &#123;</span><br><span class="line">throw new BeanInstantiationException(clazz, &quot;Specified class is an interface&quot;);</span><br><span class="line">&#125;</span><br><span class="line">try &#123;</span><br><span class="line">if (System.getSecurityManager() != null) &#123;</span><br><span class="line">constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Constructor&lt;?&gt;&gt;() &#123;</span><br><span class="line">@Override</span><br><span class="line">public Constructor&lt;?&gt; run() throws Exception &#123;</span><br><span class="line">return clazz.getDeclaredConstructor((Class[]) null);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">&#125;</span><br><span class="line">else &#123;</span><br><span class="line">constructorToUse =clazz.getDeclaredConstructor((Class[]) null);</span><br><span class="line">&#125;</span><br><span class="line">bd.resolvedConstructorOrFactoryMethod = constructorToUse;</span><br><span class="line">&#125;</span><br><span class="line">catch (Throwable ex) &#123;</span><br><span class="line">throw new BeanInstantiationException(clazz, &quot;No default constructor found&quot;, ex);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">return BeanUtils.instantiateClass(constructorToUse);</span><br><span class="line">&#125;</span><br><span class="line">else &#123;</span><br><span class="line">// Must generate CGLIB subclass.</span><br><span class="line">return instantiateWithMethodInjection(bd, beanName, owner);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p><strong>依赖注入最终步骤</strong>：在实例化Bean对象生成的基础上，对于Bean对象生成以后，怎样对这些Bean对象的依赖关系处理好，完成整个依赖注入过程？即通过populateBean方法完成，这个方法在AbstractAutowireCapableBeanFactory中实现，AutoWire的依赖注入等，都做了集中处理。</p></li><li><p><strong>Bean的初始化(InitializeBean)：</strong>：在initializeBean方法中，需要使用Bean的名字，完成依赖注入以后的Bean对象，以及这个Bean对应的BeanDefinition。然后开始初始化工作：</p><ol><li>为类型是BeanNameAware的Bean设置Bean的名字</li><li>为类型是BeanClassLoaderAware的Bean设置类装载器，</li><li>类型是BeanFactoryAware的Bean设置自身所在的IOC容器以供回调使用，对PostProcessBeforeInitialization/postAfterInitialization的回调和初始化属性init-method的处理等。</li><li>最后，就可以正常的使用由IOC容器托管的Bean了</li></ol></li></ul><h4 id="讲讲预先注入"><a href="#讲讲预先注入" class="headerlink" title="讲讲预先注入"></a>讲讲预先注入</h4><p>我们讲过依赖注入一般发生在用户第一次请求，但是也可以设置lazy-init属性实现预先依赖注入。这部分过程依然属于AbstractApplicationContext的 refresh方法中。在finishBeanFactoryInitialization的方法中，封装了lazy-init属性的处理，实际的处理是在DefaultListableBeanFactory这个基本容器的preInstantiateSingletons方法中完成的。该方法对单件Bean完成预先实例化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">public void refresh() throws BeansException, IllegalStateException &#123;</span><br><span class="line">   Object var1 = this.startupShutdownMonitor;</span><br><span class="line">   synchronized(this.startupShutdownMonitor) &#123;</span><br><span class="line">     this.prepareRefresh();</span><br><span class="line">     ConfigurableListableBeanFactory beanFactory = this.obtainFreshBeanFactory();</span><br><span class="line">     this.prepareBeanFactory(beanFactory);</span><br><span class="line"></span><br><span class="line">     try &#123;</span><br><span class="line">       this.postProcessBeanFactory(beanFactory);</span><br><span class="line">       this.invokeBeanFactoryPostProcessors(beanFactory);</span><br><span class="line">       this.registerBeanPostProcessors(beanFactory);</span><br><span class="line">       this.initMessageSource();</span><br><span class="line">       this.initApplicationEventMulticaster();</span><br><span class="line">       this.onRefresh();</span><br><span class="line">       this.registerListeners();</span><br><span class="line">       //预先实例化入口</span><br><span class="line">       this.finishBeanFactoryInitialization(beanFactory);</span><br><span class="line">       this.finishRefresh();</span><br><span class="line">     &#125; catch (BeansException var9) &#123;</span><br><span class="line">       if(this.logger.isWarnEnabled()) &#123;</span><br><span class="line">         this.logger.warn(&quot;Exception encountered during context initialization - cancelling refresh attempt: &quot; + var9);</span><br><span class="line">       &#125;</span><br><span class="line">       this.destroyBeans();</span><br><span class="line">       this.cancelRefresh(var9);</span><br><span class="line">       throw var9;</span><br><span class="line">     &#125; finally &#123;</span><br><span class="line">       this.resetCommonCaches();</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> //在finishBeanFactoryInitialization方法中进行具体的处理过程</span><br><span class="line"> protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123;</span><br><span class="line">   ...</span><br><span class="line">   beanFactory.setTempClassLoader((ClassLoader)null);</span><br><span class="line">   beanFactory.freezeConfiguration();</span><br><span class="line">   beanFactory.preInstantiateSingletons();</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line">public void preInstantiateSingletons() throws BeansException &#123;</span><br><span class="line">List&lt;String&gt; beanNames = new ArrayList&lt;String&gt;(this.beanDefinitionNames);</span><br><span class="line"></span><br><span class="line">// Trigger initialization of all non-lazy singleton beans...</span><br><span class="line">for (String beanName : beanNames) &#123;</span><br><span class="line">RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName);</span><br><span class="line">if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123;</span><br><span class="line">if (isFactoryBean(beanName)) &#123;</span><br><span class="line">final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName);</span><br><span class="line">boolean isEagerInit;</span><br><span class="line">if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123;</span><br><span class="line">isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() &#123;</span><br><span class="line">@Override</span><br><span class="line">public Boolean run() &#123;</span><br><span class="line">return ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit();</span><br><span class="line">&#125;</span><br><span class="line">&#125;, getAccessControlContext());</span><br><span class="line">&#125;</span><br><span class="line">else &#123;</span><br><span class="line">isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp;</span><br><span class="line">((SmartFactoryBean&lt;?&gt;) factory).isEagerInit());</span><br><span class="line">&#125;</span><br><span class="line">if (isEagerInit) &#123;</span><br><span class="line">getBean(beanName);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">else &#123;</span><br><span class="line">getBean(beanName);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Trigger post-initialization callback for all applicable beans...</span><br><span class="line">for (String beanName : beanNames) &#123;</span><br><span class="line">Object singletonInstance = getSingleton(beanName);</span><br><span class="line">if (singletonInstance instanceof SmartInitializingSingleton) &#123;</span><br><span class="line">final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance;</span><br><span class="line">if (System.getSecurityManager() != null) &#123;</span><br><span class="line">AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123;</span><br><span class="line">@Override</span><br><span class="line">public Object run() &#123;</span><br><span class="line">smartSingleton.afterSingletonsInstantiated();</span><br><span class="line">return null;</span><br><span class="line">&#125;</span><br><span class="line">&#125;, getAccessControlContext());</span><br><span class="line">&#125;</span><br><span class="line">else &#123;</span><br><span class="line">smartSingleton.afterSingletonsInstantiated();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>存疑：</strong> spring 2 中preInstantiateSingletons的实现是加了个Synchronized内置锁，而在当前版本中，这一步去掉了锁，why?</p><h3 id="AOP"><a href="#AOP" class="headerlink" title="AOP"></a>AOP</h3><h4 id="AOP名词解释"><a href="#AOP名词解释" class="headerlink" title="AOP名词解释"></a>AOP名词解释</h4><ul><li><strong>方面（Aspect）</strong>：<br>一个关注点的模块化，这个关注点实现可能另外横切多个对象。事务管理是J2EE应用中一个很好的横切关注点例子。方面用spring的 Advisor或拦截器实现。</li><li><strong>连接点（Joinpoint）:</strong> 程序执行过程中明确的点，如方法的调用或特定的异常被抛出。</li><li><strong>通知（Advice）:</strong> 在特定的连接点，AOP框架执行的动作。各种类型的通知包括“around”、“before”和“throws”通知。通知类型将在下面讨论。许多AOP框架包括Spring都是以拦截器做通知模型，维护一个“围绕”连接点的拦截器链。Spring中定义了四个advice: BeforeAdvice, AfterAdvice, ThrowAdvice和DynamicIntroductionAdvice</li><li><strong>切入点（Pointcut 一系列连接点的集合）:</strong> 指定一个通知将被引发的一系列连接点的集合。AOP框架必须允许开发者指定切入点：例如，使用正则表达式。 Spring定义了Pointcut接口，用来组合MethodMatcher和ClassFilter，可以通过名字很清楚的理解， MethodMatcher是用来检查目标类的方法是否可以被应用此通知，而ClassFilter是用来检查Pointcut是否应该应用到目标类上</li><li><strong>引入（Introduction）:</strong> 添加方法或字段到被通知的类。 Spring允许引入新的接口到任何被通知的对象。例如，你可以使用一个引入使任何对象实现 IsModified接口，来简化缓存。Spring中要使用Introduction, 可有通过DelegatingIntroductionInterceptor来实现通知，通过DefaultIntroductionAdvisor来配置Advice和代理类要实现的接口</li><li><strong>目标对象（Target Object）:</strong> 包含连接点的对象。也被称作被通知或被代理对象。POJO</li><li><strong>AOP代理（AOP Proxy）:</strong> AOP框架创建的对象，包含通知。 在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。</li><li><strong>织入（Weaving）:</strong> 组装方面来创建一个被通知对象。这可以在编译时完成（例如使用AspectJ编译器），也可以在运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入</li></ul><h4 id="设计原理及流程"><a href="#设计原理及流程" class="headerlink" title="设计原理及流程"></a><strong>设计原理及流程</strong></h4><p>Advice、PointCut、Advisor(通知器，组织起Advice与PointCut)</p><p><img src="http://jacobs.wanhb.cn/images/aopinterface.jpg" alt="接口设计"><br>AopProxy代理对象生成过程：ProxyFactoryBean和ProxyFactory都提供了AOP的功能封装，但是ProxyFactoryBean与IOC进行了结合，利用BeanFactoryAware获取ApplicationContext,从而可以利用context对IOC注入的bean进行获取</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">初始化通知链—— &gt;获取单例，没有的话去创建——&gt;判断是否为接口，如果为接口使用JDK,如果不是使用CGlib最后返回AopProxy</span><br></pre></td></tr></table></figure><h4 id="AOP调用"><a href="#AOP调用" class="headerlink" title="AOP调用"></a><strong>AOP调用</strong></h4><p>invoke方法，里面逐个去应用配置好的拦截器链,在逐个应用之前先进行一系列的判断：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">／／如果目标对象没有实现object的基本法方法：equals、如果目标对象没有实现object的基本方法： hashcode、根据代理对象的配置来调用服务</span><br><span class="line">if(!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123;</span><br><span class="line">  Boolean retVal3 = Boolean.valueOf(this.equals(args[0]));</span><br><span class="line">  return retVal3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if(!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method)) &#123;</span><br><span class="line">  Integer retVal2 = Integer.valueOf(this.hashCode());</span><br><span class="line">  return retVal2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if(method.getDeclaringClass() == DecoratingProxy.class) &#123;</span><br><span class="line">  Class retVal1 = AopProxyUtils.ultimateTargetClass(this.advised);</span><br><span class="line">  return retVal1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Object retVal;</span><br><span class="line">if(!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123;</span><br><span class="line">  retVal = AopUtils.invokeJoinpointUsingReflection(this.advised, method, args);</span><br><span class="line">  return retVal;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if(this.advised.exposeProxy) &#123;</span><br><span class="line">  oldProxy = AopContext.setCurrentProxy(proxy);</span><br><span class="line">  setProxyContext = true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">target = targetSource.getTarget();</span><br><span class="line">if(target != null) &#123;</span><br><span class="line">  targetClass = target.getClass();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后获取配置好的拦截器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);</span><br></pre></td></tr></table></figure><p>判断是否为空，为空的话直接调用invokeJoinpointUsingReflection方法。这个方法直接调用目标方法的实现，代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">public static Object invokeJoinpointUsingReflection(Object target, Method method, Object[] args) throws Throwable &#123;</span><br><span class="line">  try &#123;</span><br><span class="line">    ReflectionUtils.makeAccessible(method);</span><br><span class="line">    return method.invoke(target, args);</span><br><span class="line">  &#125; catch (InvocationTargetException var4) &#123;</span><br><span class="line">    throw var4.getTargetException();</span><br><span class="line">  &#125; catch (IllegalArgumentException var5) &#123;</span><br><span class="line">    throw new AopInvocationException(&quot;AOP configuration seems to be invalid: tried calling method [&quot; + method + &quot;] on target [&quot; + target + &quot;]&quot;, var5);</span><br><span class="line">  &#125; catch (IllegalAccessException var6) &#123;</span><br><span class="line">    throw new AopInvocationException(&quot;Could not access method [&quot; + method + &quot;]&quot;, var6);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">如果不为空，则创建ReflectiveMethodInvocation传入chain（拦截器链）,然后调用proceed方法，这个方法去递归调用拦截器链中的invoke方法，代码如下（在拦截器的调用一节还会详细展开介绍）：</span><br><span class="line">ReflectiveMethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain);</span><br><span class="line">retVal = invocation.proceed();</span><br><span class="line"></span><br><span class="line">public Object proceed() throws Throwable &#123;</span><br><span class="line">  if(this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123;</span><br><span class="line">    return this.invokeJoinpoint();</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex);</span><br><span class="line">    if(interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123;</span><br><span class="line">      InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher)interceptorOrInterceptionAdvice;</span><br><span class="line">      return dm.methodMatcher.matches(this.method, this.targetClass, this.arguments)?dm.interceptor.invoke(this):this.proceed();</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return ((MethodInterceptor)interceptorOrInterceptionAdvice).invoke(this);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于最后的目标对象的调用：JDK 直接通过AopUtils的反射机制而cglib则是通过 MethodProxy完成调用，这是cglib自己封住好的功能。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">retval=methodProxy.invoke(target,args);</span><br></pre></td></tr></table></figure><h4 id="AOP拦截器链的调用"><a href="#AOP拦截器链的调用" class="headerlink" title="AOP拦截器链的调用"></a><strong>AOP拦截器链的调用</strong></h4><p>了解了AOP的调用之后，再来看看AOP是怎么实现对目标对象增强的。<br>在运行拦截器链的拦截方法时，需要对代理方法完成一个匹配判断，通过这个匹配判断来决定是否满足切面增强的要求。确定是否执行拦截方法。<br>获取interceptors的操作是由advised的对象完成的。是一个AdvisedSupport对象。AdvisedSupport是ProxyFactoryBean的基类。在其中，我们可以看到getInterceptorsAndDynamicInterceptionAdvice 方法的实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Method method, Class&lt;?&gt; targetClass) &#123;</span><br><span class="line">  AdvisedSupport.MethodCacheKey cacheKey = new AdvisedSupport.MethodCacheKey(method);</span><br><span class="line">  List cached = (List)this.methodCache.get(cacheKey);</span><br><span class="line">  if(cached == null) &#123;</span><br><span class="line">    cached = this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice(this, method, targetClass);</span><br><span class="line">    this.methodCache.put(cacheKey, cached);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return cached;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里AdvisedSupport被配置成一个DefaultAdvisedSupport对象，里面实现了具体的getInterceptorsAndDynamicInterceptionAdvice方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Advised config, Method method, Class&lt;?&gt; targetClass) &#123;</span><br><span class="line">  ArrayList interceptorList = new ArrayList(config.getAdvisors().length);</span><br><span class="line">  Class actualClass = targetClass != null?targetClass:method.getDeclaringClass();</span><br><span class="line">  boolean hasIntroductions = hasMatchingIntroductions(config, actualClass);</span><br><span class="line">  AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance();</span><br><span class="line">  Advisor[] var8 = config.getAdvisors();</span><br><span class="line">  int var9 = var8.length;</span><br><span class="line"></span><br><span class="line">  for(int var10 = 0; var10 &lt; var9; ++var10) &#123;</span><br><span class="line">    Advisor advisor = var8[var10];</span><br><span class="line">    MethodInterceptor[] interceptors1;</span><br><span class="line">    if(advisor instanceof PointcutAdvisor) &#123;</span><br><span class="line">      PointcutAdvisor var20 = (PointcutAdvisor)advisor;</span><br><span class="line">      if(config.isPreFiltered() || var20.getPointcut().getClassFilter().matches(actualClass)) &#123;</span><br><span class="line">        interceptors1 = registry.getInterceptors(advisor);</span><br><span class="line">        MethodMatcher mm = var20.getPointcut().getMethodMatcher();</span><br><span class="line">        if(MethodMatchers.matches(mm, method, actualClass, hasIntroductions)) &#123;</span><br><span class="line">          if(mm.isRuntime()) &#123;</span><br><span class="line">            MethodInterceptor[] var15 = interceptors1;</span><br><span class="line">            int var16 = interceptors1.length;</span><br><span class="line"></span><br><span class="line">            for(int var17 = 0; var17 &lt; var16; ++var17) &#123;</span><br><span class="line">              MethodInterceptor interceptor = var15[var17];</span><br><span class="line">              interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm));</span><br><span class="line">            &#125;</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            interceptorList.addAll(Arrays.asList(interceptors1));</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else if(advisor instanceof IntroductionAdvisor) &#123;</span><br><span class="line">      IntroductionAdvisor var19 = (IntroductionAdvisor)advisor;</span><br><span class="line">      if(config.isPreFiltered() || var19.getClassFilter().matches(actualClass)) &#123;</span><br><span class="line">        interceptors1 = registry.getInterceptors(advisor);</span><br><span class="line">        interceptorList.addAll(Arrays.asList(interceptors1));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      MethodInterceptor[] interceptors = registry.getInterceptors(advisor);</span><br><span class="line">      interceptorList.addAll(Arrays.asList(interceptors));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return interceptorList;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>DefaultAdvisorChainFactory 会通过一个AdvisorDapterRegister实现拦截器的注册，注册完成之后,List中的拦截器会被JDK生成的AopProxy中的代理对象的invoke调用，这里通过配置的Intercepters获得拦截器列表然后逐一应用在目标方法上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public MethodInterceptor[] getInterceptors(Advisor advisor) throws UnknownAdviceTypeException &#123;</span><br><span class="line">    ArrayList interceptors = new ArrayList(3);</span><br><span class="line">    Advice advice = advisor.getAdvice();</span><br><span class="line">    if(advice instanceof MethodInterceptor) &#123;</span><br><span class="line">      interceptors.add((MethodInterceptor)advice);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Iterator var4 = this.adapters.iterator();</span><br><span class="line"></span><br><span class="line">    while(var4.hasNext()) &#123;</span><br><span class="line">      AdvisorAdapter adapter = (AdvisorAdapter)var4.next();</span><br><span class="line">      if(adapter.supportsAdvice(advice)) &#123;</span><br><span class="line">        interceptors.add(adapter.getInterceptor(advisor));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if(interceptors.isEmpty()) &#123;</span><br><span class="line">      throw new UnknownAdviceTypeException(advisor.getAdvice());</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return (MethodInterceptor[])interceptors.toArray(new MethodInterceptor[interceptors.size()]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> spring </tag>
            
            <tag> ioc </tag>
            
            <tag> aop </tag>
            
            <tag> 源码 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>类函数式的sql生成工具类的封装</title>
      <link href="/2017/01/30/%E7%B1%BB%E5%87%BD%E6%95%B0%E5%BC%8F%E7%9A%84sql%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%9A%84%E5%B0%81%E8%A3%85/"/>
      <url>/2017/01/30/%E7%B1%BB%E5%87%BD%E6%95%B0%E5%BC%8F%E7%9A%84sql%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%9A%84%E5%B0%81%E8%A3%85/</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>自从正式工作以来，公司一直用的是Spring 原生的JDBC Template以及在其上封装的扩展的一些小工具， 而摈弃了Mybatis、ibatis等ORM框架。总的来说，这种做法对于开发效率来说提高不少，由于真正的查库操作不会直接穿透到Mysql，所以抗压性也没有太大的问题。</p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>虽说直接使用原生的JDBC Template比较方便，但是构造参数条件，以及生成查询语句过于简单粗暴，导致代码不简洁，复用度也不高。举个例子，根据特定的条件查询用户：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">StringBuffer sqlBuilder = new StringBuffer(&quot;select * from user where &quot;);</span><br><span class="line">Map&lt;String, Object&gt; paramsMap = new HashMap&lt;&gt;();</span><br><span class="line">if (userId != null) &#123;</span><br><span class="line">  sqlBuilder.append(&quot;user_id=:userId&quot;);</span><br><span class="line">  paramsMap.put(&quot;userId&quot;, userId);</span><br><span class="line">&#125;</span><br><span class="line">if (batchNumber != null) &#123;</span><br><span class="line">  sqlBuilder.append(&quot;batch_number=:batchNumber&quot;);</span><br><span class="line">  paramsMap.put(&quot;batchNumber&quot;, batchNumber);</span><br><span class="line">&#125;</span><br><span class="line">if (status != null) &#123;</span><br><span class="line">  sqlBuilder.append(&quot;status=:status&quot;);</span><br><span class="line">  paramsMap.put(&quot;stats&quot;, status);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if (page &gt; 0 &amp;&amp; count &gt; 0) &#123;</span><br><span class="line">  sqlBuilder.append(&quot; limit &quot;)</span><br><span class="line">      .append(page * count)</span><br><span class="line">      .append(&quot;, &quot;)</span><br><span class="line">      .append(count);</span><br><span class="line">&#125;</span><br><span class="line">//生成sql语句</span><br><span class="line">sqlBuilder.toString();</span><br></pre></td></tr></table></figure></p><p>可以看到上面的代码大部分在重复同样的逻辑，20多行的代码仅仅只是在构造sql语句以及收集参数，而且这些重复的代码将充斥项目所有的DAO层，导致代码非常不整洁。维护困难。</p><h3 id="sql生成工具类封装"><a href="#sql生成工具类封装" class="headerlink" title="sql生成工具类封装"></a>sql生成工具类封装</h3><p>秉承着恶心重复的代码要重构抽象的态度，对sql生成的步骤，做了一次简单的封装：</p><ul><li>先介绍接口<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public interface Order &#123;</span><br><span class="line">    Where desc();</span><br><span class="line"></span><br><span class="line">    Where asc();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">public interface Where &#123;</span><br><span class="line">    Where ifPresent(Object value, String sql);</span><br><span class="line"></span><br><span class="line">    Order orderBy(String field);</span><br><span class="line"></span><br><span class="line">    Where limit(int begin, int end);</span><br><span class="line"></span><br><span class="line">    Where in(List&lt;Object&gt; values, String sql);</span><br><span class="line"></span><br><span class="line">    String sql();//构造的sql</span><br><span class="line"></span><br><span class="line">    Map&lt;String, Object&gt; params();//参数值</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li></ul><p>根据sql语句的特点，抽出子句部分的生成单独构造，涵盖Where, in(暂未实现)，limit 以及 order 排序规则。</p><ul><li>来看看接口的实现<br>将所有相关的类通过内部静态类封装入SqlWhereBuffer类中。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">public class SqlWhereBuffer &#123;</span><br><span class="line"></span><br><span class="line">  public static SqlWhereBuffer.Where builder() &#123;</span><br><span class="line">    return new Builder();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  static class Builder implements SqlWhereBuffer.Order, SqlWhereBuffer.Where &#123;</span><br><span class="line">    private List&lt;String&gt; sql = new ArrayList&lt;&gt;();</span><br><span class="line">    private Map&lt;String, Object&gt; params = new HashMap&lt;&gt;();</span><br><span class="line">    private String orderBy = &quot; &quot;;</span><br><span class="line">    private String limit = &quot; &quot;;</span><br><span class="line">    private String in = &quot; &quot;;</span><br><span class="line">    private List&lt;SqlMapperPlugin&gt; sqlMapperPlugins;</span><br><span class="line"></span><br><span class="line">    public Builder() &#123;</span><br><span class="line">      //添加基本类型默认的插件</span><br><span class="line">      sqlMapperPlugins = Lists.newArrayList();</span><br><span class="line">      this.mapperPlugins(SqlMapperPlugin.EnumSqlPlugin)</span><br><span class="line">          .mapperPlugins(SqlMapperPlugin.BooleanSqlPlugin);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Builder mapperPlugins(SqlMapperPlugin sqlMapperPlugin) &#123;</span><br><span class="line">      if (CollectionUtils.isEmpty(sqlMapperPlugins)) &#123;</span><br><span class="line">        sqlMapperPlugins = Lists.newArrayList();</span><br><span class="line">      &#125;</span><br><span class="line">      sqlMapperPlugins.add(sqlMapperPlugin);</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public SqlWhereBuffer.Where ifPresent(Object value, String sql) &#123;</span><br><span class="line">      Optional.ofNullable(value)</span><br><span class="line">          .ifPresent(val -&gt; &#123;</span><br><span class="line">            this.sql.add(sql);</span><br><span class="line">            Pattern pattern = Pattern.compile(&quot;:([a-z,A-Z,\\w,_]*)&quot;); //固定的sql参数模式</span><br><span class="line">            Matcher matcher = pattern.matcher(sql);</span><br><span class="line">            if (!matcher.find()) &#123;</span><br><span class="line">              throw new IllegalArgumentException(sql + &quot; don&apos;t include :name&quot;);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            Optional&lt;SqlMapperPlugin&gt; mapper = sqlMapperPlugins.stream()</span><br><span class="line">                .filter(sqlMapperPlugin -&gt; sqlMapperPlugin.test(val))</span><br><span class="line">                .findFirst();</span><br><span class="line">            if (mapper.isPresent()) &#123;</span><br><span class="line">              params.putAll(mapper.get()</span><br><span class="line">                  .getParams(matcher.group(1), value));</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">              //匹配不到走默认的插件</span><br><span class="line">              params.put(matcher.group(1), value);</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;);</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public SqlWhereBuffer.Order orderBy(String field) &#123;</span><br><span class="line">      orderBy += &quot;ORDER BY &quot; + field;</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public SqlWhereBuffer.Where limit(int offset, int count) &#123;</span><br><span class="line">      limit += &quot;limit &quot; + offset + &quot;,&quot; + count;</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public String sql() &#123;</span><br><span class="line">      if (sql.isEmpty() || params.isEmpty()) &#123;</span><br><span class="line">        return &quot;&quot;;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        return &quot; WHERE &quot; + sql.stream()</span><br><span class="line">            .collect(Collectors.joining(&quot; AND &quot;)) + orderBy + limit;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public ImmutableMap&lt;String, Object&gt; params() &#123;</span><br><span class="line">      return ImmutableMap.copyOf(params);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public SqlWhereBuffer.Where desc() &#123;</span><br><span class="line">      orderBy += &quot; DESC&quot;;</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public SqlWhereBuffer.Where asc() &#123;</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>静态类Builder实现了SqlWhereBuffer.Order, SqlWhereBuffer.Where这俩个接口的方法。ifPresent是主入口。流程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、在ifPresent中参数先进行非空判断，如果是空则直接过滤掉；</span><br><span class="line">2、然后通过一个正则表达式（**:([a-z,A-Z,\\w,_]*)**）对参数sql进行规则校验，如果不符合也将过滤；</span><br><span class="line">3、最后value通过一系列的自定义的注册插件的匹配判断，得出sql以及params。</span><br></pre></td></tr></table></figure></p><ul><li>关于自定义插件，本没想做这么复杂，然而在实际的使用过程中，JDBC Template对Enum的支持不够…不过想也是，Enum大多是自定义的，没法做到一套约定的接口满足所有需求。于是封装了一个插件类，方便以后扩展：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">public static class SqlMapperPlugin &#123;</span><br><span class="line">    private final Predicate&lt;Object&gt; predicate;</span><br><span class="line">    private final ParamValue paramValue;</span><br><span class="line"></span><br><span class="line">    private SqlMapperPlugin(Predicate&lt;Object&gt; predicate,</span><br><span class="line">        ParamValue paramValue) &#123;</span><br><span class="line">      this.predicate = predicate;</span><br><span class="line">      this.paramValue = paramValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //定义枚举插件</span><br><span class="line">    static SqlMapperPlugin EnumSqlPlugin = of(Enum.class).paramValue((value, sql) -&gt;</span><br><span class="line">        Collections.singletonMap(sql, getEnumValue(value))</span><br><span class="line">    );</span><br><span class="line">    //定义Boolean插件</span><br><span class="line">    static SqlMapperPlugin BooleanSqlPlugin = of(Boolean.class).paramValue((value, sql) -&gt;</span><br><span class="line">        Collections.singletonMap(sql, ((boolean) value) ? 1 : 0)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    public static SqlMapperPlugin.MapperPluginsBuilder of(Predicate&lt;Object&gt; predicate) &#123;</span><br><span class="line">      return new MapperPluginsBuilder(predicate);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static SqlMapperPlugin.MapperPluginsBuilder of(Class clazz) &#123;</span><br><span class="line">      return of((pd) -&gt; &#123;</span><br><span class="line">        return clazz.isAssignableFrom(pd.getClass());</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    boolean test(Object pd) &#123;</span><br><span class="line">      return this.predicate.test(pd);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Map&lt;String, Object&gt; getParams(String sql, Object value) &#123;</span><br><span class="line">      return paramValue.getParams(value, sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class MapperPluginsBuilder &#123;</span><br><span class="line"></span><br><span class="line">      Predicate&lt;Object&gt; predicate;</span><br><span class="line"></span><br><span class="line">      public MapperPluginsBuilder(Predicate&lt;Object&gt; predicate) &#123;</span><br><span class="line">        this.predicate = predicate;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      public SqlMapperPlugin paramValue(SqlMapperPlugin.ParamValue paramValue) &#123;</span><br><span class="line">        return new SqlMapperPlugin(this.predicate, paramValue);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @FunctionalInterface</span><br><span class="line">    public interface ParamValue &#123;</span><br><span class="line">      Map&lt;String, Object&gt; getParams(Object var1, String var2);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static String getEnumValue(Object value) &#123;</span><br><span class="line">    Method method;</span><br><span class="line">    try &#123;</span><br><span class="line">      method = value.getClass()</span><br><span class="line">          .getMethod(&quot;name&quot;);</span><br><span class="line">      return String.valueOf(method.invoke(value));</span><br><span class="line">    &#125; catch (NoSuchMethodException e) &#123;</span><br><span class="line">      throw new RuntimeException(&quot;NoSuchMethodException&quot;, e);</span><br><span class="line">    &#125; catch (IllegalAccessException e) &#123;</span><br><span class="line">      throw new RuntimeException(&quot;IllegalAccessException&quot;, e);</span><br><span class="line">    &#125; catch (InvocationTargetException e) &#123;</span><br><span class="line">      throw new RuntimeException(&quot;InvocationTargetException&quot;, e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>主要用到了java8中新增的Predicate接口，用于判断value的Class类型，以及自己定义了一个FunctionalInterface 用于获取查询参数。这里先实现了EnumSqlPlugin，BooleanSqlPlugin俩个插件，之后有其他类型的需求，也可以通过类似的形式加入。对EnumSqlPlugin实现进行解释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//定义枚举插件</span><br><span class="line">    static SqlMapperPlugin EnumSqlPlugin = of(Enum.class).paramValue((value, sql) -&gt;</span><br><span class="line">        Collections.singletonMap(sql, getEnumValue(value))</span><br><span class="line">    );</span><br></pre></td></tr></table></figure></p><p>通过定义的of方法定义赋值Predicate,作为判断value是否为指定类型的方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public static SqlMapperPlugin.MapperPluginsBuilder of(Class clazz) &#123;</span><br><span class="line">      return of((pd) -&gt; &#123;</span><br><span class="line">        return clazz.isAssignableFrom(pd.getClass());</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>of方法返回了MapperPluginsBuilder类，紧接着定义函数式接口paramValue的实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Collections.singletonMap(sql, getEnumValue(value))</span><br></pre></td></tr></table></figure></p><p>最后返回EnumSqlPlugin对应的新实例:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return new SqlMapperPlugin(this.predicate, paramValue);</span><br></pre></td></tr></table></figure></p><p>使用的时候，在Builder的构造函数中注入默认的插件即可，如日后有扩展，也可调用mapperPlugins方法动态加入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> private List&lt;SqlMapperPlugin&gt; sqlMapperPlugins;</span><br><span class="line"> public Builder() &#123;</span><br><span class="line">      //添加基本类型默认的插件</span><br><span class="line">      sqlMapperPlugins = Lists.newArrayList();</span><br><span class="line">      this.mapperPlugins(SqlMapperPlugin.EnumSqlPlugin)</span><br><span class="line">          .mapperPlugins(SqlMapperPlugin.BooleanSqlPlugin);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">public Builder mapperPlugins(SqlMapperPlugin sqlMapperPlugin)    &#123;</span><br><span class="line">      if (CollectionUtils.isEmpty(sqlMapperPlugins)) &#123;</span><br><span class="line">        sqlMapperPlugins = Lists.newArrayList();</span><br><span class="line">      &#125;</span><br><span class="line">      sqlMapperPlugins.add(sqlMapperPlugin);</span><br><span class="line">      return this;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><h3 id="工具的使用"><a href="#工具的使用" class="headerlink" title="工具的使用"></a>工具的使用</h3><p>现在，我们可以使用封装好的工具了:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SqlWhereBuffer.Where where = SqlWhereBuffer.builder()</span><br><span class="line">        .ifPresent(batchNumber, &quot;batch_number=:batchNumber&quot;)</span><br><span class="line">        .ifPresent(status, &quot;upload_result=:uploadResult&quot;)</span><br><span class="line">        .ifPresent(userId, &quot;user_id=:userId&quot;)</span><br><span class="line">        .limit(page * count, count)</span><br><span class="line">        .orderBy(&quot;id&quot;)</span><br><span class="line">        .asc();</span><br></pre></td></tr></table></figure></p><p>这样，代码变得整洁多了，也符合java8函数式风格的效果。</p>]]></content>
      
      
        <tags>
            
            <tag> java8 </tag>
            
            <tag> sqlGenerator </tag>
            
            <tag> 函数式编程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ReentrantLock与condition应用后的思考</title>
      <link href="/2017/01/20/ReentrantLock%E4%B8%8Econdition%E5%BA%94%E7%94%A8%E5%90%8E%E7%9A%84%E6%80%9D%E8%80%83/"/>
      <url>/2017/01/20/ReentrantLock%E4%B8%8Econdition%E5%BA%94%E7%94%A8%E5%90%8E%E7%9A%84%E6%80%9D%E8%80%83/</url>
      <content type="html"><![CDATA[<p>一直在断断续续看《java并发编程实战》这本书，每次看都有不一样的体会，前些日子在知乎上回答了一个关于ReentrantLock的问题<a href="https://www.zhihu.com/question/52273413" target="_blank" rel="noopener">java里是怎么通过condition接口是获取监视器方法的</a> ,那次回答之后也引发了我对其实现的进一步探究。</p><h3 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h3><p>举一个简单的例子，是基本上各个公司招聘的时候都会出现的关于多线程间通信的问题：利用多线程循环打印n次”ABC”。当然，这个题目有很多实现方法，有经典的wait和notify的原生方法,也有时髦一点的ReentrantLock写法，如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Created by lichao on 2016/1/20.</span><br><span class="line"> */</span><br><span class="line">public class PrintABC &#123;</span><br><span class="line"></span><br><span class="line">  static ReentrantLock lock = new ReentrantLock();</span><br><span class="line">  static Condition conditionA = lock.newCondition();</span><br><span class="line">  static Condition conditionB = lock.newCondition();</span><br><span class="line">  static Condition conditionC = lock.newCondition();</span><br><span class="line">  static int signal = 1;//1=&gt;A, 2=&gt;B 3=&gt;C</span><br><span class="line">  static int loopValue = 10;</span><br><span class="line"></span><br><span class="line">  class taskA implements Runnable &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">      lock.lock();</span><br><span class="line">      try &#123;</span><br><span class="line">        for (int i = 0; i &lt; loopValue; i++) &#123;</span><br><span class="line">          if (signal != 1) &#123;</span><br><span class="line">            conditionA.await();</span><br><span class="line">            conditionB.signalAll();</span><br><span class="line">            conditionC.signalAll();</span><br><span class="line">          &#125;</span><br><span class="line">          signal = 2;</span><br><span class="line">          System.out.print(&quot;A&quot;);</span><br><span class="line">          conditionB.signal();</span><br><span class="line">          conditionA.await();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch (Exception ex) &#123;</span><br><span class="line"></span><br><span class="line">      &#125; finally &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class taskB implements Runnable &#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">      lock.lock();</span><br><span class="line">      try &#123;</span><br><span class="line">        for (int i = 0; i &lt; loopValue; i++) &#123;</span><br><span class="line">          if (signal != 2) &#123;</span><br><span class="line">            conditionB.await();</span><br><span class="line">            conditionA.signalAll();</span><br><span class="line">            conditionC.signalAll();</span><br><span class="line">          &#125;</span><br><span class="line">          signal = 3;</span><br><span class="line">          System.out.print(&quot;B&quot;);</span><br><span class="line">          conditionC.signal();</span><br><span class="line">          conditionB.await();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch (Exception ex) &#123;</span><br><span class="line"></span><br><span class="line">      &#125; finally &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class taskC implements Runnable &#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">      lock.lock();</span><br><span class="line">      try &#123;</span><br><span class="line">        for (int i = 0; i &lt; loopValue; i++) &#123;</span><br><span class="line">          if (signal != 3) &#123;</span><br><span class="line">            conditionC.await();</span><br><span class="line">            conditionB.signalAll();</span><br><span class="line">            conditionA.signalAll();</span><br><span class="line">          &#125;</span><br><span class="line">          signal = 1;</span><br><span class="line">          System.out.print(&quot;C&quot;);</span><br><span class="line">          conditionA.signal();</span><br><span class="line">          conditionC.await();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch (Exception ex) &#123;</span><br><span class="line"></span><br><span class="line">      &#125; finally &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    ExecutorService executorService = Executors.newFixedThreadPool(3);</span><br><span class="line">    executorService.submit(new PrintABC().new taskC());</span><br><span class="line">    executorService.submit(new PrintABC().new taskB());</span><br><span class="line">    executorService.submit(new PrintABC().new taskA());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>首先定义了一个可重入锁，然后起了三个监视器用于控制任务a,b,c的状态，signal作为信号量用于表明当前应该哪个线程执行打印操作，实现无论任务往线程池中提交的顺序如何都能正确打印ABC的顺序</p><h3 id="讲讲原理"><a href="#讲讲原理" class="headerlink" title="讲讲原理"></a>讲讲原理</h3><ul><li>ReentrantLock（重入锁）是jdk的concurrent包提供的一种独占锁的实现。它继承自Dong Lea的 AbstractQueuedSynchronizer（同步器）。回到上面的代码，我们提交任务是按照CBA的次序来提交的，也就是打印C的任务会先开始执行，而当前的信号量signal为1,也就是A而不是3，所以通过conditonC.await()来释放锁，同时线程休眠等待唤醒，这时A拿到了，并且打印后将signal置为2即B，同时通过conditionA.await()方法使自己休眠，并唤醒B进行打印。以此类推，总的来说，ReentrantLock�与condition配合，优雅的完成了wait和notify做的事情。</li><li>我们来看看其中是如何实现这种线程的调度过程的：reentrantLock.newCondition() 返回的是Condition的一个实现，该类在AbstractQueuedSynchronizer中被实现，可以访问AbstractQueuedSynchronizer中的方法和其余内部类,await被调用时的代码如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public final void await() throws InterruptedException &#123;</span><br><span class="line">if (Thread.interrupted())</span><br><span class="line"> throw new InterruptedException();</span><br><span class="line"> //将当前线程包装下后，添加到Condition自己维护的一个链表中。</span><br><span class="line"> Node node = addConditionWaiter(); </span><br><span class="line"> //释放当前线程占有的锁</span><br><span class="line">int savedState = fullyRelease(node);</span><br><span class="line">int interruptMode = 0;</span><br><span class="line"> while (!isOnSyncQueue(node)) &#123;</span><br><span class="line"> //释放完毕后，不断AQS的队列，看当前节点是否在队列中，不在 说明它还没有竞争锁的资格，所以继续将自己沉睡。直到它被重新加入到队列中.</span><br><span class="line"> LockSupport.park(this);</span><br><span class="line"> if ((interruptMode = checkInterruptWhileWaiting(node)) != 0)</span><br><span class="line"> break;</span><br><span class="line"> &#125;</span><br><span class="line">//被唤醒后，重新开始正式竞争锁，同样，如果竞争不到还是会将自己沉睡，等待唤醒重新开始竞争。</span><br><span class="line">if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE)</span><br><span class="line"> interruptMode = REINTERRUPT;</span><br><span class="line"> if (node.nextWaiter != null)</span><br><span class="line"> unlinkCancelledWaiters();</span><br><span class="line"> if (interruptMode != 0)</span><br><span class="line"> reportInterruptAfterWait(interruptMode);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ul><p>signal方法的调用代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">public final void signal() &#123;</span><br><span class="line"> if (!isHeldExclusively())</span><br><span class="line"> throw new IllegalMonitorStateException();</span><br><span class="line"> Node first = firstWaiter; //firstWaiter为condition自己维护的一个链表的头结点，</span><br><span class="line">                          //取出第一个节点后开始唤醒操作</span><br><span class="line"> if (first != null)</span><br><span class="line"> doSignal(first);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> private void doSignal(Node first) &#123;</span><br><span class="line"> do &#123;</span><br><span class="line"> if ( (firstWaiter = first.nextWaiter) == null) //修改头结点，完成旧头结点的移出工作</span><br><span class="line"> lastWaiter = null;</span><br><span class="line"> first.nextWaiter = null;</span><br><span class="line"> &#125; while (!transferForSignal(first) &amp;&amp;//将老的头结点，加入到AQS的等待队列中</span><br><span class="line"> (first = firstWaiter) != null);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">final boolean transferForSignal(Node node) &#123;</span><br><span class="line"> /*</span><br><span class="line"> * If cannot change waitStatus, the node has been cancelled.</span><br><span class="line"> */</span><br><span class="line"> if (!compareAndSetWaitStatus(node, Node.CONDITION, 0))</span><br><span class="line"> return false;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * Splice onto queue and try to set waitStatus of predecessor to</span><br><span class="line"> * indicate that thread is (probably) waiting. If cancelled or</span><br><span class="line"> * attempt to set waitStatus fails, wake up to resync (in which</span><br><span class="line"> * case the waitStatus can be transiently and harmlessly wrong).</span><br><span class="line"> */</span><br><span class="line"> Node p = enq(node);</span><br><span class="line"> int ws = p.waitStatus;</span><br><span class="line">//如果该结点的状态为cancel 或者修改waitStatus失败，则直接唤醒。</span><br><span class="line"> if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL))</span><br><span class="line"> LockSupport.unpark(node.thread);</span><br><span class="line"> return true;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p><p>其实condition内部维护了一个等待队列，用于存放等待signal的任务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">public class ConditionObject implements Condition, java.io.Serializable &#123;</span><br><span class="line">        private static final long serialVersionUID = 1173984872572414699L;</span><br><span class="line">        /** First node of condition queue. */</span><br><span class="line">        private transient Node firstWaiter;</span><br><span class="line">        /** Last node of condition queue. */</span><br><span class="line">        private transient Node lastWaiter;</span><br><span class="line">        </span><br><span class="line">**</span><br><span class="line">         * Adds a new waiter to wait queue.</span><br><span class="line">         * @return its new wait node</span><br><span class="line">         */</span><br><span class="line">        private Node addConditionWaiter() &#123;</span><br><span class="line">            Node t = lastWaiter;</span><br><span class="line">            // If lastWaiter is cancelled, clean out.</span><br><span class="line">            if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123;</span><br><span class="line">                unlinkCancelledWaiters();</span><br><span class="line">                t = lastWaiter;</span><br><span class="line">            &#125;</span><br><span class="line">            Node node = new Node(Thread.currentThread(), Node.CONDITION);</span><br><span class="line">            if (t == null)</span><br><span class="line">                firstWaiter = node;</span><br><span class="line">            else</span><br><span class="line">                t.nextWaiter = node;</span><br><span class="line">            lastWaiter = node;</span><br><span class="line">            return node;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>而AbstractQueuedSynchronizer中也维护了一个队列，就是获取当前资源的等待队列，当资源释放掉之后，会依次从队列中恢复线程，直至为空。每个线程会在这俩个队列中来回切换，但同一时刻仅存在于一个队列中。<br><img src="http://jacobs.wanhb.cn/images/reentrantLock.jpg" alt="ReentrantLock流程"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">线程1-&gt;AQS: ReentrantLock.lock（加入AQS队列，获取资源）</span><br><span class="line">线程1-&gt;AQS: condition.await()（移除队列，释放资源）</span><br><span class="line">AQS-&gt;condition队列: 线程1加入condition等待队列</span><br><span class="line">线程2-&gt;AQS: 线程2加入AQS队列并获取资源</span><br><span class="line">线程2-&gt;AQS: condition2.await()（移除AQS队列，释放资源）</span><br><span class="line">线程2-&gt;线程1: condition.signal()（线程2调用signal唤醒线程1）</span><br><span class="line">线程1-&gt;AQS: ReentrantLock.lock(重新加入AQS队列，获取资源)</span><br></pre></td></tr></table></figure></p><p>###总结<br>还是得深入源码去看问题，不能只关注业务，否则会成为彻头彻尾的搬砖工，不仅要会用轮子，还要会造轮子。最后分享一片陈浩写的技术人员的职业生涯文章，共勉。<a href="http://coolshell.cn/articles/17583.html" target="_blank" rel="noopener">技术人员的发展之路</a></p>]]></content>
      
      
        <tags>
            
            <tag> 多线程 </tag>
            
            <tag> ReentrantLock </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>记一次失败的debug经历</title>
      <link href="/2017/01/19/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%A4%B1%E8%B4%A5%E7%9A%84debug%E7%BB%8F%E5%8E%86/"/>
      <url>/2017/01/19/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%A4%B1%E8%B4%A5%E7%9A%84debug%E7%BB%8F%E5%8E%86/</url>
      <content type="html"><![CDATA[<p>昨天使用spring aop的事务注解出现了业务中主动抛出异常却无法回滚脏数据的情形，然后崩溃的debug了一天，最后猛然发现不能通过this去掉加了增强的方法否则将拿不到代理对象。归根结底还是自己不甚理解事务实现的其中原理，才导致了bug的出现而不自知。</p><h3 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h3><p>如果对java如何实现底层的事务机制不太熟悉的话可以看看<a href="http://www.cnblogs.com/davenkin/archive/2013/02/16/java-tranaction-1.html" target="_blank" rel="noopener">java事务处理系列文章</a> 自己手动实现事务处理。</p><p>来看看我调用事务的错误示例代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">@Component</span><br><span class="line">class DemoService&#123;</span><br><span class="line"></span><br><span class="line">   public List&lt;ExpressExportRow&gt; notExpressedRecord() &#123;</span><br><span class="line">    ...</span><br><span class="line">    changeOrderStatusToExpressing();</span><br><span class="line">    ...</span><br><span class="line">    return expressExportRowList;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  @Transactional(value = &quot;apolloTransactionManager&quot;,rollbackFor = Exception.class)</span><br><span class="line">  public boolean changeOrderStatusToExpressing(Long orderId, Long orderGroupId) &#123;</span><br><span class="line">    boolean isSuccess = updateExpressRecordStatus(orderId, orderGroupId,</span><br><span class="line">        OrderStatus.EXPRESSING.getValue());</span><br><span class="line">    if (isSuccess) &#123;</span><br><span class="line">      isSuccess = apolloService.updateOrderStatus(orderId, orderGroupId,</span><br><span class="line">          OrderStatus.EXPRESSING.getValue());</span><br><span class="line">    &#125;</span><br><span class="line">    if (!isSuccess) &#123;</span><br><span class="line">      throw new RuntimeException(&quot;更改订单状态失败,订单号为: &quot; + orderId);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return isSuccess;</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>首先DemoService通过@Component注解在容器加载的时候是确实注入进去了， 由于AOP 结合了IOC 的一部分功能（ProxyFactoryBean中实现）也就是在容器启动的过程中跟着在Demoservice封装成了aop代理对象保存在容器中。上面的代码错在直接在service 里面通过this引用去调增强的方法，结果导致方法不会应用相应的增强处理。来看看aop的处理流程图<br><img src="http://jacobs.wanhb.cn/images/aop1.jpg" alt="aop事务处理流程"></p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><ul><li>清楚了导出bug的原因和spring事务的基本原理之后，相应的解决办法就很简单，即我们只需要确保我们拿到的demoservice实例是通过容器注入进来的即可，于是可以以下解法：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Component</span><br><span class="line">class Resource&#123;</span><br><span class="line">@Resource</span><br><span class="line">  DemoServcie demoService;</span><br><span class="line">  </span><br><span class="line">  public changeStatus()&#123;</span><br><span class="line">    demoService.changeOrderStatusToExpressing(...);</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ul><p>通过@Resource注入进来的demoService必然式容器中的已经加载配置好的代理对象。这样就能成功应用事务增强。</p><ul><li>或者也可以通过后置器BeanPostProcessor将自身注入进来，即在自身的service里完成调用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Component  </span><br><span class="line">public class DemoService implements BeanPostProcessor &#123;  </span><br><span class="line">    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123;  </span><br><span class="line">        return bean;  </span><br><span class="line">    &#125;  </span><br><span class="line">    public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123;  </span><br><span class="line">        if(bean instanceof BeanSelfAware) &#123; </span><br><span class="line">            ((BeanSelfAware) bean).setSelf(bean);</span><br><span class="line">        &#125;  </span><br><span class="line">        return bean;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>但是这样会出现循环依赖问题。所以第一种方法最为合适。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过这次的低级bug再一次发现自己对源码不够深入，虽然前前后后看过了不少spring源码，但是也没有立即发现bug的存在，各方面理解还有待提高。继续努力吧</p>]]></content>
      
      
        <tags>
            
            <tag> 成长 </tag>
            
            <tag> spring </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python 爬取新浪微博</title>
      <link href="/2016/12/01/python-%E7%88%AC%E5%8F%96%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A/"/>
      <url>/2016/12/01/python-%E7%88%AC%E5%8F%96%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A/</url>
      <content type="html"><![CDATA[<p>最近因为课设的要求，开始了对新浪微博数据的爬取研究，看了不少博客文章，也试了不少方法，原理无非就是模拟登录，但是感觉目前可用的方法太过分散，而且自从微博改版之后，很多以前适用的方法都基本没有用处了。这里总结一下几种可用的方法以及自己研究之后稳定可用的方法(所有的方法都是基于python2.7)：</p><hr><p>###1、绕过.com域名</p><p>如果没有爬取主站的刚需，只是对微博相关的数据感兴趣，可以尝试爬取微博cn域名下的内容(即<a href="http://weibo.cn)，亲测可用...最简单的办法就是先预先登录一下然后获取返回的cookie，贴入代码中作为请求的headers即可。" target="_blank" rel="noopener">http://weibo.cn)，亲测可用...最简单的办法就是先预先登录一下然后获取返回的cookie，贴入代码中作为请求的headers即可。</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">_header=&#123;</span><br><span class="line">        &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.86 Safari/537.36&quot;,</span><br><span class="line">        &quot;Cookie&quot;:&quot;_T_WM=03e77f532a8c1a437da863b36a62207d; SUB=_2A256KfecDeRxGeVP61MX9yzKyT-IHXVZ1ZnUrDV6PUNbvtANLRTVkW1LHesQJOUc8nbbLnoALvjmulMBSwDnAw..; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WhPUuFTXg4zll8rx_8Ap-XA5JpX5KMhUgL.Foepeh2cS0zceoet; SUHB=0cSXC9tcKk2RM7; SSOLoginState=1462601676; gsid_CTandWM=4uTtCpOz5hhWcws1tVSIdd0SYa3&quot;    &#125;</span><br><span class="line"> request = urllib2.Request(url=url, headers=self._header)</span><br><span class="line"> response = urllib2.urlopen(request)</span><br><span class="line"> html = response.read()</span><br></pre></td></tr></table></figure><p>接下来对爬取下来的html就可以通过xpath,或者bs来完成数据提取了。</p><hr><p>###2、使用urllib模拟登录微博.com主站</p><p>这个过程比较麻烦，前人有了很多铺垫做相应的改动直接拿来用就好啦，以下代码亲测可用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">-*- coding: utf-8 -*-import urllib2</span><br><span class="line">import re</span><br><span class="line">import rsa</span><br><span class="line">import cookielib  #从前的cookielibimport base64</span><br><span class="line">import json</span><br><span class="line">import urllib</span><br><span class="line">import binascii</span><br><span class="line">from lxml import etree</span><br><span class="line">import json</span><br><span class="line"> 用于模拟登陆新浪微博class launcher():</span><br><span class="line"> </span><br><span class="line">    cookieContainer=None    _headers=&#123;</span><br><span class="line">            &quot;User-Agent&quot;:&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36&quot;        &#125;</span><br><span class="line">    def __init__(self,username, password):</span><br><span class="line">        self.password = password</span><br><span class="line">        self.username = username</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def get_prelogin_args(self):</span><br><span class="line">        json_pattern = re.compile(&apos;\((.*)\)&apos;)</span><br><span class="line">        url = &apos;http://login.sina.com.cn/sso/prelogin.php?entry=weibo&amp;callback=sinaSSOController.preloginCallBack&amp;su=&amp;&apos; + self.get_encrypted_name() + &apos;&amp;rsakt=mod&amp;checkpin=1&amp;client=ssologin.js(v1.4.18)&apos;        try:</span><br><span class="line">            request = urllib2.Request(url)</span><br><span class="line">            response = urllib2.urlopen(request)</span><br><span class="line">            raw_data = response.read().decode(&apos;utf-8&apos;)</span><br><span class="line">            print &quot;get_prelogin_args&quot;+raw_data;</span><br><span class="line">            json_data = json_pattern.search(raw_data).group(1)</span><br><span class="line">            data = json.loads(json_data)</span><br><span class="line">            return data</span><br><span class="line">        except urllib2.HTTPError as e:</span><br><span class="line">            print(&quot;%d&quot;%e.code)</span><br><span class="line">            return None    def get_encrypted_pw(self,data):</span><br><span class="line">        rsa_e = 65537 #0x10001        pw_string = str(data[&apos;servertime&apos;]) + &apos;\t&apos; + str(data[&apos;nonce&apos;]) + &apos;\n&apos; + str(self.password)</span><br><span class="line">        key = rsa.PublicKey(int(data[&apos;pubkey&apos;],16),rsa_e)</span><br><span class="line">        pw_encypted = rsa.encrypt(pw_string.encode(&apos;utf-8&apos;), key)</span><br><span class="line">        self.password = &apos;&apos;   #清空password        passwd = binascii.b2a_hex(pw_encypted)</span><br><span class="line">        print(passwd)</span><br><span class="line">        return passwd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def get_encrypted_name(self):</span><br><span class="line">        username_urllike   = urllib.quote(self.username)</span><br><span class="line">        byteStr=bytes(username_urllike)</span><br><span class="line">        byteStrEncod=byteStr.encode(encoding=&quot;utf-8&quot;)</span><br><span class="line">        username_encrypted = base64.b64encode(byteStrEncod)</span><br><span class="line">        return username_encrypted.decode(&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def enableCookies(self):</span><br><span class="line">            #建立一个cookies 容器            self.cookieContainer = cookielib.MozillaCookieJar(&quot;/Users/lichao/desktop/weibo/cookie/cookie.txt&quot;);</span><br><span class="line">            # ckjar=cookielib.MozillaCookieJar(&quot;/Users/Apple/Desktop/cookie.txt&quot;)            #将一个cookies容器和一个HTTP的cookie的处理器绑定            cookie_support = urllib2.HTTPCookieProcessor(self.cookieContainer)</span><br><span class="line">            #创建一个opener,设置一个handler用于处理http的url打开            opener = urllib2.build_opener(cookie_support, urllib2.HTTPHandler)</span><br><span class="line">            #安装opener，此后调用urlopen()时会使用安装过的opener对象            # proxy_handler = urllib2.ProxyHandler(&#123;&quot;http&quot;: &apos;http://localhost:5000&apos;&#125;)            # opener=urllib2.build_opener(proxy_handler)            urllib2.install_opener(opener)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def build_post_data(self,raw):</span><br><span class="line">        post_data = &#123;</span><br><span class="line">            &quot;entry&quot;:&quot;weibo&quot;,</span><br><span class="line">            &quot;gateway&quot;:&quot;1&quot;,</span><br><span class="line">            &quot;from&quot;:&quot;&quot;,</span><br><span class="line">            &quot;savestate&quot;:&quot;7&quot;,</span><br><span class="line">            &quot;useticket&quot;:&quot;1&quot;,</span><br><span class="line">            &quot;pagerefer&quot;:&quot;Sina Visitor System&quot;,</span><br><span class="line">            &quot;vsnf&quot;:&quot;1&quot;,</span><br><span class="line">            &quot;su&quot;:self.get_encrypted_name(),</span><br><span class="line">            &quot;service&quot;:&quot;miniblog&quot;,</span><br><span class="line">            &quot;servertime&quot;:raw[&apos;servertime&apos;],</span><br><span class="line">            &quot;nonce&quot;:raw[&apos;nonce&apos;],</span><br><span class="line">            &quot;pwencode&quot;:&quot;rsa2&quot;,</span><br><span class="line">            &quot;rsakv&quot;:raw[&apos;rsakv&apos;],</span><br><span class="line">            &quot;sp&quot;:self.get_encrypted_pw(raw),</span><br><span class="line">            &quot;sr&quot;:&quot;1280*800&quot;,</span><br><span class="line">            &quot;encoding&quot;:&quot;UTF-8&quot;,</span><br><span class="line">            &quot;prelt&quot;:&quot;77&quot;,</span><br><span class="line">            &quot;url&quot;:&quot;http://weibo.com/ajaxlogin.php?framelogin=1&amp;callback=parent.sinaSSOController.feedBackUrlCallBack&quot;,</span><br><span class="line">            &quot;returntype&quot;:&quot;META&quot;        &#125;</span><br><span class="line">        data = urllib.urlencode(post_data).encode(&apos;utf-8&apos;)</span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def login(self):</span><br><span class="line">        url = &apos;新浪通行证&apos;        self.enableCookies()</span><br><span class="line">        data = self.get_prelogin_args()</span><br><span class="line">        post_data = self.build_post_data(data)</span><br><span class="line">        headers = &#123;</span><br><span class="line">            &quot;User-Agent&quot;:&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36&quot;        &#125;</span><br><span class="line">        try:</span><br><span class="line">            request = urllib2.Request(url=url,data=post_data,headers=headers)</span><br><span class="line">            response = urllib2.urlopen(request)</span><br><span class="line">            html = response.read().decode(&apos;GBK&apos;)</span><br><span class="line">            #print(html)        except urllib2.HTTPError as e:</span><br><span class="line">            print(e.code)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        p = re.compile(&apos;location\.replace\(\&apos;(.*?)\&apos;\)&apos;)</span><br><span class="line">        p2 = re.compile(r&apos;&quot;userdomain&quot;:&quot;(.*?)&quot;&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            login_url = p.search(html).group(1)</span><br><span class="line">            print(login_url)</span><br><span class="line">            request = urllib2.Request(login_url)</span><br><span class="line">            response = urllib2.urlopen(request)</span><br><span class="line">            page = response.read().decode(&apos;utf-8&apos;)</span><br><span class="line">            print(page)</span><br><span class="line">            login_url = &apos;http://weibo.com/&apos; + p2.search(page).group(1)</span><br><span class="line">            request = urllib2.Request(login_url)</span><br><span class="line">            response = urllib2.urlopen(request)</span><br><span class="line">            final = response.read().decode(&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            print(&quot;Login success!&quot;)</span><br><span class="line">            self.cookieContainer.save(ignore_discard=True, ignore_expires=True)</span><br><span class="line">        except Exception, e:</span><br><span class="line">            print(&apos;Login error!&apos;)</span><br><span class="line">            print e</span><br><span class="line">            return 0</span><br></pre></td></tr></table></figure><p>###3、使用selenium实现模拟登录</p><ul><li>selenium +phantomjs</li></ul><p>第二种方法有一个问题，因为目前新版的微博页面的渲染方式采用的是分片渲染的，这就导致我们通过第二种静态方式爬取到的页面并不是最终的页面，而是内容嵌在 js里的中间页面，这肯定不是我们想看到的结果。于是，考虑模拟浏览器渲染页面的方式获取到最终的呈现页面。selenium这个工具正好完美的解决了我们的问题，它可以模拟浏览器的行为，并且我们拿到的source可以向jquery操作dom对象那样查找定位元素，非常方便，实现的核心代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">from selenium import webdriver</span><br><span class="line">import urllib2</span><br><span class="line">import selenium.webdriver.support.ui as ui</span><br><span class="line">import sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding( &quot;utf-8&quot; )</span><br><span class="line">from selenium.webdriver.common.keys import Keys</span><br><span class="line">Chrome PhantomJS#driver = webdriver.PhantomJS(&quot;/Users/test/documents/phantomjs/bin/phantomjs&quot;)</span><br><span class="line">driver.get(&apos;http://weibo.com/&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    print &quot;登录开始&quot;</span><br><span class="line">    username = driver.find_element_by_xpath(&apos;//input[@name=&quot;username&quot;]&apos;)</span><br><span class="line">    password = driver.find_element_by_xpath(&apos;//input[@name=&quot;password&quot;]&apos;)</span><br><span class="line">    sbtn = driver.find_element_by_xpath(&apos;//a[@action-type=&quot;btn_submit&quot;]&apos;)</span><br><span class="line">    username.send_keys(&apos;&apos;) #send username       </span><br><span class="line">    password.send_keys(&apos;&apos;) #send password    sbtn.click()  </span><br><span class="line">    # 提交表单    </span><br><span class="line">    time.sleep(3)  # 等待页面加载   </span><br><span class="line">    # get the session cookie    </span><br><span class="line">    cookie = &#123;item[&quot;name&quot;] + &quot;:&quot; + item[&quot;value&quot;] for item in driver.get_cookies()&#125;    cookie=driver.get_cookies()</span><br><span class="line">    for item in driver.get_cookies():    cookieItem=&#123;&quot;name&quot;:item[&quot;name&quot;],&quot;value&quot;:item[&quot;value&quot;],&quot;domain&quot;:item[&quot;domain&quot;],&quot;httponly&quot;:item[&quot;httponly&quot;],&quot;path&quot;:item[&quot;path&quot;],&quot;secure&quot;:item[&quot;secure&quot;]&#125;    cookie.append(cookieItem)    cookie_file= open(&quot;/Users/test/desktop/weibo/cookie/cookie.txt&quot;,&apos;w&apos;)    cookie_file.write(str(cookie))    print str(str(cookie))</span><br><span class="line">except urllib2.HTTPError as e:</span><br><span class="line">    print e</span><br><span class="line">    print &quot;登录失败&quot;print &quot;开始爬取谣言大厅&quot;driver.get(&quot;http://service.account.weibo.com/show?rid=K1CaN7gJl8q8f&quot;)</span><br><span class="line">page = driver.page_source</span><br><span class="line">print page</span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure><p>我们将登录之后获取的cookie以键值对的形式存入文本文件中，方便下次直接load而不需要重复登录:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def loadCookie(self):</span><br><span class="line">self._driver.get(&quot;http://www.sina.com.cn&quot;)</span><br><span class="line">    cookie_file=open(&quot;/Users/test/desktop/weibo/cookie/cookie.txt&quot;,&apos;r&apos;)</span><br><span class="line">    cookieStr=cookie_file.read();</span><br><span class="line">print &quot;cookie is: &quot;+cookieStr</span><br><span class="line">    cookieList=list(eval(cookieStr))</span><br><span class="line">for item in cookieList:</span><br><span class="line">cookieDic= type(eval(item))</span><br><span class="line">        self._driver.add_cookie(item)</span><br></pre></td></tr></table></figure><ul><li>selenium +chromedirver</li></ul><p>使用phantomjs存在一个问题，登录过程老是失败，因为验证码无法识别获取导致登录经常失败，这里我们使用chromedirver这工具结合selenium实现开挂级别的python数据爬取，模拟登录万无一失，核心代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print &quot;登录开始&quot;</span><br><span class="line">username = driver.find_element_by_xpath(&apos;//input[@name=&quot;username&quot;]&apos;)</span><br><span class="line">password = driver.find_element_by_xpath(&apos;//input[@name=&quot;password&quot;]&apos;)</span><br><span class="line">sbtn = driver.find_element_by_xpath(&apos;//a[@action-type=&quot;btn_submit&quot;]&apos;)</span><br><span class="line">veryfiCode=driver.find_element_by_xpath(&apos;//input[@name=&quot;verifycode&quot;]&apos;)</span><br></pre></td></tr></table></figure><p> 程序启动时会自动开启一个chrome窗口，只不过这个浏览器的行为我们可以通过程序控制，这样是不是方便多了！我们在username这一行打一个断点，然后程序执行到这一步，在浏览器中输入相应的用户名，密码，验证码，然后在pycharm中点击继续，登录成功！真实浏览器结合程序，真是开挂级别的爬取微博啊…</p>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Crawler </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>手动封装HbaseTemplate mapper类</title>
      <link href="/2016/10/09/%E6%89%8B%E5%8A%A8%E5%B0%81%E8%A3%85HbaseTemplate-mapper%E7%B1%BB/"/>
      <url>/2016/10/09/%E6%89%8B%E5%8A%A8%E5%B0%81%E8%A3%85HbaseTemplate-mapper%E7%B1%BB/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近因为业务关系，用到了Hbase，因为用的是Spring boot框架 ，所以自然而然就用到了spring封装的HbaseTemplate工具类。然而HbaseTemplate封装的代码实在比较糟糕，出了一些基本的CRUD操作之外并没有给我们提供太多便利之处。先来看看痛处：</p><h3 id="痛处一及改进"><a href="#痛处一及改进" class="headerlink" title="痛处一及改进"></a>痛处一及改进</h3><ul><li>我们先来看看HabaseTemplate最基本的查询操作(以下只是demo演示)：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class UserInfo&#123;</span><br><span class="line"> string name;</span><br><span class="line"> string password;</span><br><span class="line">&#125;</span><br><span class="line">public void putUserInfo(UserInfo userInfo) &#123;</span><br><span class="line">   hBaseTemplate.execute(TABLE_NAME, (table) -&gt; &#123;</span><br><span class="line">     //根据rowKey定义一个put对象，可用作插入和更新</span><br><span class="line">     Put put = new Put(Bytes.toBytes(rowKey));</span><br><span class="line">    //name是否为空</span><br><span class="line">     if(userInfo.name!=null)&#123;</span><br><span class="line">     put.addColumn(COLUMN_FAMILY_NAME.getBytes(), Bytes.toBytes(COLUMN_RAW_DATA)，Bytes.toBytes(userInfo.name));</span><br><span class="line">     &#125;</span><br><span class="line">     //password是否为空</span><br><span class="line">     if(userInfo.password!=null)&#123;</span><br><span class="line">     put.addColumn(COLUMN_FAMILY_NAME.getBytes(), Bytes.toBytes(COLUMN_RAW_DATA)，Bytes.toBytes(userInfo.password));</span><br><span class="line">     &#125;</span><br><span class="line">     table.put(put);</span><br><span class="line">     return true;</span><br><span class="line">   &#125;);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>相信大家也看出来了，如果待插入的对象有很多字段呢？还要逐个写if语句来判读非空么？这明显使得代码非常地不简洁。于是，个人封装了一个插入更新模版类（其实只是简单的对Put对象的一个扩展）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//继承并扩展Put对象</span><br><span class="line">public class PutExtension extends Put &#123;</span><br><span class="line"></span><br><span class="line">  String columnFamilyName = &quot;demo&quot;;</span><br><span class="line"></span><br><span class="line">  public PutExtension(String columnFamilyName, byte[] row) &#123;</span><br><span class="line">    super(row);</span><br><span class="line">    this.columnFamilyName = columnFamilyName;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  public PutExtension build(String paramName, Object param) throws IOException &#123;</span><br><span class="line">    if (param != null) &#123;</span><br><span class="line">      this.addColumn(columnFamilyName.getBytes(), paramName.getBytes(),</span><br><span class="line">          Bytes.toBytes(param.toString()));</span><br><span class="line">    &#125;</span><br><span class="line">    return this;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>封装之后，之前累赘的查询操作可以变得如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">//然后操作如下</span><br><span class="line">hBaseTemplate.execute(TABLE_NAME, (table) -&gt; &#123;</span><br><span class="line">      PutExtension putExtension = new PutExtension(familyName, rowKey.getBytes());</span><br><span class="line">      putExtension.build(&quot;name&quot;,userInfo.name)</span><br><span class="line">          .build(&quot;password&quot;, userInfo.password);</span><br><span class="line">      table.put(putExtension);</span><br><span class="line">      return true;</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><p>其实这也就是一个简单的封装，只不过把冗余的逻辑判断给丢出去了而已。</p><h3 id="痛处二及改进"><a href="#痛处二及改进" class="headerlink" title="痛处二及改进"></a>痛处二及改进</h3><ul><li>在HbaseTemplate中，根据rowKey查询出来的原始数据是字节数组，我们要将字节数组转化成业务逻辑中希望的java bean需要做很多重复的判断匹配逻辑，以下是没改进前的代码：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> public UserInfo getUserInfo() &#123;</span><br><span class="line">    return (UserInfo) hBaseTemplate.get(TABLE_NAME, rowKey, familyName,(result, i) -&gt;&#123;</span><br><span class="line">     UserInfo userInfo=new UserInfo()</span><br><span class="line">     //重复逻辑一</span><br><span class="line">bytes[] nameBytes=result.getValue(familyName.getBytes(), &quot;name&quot;.getBytes()));</span><br><span class="line">if(nameBytes!=null)&#123;</span><br><span class="line">  userInfo.setName(Bytes.toString(nameBytes));</span><br><span class="line">&#125;</span><br><span class="line">//重复逻辑二</span><br><span class="line">bytes[] passwordBytes=result.getValue(familyName.getBytes(), &quot;password&quot;.getBytes()));</span><br><span class="line">if(passwordBytes!=null)&#123;</span><br><span class="line">  userInfo.setPassword(Bytes.toString(passwordBytes));</span><br><span class="line">&#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出，这样做的缺点是一旦java bean的字段一多，重复的非空判断逻辑也会增多，从而使得代码变得十分累赘且不可维护。于是我参考Spring JDBC的RowMapper的封装，利用了Spring框架自带的反射工具beanUtils和beanWrapper，自己实现了如下封装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">public class HBaseResultBuilder&lt;T&gt; &#123;</span><br><span class="line">  private Class&lt;T&gt; mappedClass;</span><br><span class="line">  private Map&lt;String, PropertyDescriptor&gt; mappedFields;</span><br><span class="line">  private Set&lt;String&gt; mappedProperties;</span><br><span class="line">  HashSet populatedProperties;</span><br><span class="line">  private BeanWrapper beanWrapper;</span><br><span class="line">  private Result result;</span><br><span class="line">  private String columnFamilyName;</span><br><span class="line">  private T t;</span><br><span class="line">  //接受一些列参数并实例化要返回的结果对象</span><br><span class="line">  public HBaseResultBuilder(String columnFamilyName, Result result, Class&lt;T&gt; clazz) &#123;</span><br><span class="line">    this.columnFamilyName = columnFamilyName;</span><br><span class="line">    this.result = result;</span><br><span class="line">    this.mappedClass = clazz;</span><br><span class="line">    mappedFields = new HashMap&lt;&gt;();</span><br><span class="line">    mappedProperties = new HashSet&lt;&gt;();</span><br><span class="line">    populatedProperties = new HashSet&lt;&gt;();</span><br><span class="line">    this.t = BeanUtils.instantiate(clazz);</span><br><span class="line">    PropertyDescriptor[] pds = BeanUtils.getPropertyDescriptors(mappedClass);</span><br><span class="line">    PropertyDescriptor[] var3 = pds;</span><br><span class="line">    int var4 = pds.length;</span><br><span class="line">    for (int var5 = 0; var5 &lt; var4; ++var5) &#123;</span><br><span class="line">      PropertyDescriptor pd = var3[var5];</span><br><span class="line">      if (pd.getWriteMethod() != null) &#123;</span><br><span class="line">        this.mappedFields.put(this.lowerCaseName(pd.getName()), pd);</span><br><span class="line">        String underscoredName = this.underscoreName(pd.getName());</span><br><span class="line">        if (!this.lowerCaseName(pd.getName()).equals(underscoredName)) &#123;</span><br><span class="line">          this.mappedFields.put(underscoredName, pd);</span><br><span class="line">        &#125;</span><br><span class="line">        this.mappedProperties.add(pd.getName());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    beanWrapper = PropertyAccessorFactory.forBeanPropertyAccess(t);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private String underscoreName(String name) &#123;</span><br><span class="line">    if (!StringUtils.hasLength(name)) &#123;</span><br><span class="line">      return &quot;&quot;;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      StringBuilder result = new StringBuilder();</span><br><span class="line">      result.append(this.lowerCaseName(name.substring(0, 1)));</span><br><span class="line"></span><br><span class="line">      for (int i = 1; i &lt; name.length(); ++i) &#123;</span><br><span class="line">        String s = name.substring(i, i + 1);</span><br><span class="line">        String slc = this.lowerCaseName(s);</span><br><span class="line">        if (!s.equals(slc)) &#123;</span><br><span class="line">          result.append(&quot;_&quot;).append(slc);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          result.append(s);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      return result.toString();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private String lowerCaseName(String name) &#123;</span><br><span class="line">    return name.toLowerCase(Locale.US);</span><br><span class="line">  &#125;</span><br><span class="line">  //使用时根据要解析的字段频繁调用此方法即可，仿造java8 流式操作</span><br><span class="line">  public HBaseResultBuilder build(String columnName) &#123;</span><br><span class="line">    byte[] value = result.getValue(columnFamilyName.getBytes(), columnName.getBytes());</span><br><span class="line">    if (value == null || value.length == 0) &#123;</span><br><span class="line">      return this;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      String field = this.lowerCaseName(columnName.replaceAll(&quot; &quot;, &quot;&quot;));</span><br><span class="line">      PropertyDescriptor pd = this.mappedFields.get(field);</span><br><span class="line">      if (pd == null) &#123;</span><br><span class="line">        log.error(&quot;HBaseResultBuilder error: can not find property: &quot; + field);</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        beanWrapper.setPropertyValue(pd.getName(), Bytes.toString(value));</span><br><span class="line">        populatedProperties.add(pd.getName());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return this;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  //伪造Java8的即视感，“流最后的终端操作“。</span><br><span class="line">  public T fetch() &#123;</span><br><span class="line">    //只要有一个属性被解析出来就返回结果对象，毕竟hbase存的是稀疏数据，不一定全量</span><br><span class="line">    if (CollectionUtils.isNotEmpty(populatedProperties)) &#123;</span><br><span class="line">      return this.t;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return null;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>通过利用反射的基本原理，我们可以通过结果数据构造出我们需要的java bean。最后我们的调用过程可以简化成如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public UserInfo getUserInfo() &#123;</span><br><span class="line">    return (UserInfo) hBaseTemplate.get(TABLE_NAME, rowKey,    familyName,</span><br><span class="line">        (result, i) -&gt; new HBaseResultBuilder&lt;&gt;(familyName, result, UserInfo.class).build(&quot;name&quot;).build(&quot;password&quot;).fetch());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>成功！！！是不是代码整洁多了，其实也就是将一些复杂的逻辑给抽出去了，正好最近看了Java8实战，从而萌生的一点小想法。</p>]]></content>
      
      
        <tags>
            
            <tag> Hbase </tag>
            
            <tag> Java </tag>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
